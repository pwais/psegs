{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheap Optical Flow: Is it Good? Does it Boost?\n",
    "\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "## Credits\n",
    "\n",
    "Some portions of this notebook adapted from:\n",
    " * [Middlebury Flow code by Johannes Oswald](https://github.com/Johswald/flow-code-python/blob/master/readFlowFile.py)\n",
    " * [DeepDeform Demo Code](https://github.com/AljazBozic/DeepDeform)\n",
    " * [OpticalFlowToolkit by RUOTENG LI](https://github.com/liruoteng/OpticalFlowToolkit)\n",
    " * [OpenCV Samples](https://github.com/opencv/opencv/blob/master/samples/python/opt_flow.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "SHOW_DEMO_OUTPUT = False\n",
    "DEMO_FPS = []\n",
    "\n",
    "RUN_FULL_ANALYSIS = False\n",
    "ALL_FP_FACTORY_CLSS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypng in /usr/local/lib/python3.8/dist-packages (0.0.20)\n",
      "Requirement already satisfied: scikit-image in /usr/lib/python3/dist-packages (0.16.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "fixme installs\n",
      "\n",
      "\n",
      "Putting analysis lib in /tmp/tmpekna3vk7_cheap_optical_flow_eval_analysis\n",
      "running clean\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing psegs.egg-info/PKG-INFO\n",
      "writing dependency_links to psegs.egg-info/dependency_links.txt\n",
      "writing top-level names to psegs.egg-info/top_level.txt\n",
      "reading manifest file 'psegs.egg-info/SOURCES.txt'\n",
      "writing manifest file 'psegs.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "warning: build_py: byte-compiling is disabled, skipping.\n",
      "\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/psegs\n",
      "copying build/lib/psegs/dummyrun.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "creating build/bdist.linux-x86_64/egg/psegs/datasets\n",
      "copying build/lib/psegs/datasets/kitti.py -> build/bdist.linux-x86_64/egg/psegs/datasets\n",
      "copying build/lib/psegs/datasets/idsutil.py -> build/bdist.linux-x86_64/egg/psegs/datasets\n",
      "copying build/lib/psegs/datasets/kitti_360.py -> build/bdist.linux-x86_64/egg/psegs/datasets\n",
      "copying build/lib/psegs/datasets/__init__.py -> build/bdist.linux-x86_64/egg/psegs/datasets\n",
      "copying build/lib/psegs/datasets/nuscenes.py -> build/bdist.linux-x86_64/egg/psegs/datasets\n",
      "creating build/bdist.linux-x86_64/egg/psegs/util\n",
      "copying build/lib/psegs/util/__init__.py -> build/bdist.linux-x86_64/egg/psegs/util\n",
      "copying build/lib/psegs/util/misc.py -> build/bdist.linux-x86_64/egg/psegs/util\n",
      "copying build/lib/psegs/util/plotting.py -> build/bdist.linux-x86_64/egg/psegs/util\n",
      "creating build/bdist.linux-x86_64/egg/psegs/table\n",
      "copying build/lib/psegs/table/sd_db.py -> build/bdist.linux-x86_64/egg/psegs/table\n",
      "copying build/lib/psegs/table/sd_table.py -> build/bdist.linux-x86_64/egg/psegs/table\n",
      "copying build/lib/psegs/table/__init__.py -> build/bdist.linux-x86_64/egg/psegs/table\n",
      "copying build/lib/psegs/browser.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "creating build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/uri.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/bbox2d.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/datumutils.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/transform.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/stamped_datum.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/cuboid.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/frame.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/point_cloud.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/camera_image.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/__init__.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/spark.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "copying build/lib/psegs/conf.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "copying build/lib/psegs/dsutil.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "copying build/lib/psegs/__init__.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "creating build/bdist.linux-x86_64/egg/psegs/exp\n",
      "copying build/lib/psegs/exp/semantic_kitti.py -> build/bdist.linux-x86_64/egg/psegs/exp\n",
      "copying build/lib/psegs/exp/fused_lidar_flow.py -> build/bdist.linux-x86_64/egg/psegs/exp\n",
      "copying build/lib/psegs/exp/__init__.py -> build/bdist.linux-x86_64/egg/psegs/exp\n",
      "copying build/lib/psegs/ros.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "warning: install_lib: byte-compiling is disabled, skipping.\n",
      "\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying psegs.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying psegs.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying psegs.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying psegs.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/psegs-0.0.1-py3.8.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:11:44,327\toarph 1807391 : Using source root /tmp/tmpekna3vk7_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis \n",
      "2021-04-28 21:11:44,327\toarph 1807391 : Using source root /tmp/tmpekna3vk7_cheap_optical_flow_eval_analysis \n",
      "2021-04-28 21:11:44,371\toarph 1807391 : Generating egg to /tmp/tmprsc_pbn6_oarphpy_eggbuild ...\n",
      "2021-04-28 21:11:44,383\toarph 1807391 : ... done.  Egg at /tmp/tmprsc_pbn6_oarphpy_eggbuild/cheap_optical_flow_eval_analysis-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "## Setup\n",
    "\n",
    "!pip3 install pypng scikit-image\n",
    "print('fixme installs')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import copy\n",
    "import imageio\n",
    "import IPython.display\n",
    "import math\n",
    "import os\n",
    "import PIL.Image\n",
    "import six\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "\n",
    "## General Notebook Utilities\n",
    "    \n",
    "def imshow(x):\n",
    "    IPython.display.display(PIL.Image.fromarray(x))\n",
    "\n",
    "def show_html(x):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(x))\n",
    "\n",
    "    \n",
    "PLOTLY_INIT_HTML = \"\"\"\n",
    "    <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "    <script src='https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js'></script>\n",
    "    <script>requirejs.config({\n",
    "        paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});\n",
    "        if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>\n",
    "    \"\"\"\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    show_html(PLOTLY_INIT_HTML)\n",
    "\n",
    "## Create a random temporary directory for analysis library (for Spark-enabled full analysis mode)\n",
    "old_cwd = os.getcwd()\n",
    "tempdir = tempfile.TemporaryDirectory(suffix='_cheap_optical_flow_eval_analysis')\n",
    "ALIB_SRC_DIR = tempdir.name\n",
    "print(\"Putting analysis lib in %s\" % ALIB_SRC_DIR)\n",
    "os.chdir(ALIB_SRC_DIR)\n",
    "!mkdir -p cheap_optical_flow_eval_analysis\n",
    "!touch cheap_optical_flow_eval_analysis/__init__.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append(ALIB_SRC_DIR)\n",
    "\n",
    "\n",
    "## Prepare a build of local psegs for inclusion\n",
    "!cd /opt/psegs && python3 setup.py clean bdist_egg\n",
    "PSEGS_EGG_PATH = '/opt/psegs/dist/psegs-0.0.1-py3.8.egg'\n",
    "assert os.path.exists(PSEGS_EGG_PATH), \"Build failed?\"\n",
    "sys.path.append('/opt/psegs')\n",
    "import psegs\n",
    "\n",
    "\n",
    "## Prepare Spark session with local PSegs and local Analysis Lib\n",
    "from psegs.spark import NBSpark\n",
    "NBSpark.SRC_ROOT = os.path.join(ALIB_SRC_DIR, 'cheap_optical_flow_eval_analysis')\n",
    "NBSpark.CONF_KV.update({\n",
    "    'spark.driver.maxResultSize': '2g',\n",
    "    'spark.driver.memory': '16g',\n",
    "    'spark.submit.pyFiles': PSEGS_EGG_PATH,\n",
    "  })\n",
    "spark = NBSpark.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cheap_optical_flow_eval_analysis/ofp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cheap_optical_flow_eval_analysis/ofp.py\n",
    "\n",
    "## Data Model & Utility Code\n",
    "\n",
    "import attr\n",
    "import cv2\n",
    "import imageio\n",
    "import math\n",
    "import os\n",
    "import PIL.Image\n",
    "import six\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from psegs import datum\n",
    "\n",
    "from oarphpy import plotting as op_plt\n",
    "from oarphpy.spark import CloudpickeledCallable\n",
    "img_to_data_uri = lambda x: op_plt.img_to_data_uri(x, format='png')\n",
    "\n",
    "@attr.s(slots=True, eq=False, weakref_slot=False)\n",
    "class OpticalFlowPair(object):\n",
    "    \"\"\"A flyweight for a pair of images with an optical flow field.\n",
    "    Supports lazy-loading of large data attributes.\"\"\"\n",
    "    \n",
    "    ## Core Attributes (Required for All Datasets)\n",
    "    \n",
    "    dataset = attr.ib(type=str, default='')\n",
    "    \"\"\"(Display name) To which dataset does this pair belong?\"\"\"\n",
    "    \n",
    "    id1 = attr.ib(type=str, default='')\n",
    "    \"\"\"Identifier or URI for the first image\"\"\"\n",
    "    \n",
    "    id2 = attr.ib(type=str, default='')\n",
    "    \"\"\"Identifier or URI for the second image\"\"\"\n",
    "    \n",
    "    img1 = attr.ib(default=None)\n",
    "    \"\"\"URI or numpy array or CloudPickleCallable for the first image (source image)\"\"\"\n",
    "\n",
    "    img2 = attr.ib(default=None)\n",
    "    \"\"\"URI or numpy array or CloudpickeledCallable for the second image (target image)\"\"\"\n",
    "    \n",
    "    flow = attr.ib(default=None)\n",
    "    \"\"\"A numpy array or callable or CloudpickeledCallable representing optical flow from img1 -> img2\"\"\"\n",
    "    \n",
    "    uri = attr.ib(type=datum.URI, default=None, converter=datum.URI.from_str)\n",
    "    \"\"\"A URI addressing this pair; to make dynamic construction of the pair easier\"\"\"\n",
    "    \n",
    "    \n",
    "    ## Optional Attributes (For Select Datasets)\n",
    "    \n",
    "    diff_time_sec = attr.ib(type=float, default=-1.0)\n",
    "    \"\"\"Difference in time (in seconds) between the views / poses depicted in `img1` and `img2`.\"\"\"\n",
    "    \n",
    "    translation_meters = attr.ib(type=float, default=-1.0)\n",
    "    \"\"\"Difference in ego translation (in meters) between the views / poses depicted in `img1` and `img2`.\"\"\"\n",
    "    \n",
    "    uvdviz_im1 = attr.ib(default=None)\n",
    "    \"\"\"An nx4 numpy array representing UVD-visible points for `img1`\"\"\"\n",
    "    \n",
    "    uvdviz_im2 = attr.ib(default=None)\n",
    "    \"\"\"An nx4 numpy array representing UVD-visible points for `img2`\"\"\"\n",
    "    \n",
    "    K = attr.ib(default=None)\n",
    "    \"\"\"A 3x3 numpy array representing the camera matrix K for both views\"\"\"\n",
    "    \n",
    "    # to add:\n",
    "    # semantic image for frame 1, frame 2 [could be painted by cuboids]\n",
    "    # instance images for frame 1, frame 2 [could be painted by cuboids]\n",
    "    #   -- for colored images, at first just pivot all oflow metrics by colors\n",
    "    # get uvd1 uvd2 (lidar for nearest neighbor stuff)\n",
    "    # depth image for frame 1, frame 2 [could be interpolated by cuboids]\n",
    "    #   -- at first bucket the depth coarsely and pivot al oflow by colors\n",
    "    \n",
    "    def get_img1(self):\n",
    "        if isinstance(self.img1, CloudpickeledCallable):\n",
    "            self.img1 = self.img1()\n",
    "        if isinstance(self.img1, six.string_types):\n",
    "            self.img1 = imageio.imread(self.img1)\n",
    "        return self.img1\n",
    "    \n",
    "    def get_img2(self):\n",
    "        if isinstance(self.img2, CloudpickeledCallable):\n",
    "            self.img2 = self.img2()\n",
    "        if isinstance(self.img2, six.string_types):\n",
    "            self.img2 = imageio.imread(self.img2)\n",
    "        return self.img2\n",
    "    \n",
    "    def get_flow(self):\n",
    "        if not isinstance(self.flow, (np.ndarray, np.generic)):\n",
    "            self.flow = self.flow()\n",
    "        return self.flow\n",
    "    \n",
    "    def has_scene_flow(self):\n",
    "        return (\n",
    "            self.uvdviz_im1 is not None and \n",
    "            self.uvdviz_im1.shape[0] > 0 and\n",
    "            self.uvdviz_im2 is not None and \n",
    "            self.uvdviz_im2.shape[0] > 0 and\n",
    "            self.K is not None)\n",
    "    \n",
    "    def get_sf_viz_html(self):\n",
    "        uvd1 = self.uvdviz_im1[:, :3]\n",
    "        uvd2 = self.uvdviz_im2[:, :3]\n",
    "#         visible_either = ((uvd1[:, -1] == 1) | (uvd2[:, -1] == 1))\n",
    "#         uvd1 = uvd1[visible_either, :3]\n",
    "#         uvd2 = uvd2[visible_either, :3]\n",
    "        \n",
    "        xyzrgb1 = uvd_to_xyzrgb(uvd1, self.K, imgs=[self.get_img1()])\n",
    "        xyzrgb2 = uvd_to_xyzrgb(uvd2, self.K, imgs=[self.get_img2()])\n",
    "        html1 = create_xyzrgb_3d_plot_html(xyzrgb1)\n",
    "        html2 = create_xyzrgb_3d_plot_html(xyzrgb2)\n",
    "        html_sf = create_xyzrgb_3d_sf_plot_html(xyzrgb1, xyzrgb2)\n",
    "        \n",
    "        html = \"View 1:<br />%s<br /><br />View 2:<br />%s<br /><br />Flow:<br />%s\" % (html1, html2, html_sf)\n",
    "        return html\n",
    "    \n",
    "    def to_html(self):\n",
    "        im1 = self.get_img1()\n",
    "        im2 = self.get_img2()\n",
    "        flow = self.get_flow()\n",
    "        fviz = draw_flow(im1, flow)\n",
    "        \n",
    "        sf_html = ''\n",
    "        if self.has_scene_flow():\n",
    "            sf_html = \"\"\"\n",
    "                <tr><td style=\"text-align:left\"><b>Scene Flow</b></td></tr>\n",
    "                <tr><td>{viz_html}</td></tr>\n",
    "            \"\"\".format(viz_html=self.get_sf_viz_html())\n",
    "        \n",
    "        html = \"\"\"\n",
    "            \n",
    "            <table>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Dataset:</b> {dataset}</td></tr>\n",
    "            <tr><td style=\"text-align:left\"><b>URI:</b> {uri}</td></tr>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Diff (seconds, optional):</b> {diff_time_sec}</td></tr>\n",
    "            <tr><td style=\"text-align:left\"><b>Translation (meters, optional):</b> {translation_meters}</td></tr>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Source Image:</b> {id1}</td></tr>\n",
    "            <tr><td><img src=\"{im1}\" /></td></tr>\n",
    "\n",
    "            <tr><td style=\"text-align:left\"><b>Target Image:</b> {id2}</td></tr>\n",
    "            <tr><td><img src=\"{im2}\" /></td></tr>\n",
    "\n",
    "            <tr><td style=\"text-align:left\"><b>Flow</b></td></tr>\n",
    "            <tr><td><img src=\"{fviz}\" /></td></tr>\n",
    "            \n",
    "            {sf_html}\n",
    "            </table>\n",
    "        \"\"\".format(\n",
    "                dataset=self.dataset,\n",
    "                uri=str(self.uri),\n",
    "                diff_time_sec=self.diff_time_sec,\n",
    "                translation_meters=self.translation_meters,\n",
    "                id1=self.id1, id2=self.id2,\n",
    "                im1=img_to_data_uri(im1), im2=img_to_data_uri(im2),\n",
    "                fviz=img_to_data_uri(fviz),\n",
    "                sf_html=sf_html)\n",
    "        return html\n",
    "\n",
    "def draw_flow(img, flow, step=8):\n",
    "    \"\"\"Based upon OpenCV sample: https://github.com/opencv/opencv/blob/master/samples/python/opt_flow.py\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n",
    "    fx, fy = flow[y,x].T\n",
    "    lines = np.vstack([x, y, x+fx, y+fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "    vis = img.copy()\n",
    "    cv2.polylines(vis, lines, 0, (0, 255, 0))\n",
    "    for (x1, y1), (_x2, _y2) in lines:\n",
    "        cv2.circle(vis, (x1, y1), 1, (0, 255, 0), -1)\n",
    "    return vis\n",
    "\n",
    "def uvd_to_xyzrgb(uvd, K, imgs=None):\n",
    "    import numpy as np\n",
    "    from psegs import datum\n",
    "    \n",
    "    fx = K[0, 0]\n",
    "    cx = K[0, 2]\n",
    "    fy = K[1, 1]\n",
    "    cy = K[1, 2]\n",
    "    \n",
    "    rays = np.zeros((uvd.shape[0], 3))\n",
    "    rays[:, 0] = (uvd[:, 0] - cx) / fx\n",
    "    rays[:, 1] = (uvd[:, 1] - cy) / fy\n",
    "    rays[:, 2] = 1.\n",
    "    rays /= np.linalg.norm(rays, axis=-1)[:, np.newaxis]\n",
    "    xyz = uvd[:, 2][:, np.newaxis] * rays\n",
    "    \n",
    "    from psegs import datum\n",
    "    pc = datum.PointCloud(cloud=xyz)\n",
    "    cis = [datum.CameraImage(image_factory=lambda: img, K=K) for img in (imgs or [])]\n",
    "    xyzrgb = datum.PointCloud.paint_ego_cloud(xyz, camera_images=cis)\n",
    "    return xyzrgb\n",
    "\n",
    "def create_xyzrgb_3d_plot_html(xyzrgb, max_points=10000):\n",
    "    import plotly\n",
    "    import plotly.graph_objects as go\n",
    "    import pandas as pd\n",
    "\n",
    "    pcloud_df = pd.DataFrame(xyzrgb, columns=['x', 'y', 'z', 'r', 'g', 'b'])\n",
    "    pcloud_df = pcloud_df.sample(n=min(xyzrgb.shape[0], max_points))\n",
    "    scatter = go.Scatter3d(\n",
    "                x=pcloud_df['x'], y=pcloud_df['y'], z=pcloud_df['z'],\n",
    "                mode='markers',\n",
    "                marker=dict(size=3, color=pcloud_df[['r', 'g', 'b']], opacity=0.9))\n",
    "    fig = go.Figure(data=[scatter])\n",
    "    fig.update_layout(\n",
    "            width=900, height=600,\n",
    "            scene_camera=dict(\n",
    "                up=dict(x=0, y=-1, z=0),\n",
    "                center=dict(x=0, y=0, z=0),\n",
    "                eye=dict(x=1.25, y=-1.25, z=-1.25)\n",
    "            ),\n",
    "            scene_aspectmode='data')\n",
    "    \n",
    "#     trace0 = go.Scatter(\n",
    "#       x=[1, 2, 3, 4],\n",
    "#       y=[10, 15, 13, 17]\n",
    "#     )\n",
    "#     fig = go.Figure(data=[trace0])\n",
    "    \n",
    "    center = xyzrgb[:, :3].mean(axis=0)\n",
    "    footer = \"<i>Showing %s of %s points with mean (%s, %s, %s)</i>\" % (\n",
    "                    len(pcloud_df), xyzrgb.shape[0], center[0], center[1], center[2])\n",
    "    \n",
    "    return fig.to_html(include_plotlyjs=False, full_html=False) + '<br/><br/>' + footer\n",
    "#     fig_html = plotly.offline.plot(fig, include_plotlyjs=True, output_type='file', filename='/tmp/yay.html')\n",
    "    \n",
    "def create_xyzrgb_3d_sf_plot_html(xyzrgb1, xyzrgb2, max_points=5000):\n",
    "    import plotly\n",
    "    import plotly.graph_objects as go\n",
    "    import pandas as pd\n",
    "    \n",
    "    xyzrgbuvw = np.zeros((xyzrgb1.shape[0], 9))\n",
    "    xyzrgbuvw[:, :6] = xyzrgb1\n",
    "    xyzrgbuvw[:, 6:] = xyzrgb2[:, :3] - xyzrgb1[:, :3]\n",
    "    \n",
    "#     # Change the colors to make it easier to distinguish source from target\n",
    "#     xyzrgb1 = xyzrgb1.copy()\n",
    "#     xyzrgb1[:, (3, 4, 5)] *= 1.5\n",
    "#     xyzrgb2 = xyzrgb2.copy()\n",
    "#     xyzrgb2[:, (3, 4, 5)] *= 0.5\n",
    "    \n",
    "    pcloud_df = pd.DataFrame(xyzrgbuvw, columns=['x', 'y', 'z', 'r', 'g', 'b', 'u', 'v', 'w'])\n",
    "    pcloud_df = pcloud_df.sample(n=min(xyzrgbuvw.shape[0], max_points))\n",
    "    cones = go.Cone(\n",
    "                x=pcloud_df['x'], y=pcloud_df['y'], z=pcloud_df['z'],\n",
    "                u=pcloud_df['u'], v=pcloud_df['v'], w=pcloud_df['w'],\n",
    "                sizemode=\"scaled\",\n",
    "                sizeref=2,\n",
    "                colorscale='Blues')\n",
    "#     ,\n",
    "#                 marker=dict(size=3, color=pcloud_df[['r', 'g', 'b']], opacity=0.9))\n",
    "    \n",
    "    fig = go.Figure(data=[cones])\n",
    "    fig.update_layout(\n",
    "            width=900, height=600,\n",
    "            scene_camera=dict(\n",
    "                up=dict(x=0, y=-1, z=0),\n",
    "                center=dict(x=0, y=0, z=0),\n",
    "                eye=dict(x=1.25, y=-1.25, z=-1.25)\n",
    "            ),\n",
    "            scene_aspectmode='data')\n",
    "    return fig.to_html(include_plotlyjs=False, full_html=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return \"<iframe>\" + open('/tmp/yay.html').read() + \"</iframe>\"\n",
    "# #     return fig.to_html(full_html=False, include_plotlyjs=\"cdn\")\n",
    "# #     html = \"\"\"\n",
    "# #         <iframe>\n",
    "# #         <html><head>\n",
    "# #             <script type=\"text/javascript\">\n",
    "# #                 if (typeof require !== 'undefined') {{\n",
    "# #                 require.undef(\"plotly\");\n",
    "# #                 requirejs.config({{\n",
    "# #                     paths: {{\n",
    "# #                         'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
    "# #                     }}\n",
    "# #                 }});\n",
    "# #                 require(['plotly'], function(Plotly) {{\n",
    "# #                     window._Plotly = Plotly;\n",
    "# #                 }});\n",
    "# #                 }}\n",
    "# #         </script></head>\n",
    "# #         <body>\"\"\" + fig_html + \"\"\"\n",
    "# #         <div id=\"7979e646-13e6-4f44-8d32-d8effc3816df\" style=\"height: 525; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"7979e646-13e6-4f44-8d32-d8effc3816df\", [{\"x\": [1, 2, 3], \"y\": [3, 1, 6]}], {}, {\"showLink\": false, \"linkText\": \"\"})</script>\n",
    "# #         </body>        \n",
    "# #         </iframe>\n",
    "# #         \"\"\"\n",
    "# #     return html\n",
    "\n",
    "class FlowPairFactoryBase(object):\n",
    "    DATASET = ''\n",
    "\n",
    "    @classmethod\n",
    "    def list_fp_uris(cls, spark):\n",
    "        return []\n",
    "    \n",
    "    @classmethod\n",
    "    def get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        uris = [datum.URI.from_str(u) for u in uris]\n",
    "        uris = [u for u in uris if u.dataset == cls.DATASET]\n",
    "        if not uris:\n",
    "            return None\n",
    "        return cls._get_fp_rdd_for_uris(spark, uris)\n",
    "\n",
    "    @classmethod\n",
    "    def _get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        return None\n",
    "\n",
    "class FlowPairUnionFactory(FlowPairFactoryBase):\n",
    "    FACTORIES = []\n",
    "    \n",
    "    @classmethod\n",
    "    def list_fp_uris(cls, spark):\n",
    "        import itertools\n",
    "        return list(itertools.chain.from_iterable(F.list_fp_uris(spark) for F in cls.FACTORIES))\n",
    "    \n",
    "    @classmethod\n",
    "    def get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        rdds = []\n",
    "        for F in cls.FACTORIES:\n",
    "            rdd = F.get_fp_rdd_for_uris(spark, uris)\n",
    "            if rdd is not None:\n",
    "                rdds.append(rdd)\n",
    "        assert rdds, \"No RDDs for %s\" % uris\n",
    "        return spark.sparkContext.union(rdds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:11:47,935\toarph 1807391 : Source has changed! Rebuilding Egg ...\n",
      "2021-04-28 21:11:47,936\toarph 1807391 : Using source root /tmp/tmpekna3vk7_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis \n",
      "2021-04-28 21:11:47,936\toarph 1807391 : Using source root /tmp/tmpekna3vk7_cheap_optical_flow_eval_analysis \n",
      "2021-04-28 21:11:47,938\toarph 1807391 : Generating egg to /tmp/tmpr662tmbc_oarphpy_eggbuild ...\n",
      "2021-04-28 21:11:47,945\toarph 1807391 : ... done.  Egg at /tmp/tmpr662tmbc_oarphpy_eggbuild/cheap_optical_flow_eval_analysis-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from cheap_optical_flow_eval_analysis.ofp import *\n",
    "\n",
    "# # from plotly.offline import init_notebook_mode, iplot\n",
    "# # from plotly.graph_objs import *\n",
    "\n",
    "# # # init_notebook_mode(connected=False)         # initiate notebook for offline plot\n",
    "\n",
    "# # trace0 = Scatter(\n",
    "# #   x=[1, 2, 3, 4],\n",
    "# #   y=[10, 15, 13, 17]\n",
    "# # )\n",
    "# # trace1 = Scatter(\n",
    "# #   x=[1, 2, 3, 4],\n",
    "# #   y=[16, 5, 11, 9]\n",
    "# # )\n",
    "\n",
    "# # iplot([trace0, trace1])  \n",
    "\n",
    "# # from IPython.core.display import display, HTML\n",
    "# # omg finally!\n",
    "# show_html('''\n",
    "#             <script src='https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js'></script>\n",
    "#             <script>requirejs.config({\n",
    "#                 paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});\n",
    "#                 if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>\n",
    "#                 ''')\n",
    "\n",
    "# # import plotly\n",
    "# # plotly.offline.init_notebook_mode(connected=True)\n",
    "# show_html(fp.to_html())\n",
    "\n",
    "# # import plotly\n",
    "# # plotly.__version__\n",
    "# with open('/opt/psegs/tast.html', 'w') as f:\n",
    "#     f.write(fp.to_html())\n",
    "# # import IPython\n",
    "# # IPython.display.HTML(filename='/opt/psegs/tast.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Middlebury Optical Flow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO talk configs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cheap_optical_flow_eval_analysis/midd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cheap_optical_flow_eval_analysis/midd.py\n",
    "\n",
    "from psegs import datum\n",
    "\n",
    "from cheap_optical_flow_eval_analysis.ofp import *\n",
    "\n",
    "# Please unzip `other-color-allframes.zip` and `other-gt-flow.zip` to a directory and provide the target below:\n",
    "MIDD_DATA_ROOT = '/opt/psegs/ext_data/middlebury-flow/'\n",
    "\n",
    "# For the Middlebury Flow dataset, we only consider the real scenes\n",
    "MIDD_SCENES = [\n",
    "    {\n",
    "        'input': 'other-data/Dimetrodon/frame10.png',\n",
    "        'expected_out': 'other-data/Dimetrodon/frame11.png',\n",
    "        'flow_gt': 'other-gt-flow/Dimetrodon/flow10.flo',\n",
    "    },\n",
    "        {\n",
    "        'input': 'other-data/Hydrangea/frame10.png',\n",
    "        'expected_out': 'other-data/Hydrangea/frame11.png',\n",
    "        'flow_gt': 'other-gt-flow/Hydrangea/flow10.flo',\n",
    "    },\n",
    "        {\n",
    "        'input': 'other-data/RubberWhale/frame10.png',\n",
    "        'expected_out': 'other-data/RubberWhale/frame11.png',\n",
    "        'flow_gt': 'other-gt-flow/RubberWhale/flow10.flo',\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def midd_read_flow(path):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    # Based upon: https://github.com/Johswald/flow-code-python/blob/master/readFlowFile.py\n",
    "    # compute colored image to visualize optical flow file .flo\n",
    "    # Author: Johannes Oswald, Technical University Munich\n",
    "    # Contact: johannes.oswald@tum.de\n",
    "    # Date: 26/04/2017\n",
    "    # For more information, check http://vision.middlebury.edu/flow/ \n",
    "    assert os.path.exists(path) and path.endswith('.flo'), path\n",
    "    f = open(path, 'rb')\n",
    "    flo_number = np.fromfile(f, np.float32, count=1)[0]\n",
    "    TAG_FLOAT = 202021.25\n",
    "    assert flo_number == TAG_FLOAT, 'Flow number %r incorrect.' % flo_number\n",
    "    w = np.fromfile(f, np.int32, count=1)\n",
    "    h = np.fromfile(f, np.int32, count=1)\n",
    "\n",
    "    #if error try: data = np.fromfile(f, np.float32, count=2*w[0]*h[0])\n",
    "    data = np.fromfile(f, np.float32, count=int(2*w*h))\n",
    "\n",
    "    # Reshape data into 3D array (columns, rows, bands)\n",
    "    flow = np.resize(data, (int(h), int(w), 2))\t\n",
    "    f.close()\n",
    "\n",
    "    # We found that there are some invalid (?) (i.e. very large) flows, so we're going\n",
    "    # to ignore those for this experiment.\n",
    "    invalid = (flow >= 1666)\n",
    "    flow[invalid] = 0\n",
    "\n",
    "    return flow\n",
    "\n",
    "def midd_create_fp(uri):\n",
    "    scene_idx = int(uri.extra['midd.scene_idx'])\n",
    "    scene = MIDD_SCENES[scene_idx]\n",
    "    data_root = uri.extra['midd.dataroot']\n",
    "    return OpticalFlowPair(\n",
    "                uri=uri,\n",
    "                dataset=\"Middlebury Optical Flow\",\n",
    "                id1=scene['input'],\n",
    "                img1='file://' + os.path.join(data_root, scene['input']),\n",
    "                id2=scene['expected_out'],\n",
    "                img2='file://' + os.path.join(data_root, scene['expected_out']),\n",
    "                flow=CloudpickeledCallable(lambda: midd_read_flow(os.path.join(data_root, scene['flow_gt']))))\n",
    "    \n",
    "\n",
    "class MiddFactory(FlowPairFactoryBase):\n",
    "    DATASET = 'midd_oflow'\n",
    "    \n",
    "    @classmethod\n",
    "    def list_fp_uris(cls, spark):\n",
    "        return [\n",
    "            datum.URI(dataset=cls.DATASET, extra={'midd.scene_idx': i, 'midd.dataroot': MIDD_DATA_ROOT})\n",
    "            for i, scene in enumerate(MIDD_SCENES)\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        uri_rdd = spark.sparkContext.parallelize(uris)\n",
    "        fp_rdd = uri_rdd.map(midd_create_fp)\n",
    "        return fp_rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:11:48,035\toarph 1807391 : Source has changed! Rebuilding Egg ...\n",
      "2021-04-28 21:11:48,035\toarph 1807391 : Using source root /tmp/tmpekna3vk7_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis \n",
      "2021-04-28 21:11:48,036\toarph 1807391 : Using source root /tmp/tmpekna3vk7_cheap_optical_flow_eval_analysis \n",
      "2021-04-28 21:11:48,037\toarph 1807391 : Generating egg to /tmp/tmp7q4rpnel_oarphpy_eggbuild ...\n",
      "2021-04-28 21:11:48,044\toarph 1807391 : ... done.  Egg at /tmp/tmp7q4rpnel_oarphpy_eggbuild/cheap_optical_flow_eval_analysis-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from cheap_optical_flow_eval_analysis.midd import MiddFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 Midd scenes\n"
     ]
    }
   ],
   "source": [
    "ALL_FP_FACTORY_CLSS.append(MiddFactory)\n",
    "\n",
    "print(\"Found %s Midd scenes\" % len(MiddFactory.list_fp_uris(spark)))\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    demo_uris = MiddFactory.list_fp_uris(spark)\n",
    "    fp_rdd = MiddFactory.get_fp_rdd_for_uris(spark, demo_uris)\n",
    "    fps = fp_rdd.collect()\n",
    "    \n",
    "    for fp in fps:\n",
    "        show_html(fp.to_html() + \"<br/><br/><br/>\")\n",
    "        DEMO_FPS.append(fp)\n",
    "\n",
    "# for i, scene in enumerate(MIDD_SCENES):\n",
    "#     p = OpticalFlowPair(\n",
    "#             dataset=\"Middlebury Optical Flow\",\n",
    "#             id1=scene['input'],\n",
    "#             img1='file://' + os.path.join(MIDD_DATA_ROOT, scene['input']),\n",
    "#             id2=scene['expected_out'],\n",
    "#             img2='file://' + os.path.join(MIDD_DATA_ROOT, scene['expected_out']),\n",
    "#             flow=CloudpickeledCallable(lambda: midd_read_flow(os.path.join(MIDD_DATA_ROOT, scene['flow_gt']))))\n",
    "    \n",
    "#     if RUN_FULL_ANALYSIS:\n",
    "#         ALL_FPS.append(copy.deepcopy(p))\n",
    "    \n",
    "#     if SHOW_DEMO_OUTPUT:\n",
    "#         show_html(p.to_html() + \"<br/><br/><br/>\")\n",
    "#         DEMO_FPS.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepDeform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cheap_optical_flow_eval_analysis/deepdeform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cheap_optical_flow_eval_analysis/deepdeform.py\n",
    "\n",
    "from psegs import datum\n",
    "\n",
    "from cheap_optical_flow_eval_analysis.ofp import *\n",
    "\n",
    "# Please extract deepdeform_v1.7z to a directory and provide the target below:\n",
    "DD_DATA_ROOT = '/opt/psegs/ext_data/deepdeform_v1/'\n",
    "\n",
    "def dd_load_raw_flow(path):\n",
    "    # Based upon https://github.com/AljazBozic/DeepDeform/blob/master/utils.py#L1\n",
    "    import shutil\n",
    "    import struct\n",
    "    import os\n",
    "    import numpy as np\n",
    "\n",
    "    # Flow is stored row-wise in order [channels, height, width].\n",
    "    assert os.path.isfile(path), path\n",
    "\n",
    "    flow_gt = None\n",
    "    with open(path, 'rb') as fin:\n",
    "        width = struct.unpack('I', fin.read(4))[0]\n",
    "        height = struct.unpack('I', fin.read(4))[0]\n",
    "        channels = struct.unpack('I', fin.read(4))[0]\n",
    "        n_elems = height * width * channels\n",
    "\n",
    "        flow = struct.unpack('f' * n_elems, fin.read(n_elems * 4))\n",
    "        raw_flow_gt = np.asarray(flow, dtype=np.float32).reshape([channels, height, width])\n",
    "    return raw_flow_gt\n",
    "\n",
    "def dd_load_oflow(path):\n",
    "    raw_flow_gt = dd_load_raw_flow(path)\n",
    "\n",
    "    # Match format used in this analysis\n",
    "    flow_gt = np.moveaxis(raw_flow_gt, 0, -1) # (h, w, 2)\n",
    "    invalid_flow = flow_gt == -np.Inf\n",
    "    flow_gt[invalid_flow] = 0.0\n",
    "    return flow_gt\n",
    "\n",
    "def dd_load_depth_meters(path):\n",
    "    import imageio\n",
    "    \n",
    "    # \"Every pixel contains 3 values for flow in x, y and z direction, in meters\"\n",
    "    depth_img_raw = imageio.imread(path)\n",
    "    d_meters = depth_img_raw.astype('float64') / 1000.\n",
    "    return d_meters\n",
    "\n",
    "def dd_load_sflow(sflow_path):\n",
    "    # NB: we actually ignore the the DeepDeform SceneFlow data since it appears to be\n",
    "    # deduced from the optical flow / visual point correspondence.  So we just\n",
    "    # do the same deduction but capture the Scene Flow in uvd form; Deep\n",
    "    # Deform has it in (x, y, z) [meters]\n",
    "    raw_flow = dd_load_raw_flow(sflow_path.replace('.sflow', '.oflow').replace('scene_flow', 'optical_flow'))\n",
    "    raw_flow = np.moveaxis(raw_flow, 0, -1) # (h, w, 2)\n",
    "    \n",
    "    # File name format: {obj_id}_{src_frame}_{dest_frame}.sflow\n",
    "    obj_id, src_id, target_id = os.path.basename(sflow_path).replace('.sflow', '').split('_')\n",
    "    \n",
    "    # So we need the depth for frame 1 at least\n",
    "    depth_path1 = os.path.join(os.path.dirname(sflow_path), '../depth/%s.png' % src_id)\n",
    "    depth_path2 = os.path.join(os.path.dirname(sflow_path), '../depth/%s.png' % target_id)\n",
    "    \n",
    "    d1 = dd_load_depth_meters(depth_path1)\n",
    "    d2 = dd_load_depth_meters(depth_path2)\n",
    "    \n",
    "    h, w = d1.shape[:2]\n",
    "    px_y = np.tile(np.arange(h)[:, np.newaxis], [1, w])\n",
    "    px_x = np.tile(np.arange(w)[np.newaxis, :], [h, 1])\n",
    "    pyx = np.concatenate([px_y[:,:,np.newaxis], px_x[:, :, np.newaxis]], axis=-1)\n",
    "    pyx = pyx.astype(np.float32)\n",
    "    \n",
    "    vud1 = np.dstack([pyx, d1]).reshape([-1, 3])\n",
    "    uvdviz_im1 = np.zeros((vud1.shape[0], 4))\n",
    "    uvdviz_im1[:, :3] = vud1[:, (1, 0, 2)]\n",
    "    uvdviz_im1[:, -1] = np.logical_and(\n",
    "                            (raw_flow != -np.Inf).reshape([-1, 2])[:, 0], # Flow is valid\n",
    "                            uvdviz_im1[:, 2] > 0)                        # Depth is valid\n",
    "    \n",
    "    vu2 = (pyx + raw_flow[:, :, (1, 0)]).reshape([-1, 2])\n",
    "    invalid = np.where(\n",
    "            (np.rint(vu2[:, 0]) < 0) | (np.rint(vu2[:, 0]) >= h) |\n",
    "            (np.rint(vu2[:, 1]) < 0) | (np.rint(vu2[:, 1]) >= w) |\n",
    "            (vu2[:, 0] == -np.Inf))\n",
    "    j2 = np.rint(vu2[:, 0]).astype(np.int64)\n",
    "    i2 = np.rint(vu2[:, 1]).astype(np.int64)\n",
    "    j2[invalid] = 0\n",
    "    i2[invalid] = 0\n",
    "    d2_col = d2[j2, i2]\n",
    "    vud2 = np.hstack([vu2, d2_col[:, np.newaxis]])\n",
    "    \n",
    "    uvdviz_im2 = np.ones((vud1.shape[0], 4))\n",
    "    uvdviz_im2[:, :3] = vud2[:, (1, 0, 2)]\n",
    "    uvdviz_im2[invalid, -1] = 0\n",
    "    \n",
    "#     vudviz_im2[:, -1] = (vudviz_im2[:, 0] != -np.Inf)\n",
    "#     vudviz_im1[:, -1] = np.logical_and(vudviz_im1[:, -1], (vudviz_im1[:, 2] > 0))\n",
    "    \n",
    "    visible_either = ((uvdviz_im1[:, -1] == 1) | (uvdviz_im2[:, -1] == 1))\n",
    "    uvdviz_im1 = uvdviz_im1[visible_either]\n",
    "    uvdviz_im2 = uvdviz_im2[visible_either]\n",
    "#         xyz1 = uvd_to_xyzrgb(uvd1, fp.K)[:, :3]\n",
    "#         xyz2 = uvd_to_xyzrgb(uvd2, fp.K)[:, :3]     \n",
    "    \n",
    "    return uvdviz_im1, uvdviz_im2\n",
    "\n",
    "\n",
    "# def dd_load_sflow(path):\n",
    "#     # Based upon https://github.com/AljazBozic/DeepDeform/blob/master/utils.py#L1\n",
    "#     import shutil\n",
    "#     import struct\n",
    "#     import os\n",
    "#     import numpy as np\n",
    "#     import imageio\n",
    "\n",
    "#     # Scene Flow is stored row-wise in order [channels (x, y, z), height, width].\n",
    "#     assert os.path.isfile(path)\n",
    "\n",
    "#     flow_gt = None\n",
    "#     with open(path, 'rb') as fin:\n",
    "#         width = struct.unpack('I', fin.read(4))[0]\n",
    "#         height = struct.unpack('I', fin.read(4))[0]\n",
    "#         channels = struct.unpack('I', fin.read(4))[0]\n",
    "#         n_elems = height * width * channels\n",
    "#         flow = struct.unpack('f' * n_elems, fin.read(n_elems * 4))\n",
    "#         flow_gt = np.asarray(flow, dtype=np.float32).reshape([channels, height, width])\n",
    "\n",
    "#     sflow_gt = np.moveaxis(flow_gt, 0, -1) # (h, w, 3)\n",
    "        \n",
    "#     # \"Every pixel contains 3 values for flow in x, y and z direction, in meters\"\n",
    "#     # So we need the depth for frame 1 at least\n",
    "#     obj_id, src_id, target_id = os.path.basename(path).replace('.sflow', '').split('_')\n",
    "#     depth_path = os.path.join(os.path.dirname(path), '../depth/%s.png' % src_id)\n",
    "#     depth_img_raw = imageio.imread(depth_path)\n",
    "    \n",
    "#     # \"depth images as 16-bit .png (divide by 1000 to obtain depth in meters)\"\n",
    "#     d_uvd_meters = depth_img_raw.astype('float64') / 1000.\n",
    "        \n",
    "#     sflow_uv_dxyz = np.dstack([d_uvd_meters[:, :, np.newaxis], sflow_gt])\n",
    "#     return sflow_uv_dxyz\n",
    "\n",
    "\n",
    "# def dd_convert_sflow(K, sflow_uv_dxyz):\n",
    "#     import numpy as np\n",
    "\n",
    "#     h, w = sflow_uv_dxyz.shape[:2]\n",
    "#     px_y = np.tile(np.arange(h)[:, np.newaxis], [1, w])\n",
    "#     px_x = np.tile(np.arange(w)[np.newaxis, :], [h, 1])\n",
    "#     pxy = np.concatenate([px_y[:,:,np.newaxis], px_x[:, :, np.newaxis]], axis=-1)\n",
    "\n",
    "    \n",
    "#     sflow_uv_ijdxyz = np.dstack([pxy, sflow_uv_dxyz])\n",
    "#     sflow_ijdxyz = sflow_uv_ijdxyz.reshape([-1, 6])\n",
    "    \n",
    "#     # Trim invalid / invisible\n",
    "#     sflow_ijdxyz = sflow_ijdxyz[sflow_ijdxyz[:, -1] != -np.Inf]\n",
    "\n",
    "#     uvdviz_im1 = np.ones((sflow_ijdxyz.shape[0], 4))\n",
    "#     uvdviz_im1[:, 0] = sflow_ijdxyz[:, 1]\n",
    "#     uvdviz_im1[:, 1] = sflow_ijdxyz[:, 0]\n",
    "#     uvdviz_im1[:, 2] = sflow_ijdxyz[:, 2]\n",
    "\n",
    "#     fx, fy, cx, cy = K[0, 0], K[1, 1], K[0, 2], K[1, 2]\n",
    "#     rays = np.zeros((h, w, 3))\n",
    "#     rays[:, :, 0] = (rays[:, :, 0] - cy) / fy\n",
    "#     rays[:, :, 1] = (rays[:, :, 1] - cx) / fx\n",
    "#     rays[:, :, 2] = 1\n",
    "#     rays /= np.linalg.norm(rays, axis=-1)[:, :, np.newaxis]\n",
    "\n",
    "#     yxz_1 = rays * sflow_uv_dxyz[:, :, 0][:, :, np.newaxis]\n",
    "#     yxz_2 = yxz_1 + sflow_uv_dxyz[:, :, (2, 1, 3)]\n",
    "#     xyz_2 = yxz_2[:, :, (1, 0, 2)]\n",
    "    \n",
    "#     xyz_2_valid = xyz_2.reshape([-1, 3])\n",
    "#     xyz_2_valid = xyz_2_valid[xyz_2_valid[:, 0] != -np.Inf]\n",
    "    \n",
    "#     # nawwww use the oflow look at the paper they prolly are inferring SF from oflow and depth \n",
    "    \n",
    "#     # DeepDeform scene flow is always visible -> visible\n",
    "#     assert xyz_2_valid.shape[0] == uvdviz_im1.shape[0], (xyz_2_valid.shape, uvdviz_im1.shape)\n",
    "    \n",
    "#     uvd2 = K[:3, :3].dot(xyz_2_valid.T)\n",
    "#     uvd2[0:2, :] /= uvd2[2, :]\n",
    "#     uvd2 = uvd2.T\n",
    "    \n",
    "#     uvdviz_im2 = np.ones((xyz_2_valid.shape[0], 4))\n",
    "#     uvdviz_im2[:, 0:3] = uvd2\n",
    "    \n",
    "    \n",
    "# #     uvdviz_im2[:, 0] = (fx * xyz_2_valid[:, 0]) / (xyz_2_valid[:, 2] + cx)\n",
    "# #     uvdviz_im2[:, 1] = (fy * xyz_2_valid[:, 1]) / (xyz_2_valid[:, 2] + cy)\n",
    "# #     uvdviz_im2[:, 2] = np.linalg.norm(xyz_2_valid, axis=1)\n",
    "    \n",
    "#     return uvdviz_im1, uvdviz_im2\n",
    "\n",
    "def dd_create_fp(uri):\n",
    "    if \"dd.sf_gt\" in uri.extra:\n",
    "        K = dd_read_K(os.path.join(DD_DATA_ROOT, uri.extra['dd.K']))\n",
    "        uvdviz_im1, uvdviz_im2 = dd_load_sflow(os.path.join(DD_DATA_ROOT, uri.extra[\"dd.sf_gt\"]))\n",
    "    else:\n",
    "        K = None\n",
    "        uvdviz_im1 = None\n",
    "        uvdviz_im2 = None\n",
    "    return OpticalFlowPair(\n",
    "                uri=uri,\n",
    "                dataset=\"DeepDeform Semi-Synthetic Optical Flow\",\n",
    "                id1=uri.extra['dd.input'],\n",
    "                img1='file://' + os.path.join(DD_DATA_ROOT, uri.extra['dd.input']),\n",
    "                id2=uri.extra['dd.expected_out'],\n",
    "                img2='file://' + os.path.join(DD_DATA_ROOT, uri.extra['dd.expected_out']),\n",
    "                flow=dd_load_oflow(os.path.join(DD_DATA_ROOT, uri.extra['dd.flow_gt'])),\n",
    "        \n",
    "                K=K[:3, :3],\n",
    "                uvdviz_im1=uvdviz_im1,\n",
    "                uvdviz_im2=uvdviz_im2)\n",
    "\n",
    "def dd_read_K(path):\n",
    "    import numpy as np\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    lines = [l for l in lines if l]\n",
    "    K = np.array([[float(ll) for ll in l.split(' ') if ll] for l in lines])\n",
    "    return K\n",
    "\n",
    "class DDFactory(FlowPairFactoryBase):\n",
    "    DATASET = 'deep_deform'\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_all_scenes(cls):\n",
    "        import json\n",
    "        DD_ALIGNMENTS = json.load(open(os.path.join(DD_DATA_ROOT, 'train_alignments.json')))\n",
    "        ALL_DD_SCENES = [\n",
    "            {\n",
    "                \"dd.input\": ascene['source_color'],\n",
    "                \"dd.expected_out\": ascene['target_color'],\n",
    "                \"dd.flow_gt\": ascene['optical_flow'],\n",
    "                \"dd.sf_gt\": ascene['scene_flow'],\n",
    "                \"dd.K\": os.path.join(os.path.dirname(ascene['scene_flow']), '../intrinsics.txt'),\n",
    "            }\n",
    "            for ascene in DD_ALIGNMENTS\n",
    "        ]\n",
    "        return ALL_DD_SCENES\n",
    "    \n",
    "    @classmethod\n",
    "    def list_fp_uris(cls, spark):\n",
    "        scenes = cls._get_all_scenes()\n",
    "        return [\n",
    "            datum.URI(dataset=cls.DATASET, extra=scene)\n",
    "            for scene in scenes\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        uri_rdd = spark.sparkContext.parallelize(uris)\n",
    "        fp_rdd = uri_rdd.map(dd_create_fp)\n",
    "        return fp_rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 15:38:47,827\toarph 723307 : Source has changed! Rebuilding Egg ...\n",
      "2021-04-28 15:38:47,828\toarph 723307 : Using source root /tmp/tmpcbogta6w_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis \n",
      "2021-04-28 15:38:47,828\toarph 723307 : Using source root /tmp/tmpcbogta6w_cheap_optical_flow_eval_analysis \n",
      "2021-04-28 15:38:47,830\toarph 723307 : Generating egg to /tmp/tmp3c2y9mgb_oarphpy_eggbuild ...\n",
      "2021-04-28 15:38:47,836\toarph 723307 : ... done.  Egg at /tmp/tmp3c2y9mgb_oarphpy_eggbuild/cheap_optical_flow_eval_analysis-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from cheap_optical_flow_eval_analysis.deepdeform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4540 DeepDeform scenes\n"
     ]
    }
   ],
   "source": [
    "from psegs import datum\n",
    "\n",
    "DD_DEMO_URIS = [\n",
    "    datum.URI(dataset=DDFactory.DATASET, extra={\n",
    "        \"dd.input\": \"train/seq000/color/000000.jpg\",\n",
    "        \"dd.expected_out\": \"train/seq000/color/000200.jpg\",\n",
    "        \"dd.flow_gt\": \"train/seq000/optical_flow/blackdog_000000_000200.oflow\",\n",
    "        \"dd.sf_gt\": \"train/seq000/scene_flow/blackdog_000000_000200.sflow\",\n",
    "        \"dd.K\": \"train/seq000/intrinsics.txt\",\n",
    "    }),\n",
    "    datum.URI(dataset=DDFactory.DATASET, extra={\n",
    "        \"dd.input\": \"train/seq000/color/000000.jpg\",\n",
    "        \"dd.expected_out\": \"train/seq000/color/001200.jpg\",\n",
    "        \"dd.flow_gt\": \"train/seq000/optical_flow/blackdog_000000_001200.oflow\",\n",
    "        \"dd.sf_gt\": \"train/seq000/scene_flow/blackdog_000000_001200.sflow\",\n",
    "        \"dd.K\": \"train/seq000/intrinsics.txt\",\n",
    "    }),\n",
    "    datum.URI(dataset=DDFactory.DATASET, extra={\n",
    "        \"dd.input\": \"train/seq001/color/003400.jpg\",\n",
    "        \"dd.expected_out\": \"train/seq001/color/003600.jpg\",\n",
    "        \"dd.flow_gt\": \"train/seq001/optical_flow/lady_003400_003600.oflow\",\n",
    "        \"dd.sf_gt\": \"train/seq001/scene_flow/lady_003400_003600.sflow\",\n",
    "        \"dd.K\": \"train/seq001/intrinsics.txt\",\n",
    "    }),\n",
    "    datum.URI(dataset=DDFactory.DATASET, extra={\n",
    "        \"dd.input\": \"train/seq337/color/000050.jpg\",\n",
    "        \"dd.expected_out\": \"train/seq337/color/000350.jpg\",\n",
    "        \"dd.flow_gt\": \"train/seq337/optical_flow/adult_000050_000350.oflow\",\n",
    "        \"dd.sf_gt\": \"train/seq337/scene_flow/adult_000050_000350.sflow\",\n",
    "        \"dd.K\": \"train/seq337/intrinsics.txt\",\n",
    "    }),\n",
    "]\n",
    "\n",
    "ALL_FP_FACTORY_CLSS.append(DDFactory)\n",
    "\n",
    "print(\"Found %s DeepDeform scenes\" % len(DDFactory.list_fp_uris(spark)))\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    fp_rdd = DDFactory.get_fp_rdd_for_uris(spark, DD_DEMO_URIS)\n",
    "    fps = fp_rdd.collect()\n",
    "    \n",
    "    for fp in fps:\n",
    "        show_html(fp.to_html() + \"<br/><br/><br/>\")\n",
    "        DEMO_FPS.append(fp)\n",
    "\n",
    "\n",
    "\n",
    "# import json\n",
    "# DD_ALIGNMENTS = json.load(open(os.path.join(DD_DATA_ROOT, 'train_alignments.json')))\n",
    "# ALL_DD_SCENES = [\n",
    "#     {\n",
    "#         \"input\": ascene['source_color'],\n",
    "#         \"expected_out\": ascene['target_color'],\n",
    "#         \"flow_gt\": ascene['optical_flow'],\n",
    "#     }\n",
    "#     for ascene in DD_ALIGNMENTS\n",
    "# ]\n",
    "\n",
    "# print(\"Found %s DeepDeform scenes\" % len(ALL_DD_SCENES))\n",
    "# if SHOW_DEMO_OUTPUT:\n",
    "#     for scene in DD_DEMO_SCENES:\n",
    "#         p = dd_create_fp(scene)\n",
    "#         show_html(p.to_html())\n",
    "#         DEMO_FPS.append(p)\n",
    "\n",
    "# if RUN_FULL_ANALYSIS:\n",
    "#     for scene in ALL_DD_SCENES:\n",
    "#         p = dd_create_fp(scene)\n",
    "#         ALL_FPS.append(p)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kitti Scene Flow Benchmark (2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Please unzip `data_scene_flow.zip` and `data_scene_flow_calib.zip` to a directory and provide that target below:\n",
    "# KITTI_SF15_DATA_ROOT = '/opt/psegs/ext_data/kitti_scene_flow_2015/'\n",
    "\n",
    "\n",
    "\n",
    "# from oarphpy import util as oputil\n",
    "# KITTI_SF15_ALL_FLOW_OCC = [\n",
    "#     os.path.basename(p)\n",
    "#     for p in oputil.all_files_recursive(\n",
    "#         os.path.join(KITTI_SF15_DATA_ROOT, 'training/flow_occ'), pattern='*.png')\n",
    "# ]\n",
    "    \n",
    "# KITTI_SF15_ALL_SCENES = [\n",
    "#     {\n",
    "#         \"input\": 'training/image_2/%s' % fname,\n",
    "#         \"expected_out\": 'training/image_2/%s' % fname.replace('_10', '_11'),\n",
    "#         \"flow_gt\": 'training/flow_occ/%s' % fname,\n",
    "#     }\n",
    "#     for fname in KITTI_SF15_ALL_FLOW_OCC\n",
    "# ]\n",
    "# print(\"Found %s KITTI SceneFlow 2015 scenes\" % len(KITTI_SF15_ALL_SCENES))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cheap_optical_flow_eval_analysis/kittisf15.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cheap_optical_flow_eval_analysis/kittisf15.py\n",
    "\n",
    "from psegs import datum\n",
    "\n",
    "from cheap_optical_flow_eval_analysis.ofp import *\n",
    "\n",
    "# Please unzip `data_scene_flow.zip` and `data_scene_flow_calib.zip` to a directory and provide that target below:\n",
    "KITTI_SF15_DATA_ROOT = '/opt/psegs/ext_data/kitti_scene_flow_2015/'\n",
    "\n",
    "\n",
    "def kittisf15_load_flow(path):\n",
    "    # Based upon https://github.com/liruoteng/OpticalFlowToolkit/blob/master/lib/flowlib.py#L559\n",
    "    import png\n",
    "    import numpy as np\n",
    "    flow_object = png.Reader(filename=path)\n",
    "    flow_direct = flow_object.asDirect()\n",
    "    flow_data = list(flow_direct[2])\n",
    "    w, h = flow_direct[3]['size']\n",
    "    flow = np.zeros((h, w, 3), dtype=np.float64)\n",
    "    for i in range(len(flow_data)):\n",
    "        flow[i, :, 0] = flow_data[i][0::3]\n",
    "        flow[i, :, 1] = flow_data[i][1::3]\n",
    "        flow[i, :, 2] = flow_data[i][2::3]\n",
    "\n",
    "    invalid_idx = (flow[:, :, 2] == 0)\n",
    "    flow[:, :, 0:2] = (flow[:, :, 0:2] - 2 ** 15) / 64.0\n",
    "    flow[invalid_idx, 0] = 0\n",
    "    flow[invalid_idx, 1] = 0\n",
    "    return flow[:, :, :2]\n",
    "\n",
    "def kittisf15_load_disp(disp_path):\n",
    "    import imageio\n",
    "    \n",
    "    # From KITTI SF Devkit:\n",
    "    # \"Disparity maps are saved as uint16 PNG images, which can be opened with\n",
    "    # either MATLAB or libpng++. A 0 value indicates an invalid pixel (ie, no\n",
    "    # ground truth exists, or the estimation algorithm didn't produce an estimate\n",
    "    # for that pixel). Otherwise, the disparity for a pixel can be computed by\n",
    "    # converting the uint16 value to float and dividing it by 256.0\"\n",
    "\n",
    "    img = imageio.imread(disp_path)\n",
    "    disp = img.astype('float32') / 256.\n",
    "    return disp\n",
    "\n",
    "def kittisf15_load_K_baseline(cam_to_cam_path):\n",
    "    import numpy as np\n",
    "    \n",
    "    K_line = None\n",
    "    T_00_line = None\n",
    "    T_01_line = None\n",
    "    with open(cam_to_cam_path, 'r') as f:\n",
    "        for l in f.readlines():\n",
    "            if 'P_rect_02' in l:\n",
    "                K_line = l\n",
    "            if 'T_02' in l:\n",
    "                T_00_line = l\n",
    "            if 'T_03' in l:\n",
    "                T_01_line = l\n",
    "    \n",
    "    assert K_line\n",
    "    params = K_line.split('P_rect_02: ')[-1]\n",
    "    params = [float(tok.strip()) for tok in params.split(' ') if tok]\n",
    "    K = np.array(params).reshape([3, 4])\n",
    "    K = K[:3, :3]\n",
    "    \n",
    "    assert T_00_line\n",
    "    assert T_01_line\n",
    "    params = T_00_line.split('T_02: ')[-1]\n",
    "    params = [float(tok.strip()) for tok in params.split(' ') if tok]\n",
    "    T_00 = np.array(params)\n",
    "    params = T_01_line.split('T_03: ')[-1]\n",
    "    params = [float(tok.strip()) for tok in params.split(' ') if tok]\n",
    "    T_01 = np.array(params)\n",
    "    baseline = np.linalg.norm(T_00 - T_01)\n",
    "    \n",
    "    return K, baseline\n",
    "\n",
    "def kittisf15_load_sflow(flow, K, baseline, disp0_path, disp1_path):\n",
    "    fx = K[0, 0]\n",
    "    \n",
    "    disp0 = kittisf15_load_disp(disp0_path)\n",
    "    disp0_valid = disp0[:, :] > 0\n",
    "    d0 = fx * baseline / (disp0 + 1e-5)\n",
    "    d0[~disp0_valid] = 0\n",
    "    \n",
    "    disp1 = kittisf15_load_disp(disp1_path)\n",
    "    disp1_valid = disp1[:, :] > 0\n",
    "    d1 = fx * baseline / (disp1 + 1e-5)\n",
    "    d1[~disp1_valid] = 0\n",
    "    \n",
    "    h, w = d1.shape[:2]\n",
    "    px_y = np.tile(np.arange(h)[:, np.newaxis], [1, w])\n",
    "    px_x = np.tile(np.arange(w)[np.newaxis, :], [h, 1])\n",
    "    pyx = np.concatenate([px_y[:,:,np.newaxis], px_x[:, :, np.newaxis]], axis=-1)\n",
    "    pyx = pyx.astype(np.float32)\n",
    "    \n",
    "    vud1 = np.dstack([pyx, d0]).reshape([-1, 3])\n",
    "    uvdviz_im1 = np.zeros((vud1.shape[0], 4))\n",
    "    uvdviz_im1[:, :3] = vud1[:, (1, 0, 2)]\n",
    "    uvdviz_im1[:, -1] = np.logical_and(\n",
    "                            (flow > 0).reshape([-1, 2])[:, 0], # Flow is valid\n",
    "                            (d0 > 0).reshape([-1]))            # Depth is valid\n",
    "\n",
    "    vu2 = (pyx + flow[:, :, (1, 0)]).reshape([-1, 2])\n",
    "    d2_valid = (d1 > 0).reshape([-1])\n",
    "    invalid = np.where(\n",
    "            (np.rint(vu2[:, 0]) < 0) | (np.rint(vu2[:, 0]) >= h) |\n",
    "            (np.rint(vu2[:, 1]) < 0) | (np.rint(vu2[:, 1]) >= w) |\n",
    "            (flow[:, :, 0] == 0).reshape([-1]) |\n",
    "            (~d2_valid))\n",
    "    j2 = np.rint(vu2[:, 0]).astype(np.int64)\n",
    "    i2 = np.rint(vu2[:, 1]).astype(np.int64)\n",
    "    j2[invalid] = 0\n",
    "    i2[invalid] = 0\n",
    "    d2_col = d1[j2, i2]\n",
    "    vud2 = np.hstack([vu2, d2_col[:, np.newaxis]])\n",
    "    \n",
    "    uvdviz_im2 = np.ones((vud1.shape[0], 4))\n",
    "    uvdviz_im2[:, :3] = vud2[:, (1, 0, 2)]\n",
    "    uvdviz_im2[invalid, -1] = 0\n",
    "    \n",
    "#     vudviz_im2[:, -1] = (vudviz_im2[:, 0] != -np.Inf)\n",
    "#     vudviz_im1[:, -1] = np.logical_and(vudviz_im1[:, -1], (vudviz_im1[:, 2] > 0))\n",
    "    \n",
    "    visible_either = ((uvdviz_im1[:, -1] == 1) | (uvdviz_im2[:, -1] == 1))\n",
    "    uvdviz_im1 = uvdviz_im1[visible_either]\n",
    "    uvdviz_im2 = uvdviz_im2[visible_either]\n",
    "#         xyz1 = uvd_to_xyzrgb(uvd1, fp.K)[:, :3]\n",
    "#         xyz2 = uvd_to_xyzrgb(uvd2, fp.K)[:, :3]     \n",
    "    \n",
    "    return uvdviz_im1, uvdviz_im2\n",
    "\n",
    "\n",
    "def kittisf15_create_fp(uri):\n",
    "    flow = kittisf15_load_flow(os.path.join(KITTI_SF15_DATA_ROOT, uri.extra['ksf15.flow_gt']))\n",
    "    K, baseline = kittisf15_load_K_baseline(os.path.join(KITTI_SF15_DATA_ROOT, uri.extra['ksf15.K']))\n",
    "    uvdviz_im1, uvdviz_im2 = kittisf15_load_sflow(\n",
    "                                    flow, K, baseline,\n",
    "                                    os.path.join(KITTI_SF15_DATA_ROOT, uri.extra['ksf15.disp0']),\n",
    "                                    os.path.join(KITTI_SF15_DATA_ROOT, uri.extra['ksf15.disp1']))\n",
    "    \n",
    "    return OpticalFlowPair(\n",
    "                uri=uri,\n",
    "                dataset=\"KITTI Scene Flow 2015\",\n",
    "                id1=uri.extra['ksf15.input'],\n",
    "                img1='file://' + os.path.join(KITTI_SF15_DATA_ROOT, uri.extra['ksf15.input']),\n",
    "                id2=uri.extra['ksf15.expected_out'],\n",
    "                img2='file://' + os.path.join(KITTI_SF15_DATA_ROOT, uri.extra['ksf15.expected_out']),\n",
    "                flow=flow,\n",
    "        \n",
    "                K=K,\n",
    "                uvdviz_im1=uvdviz_im1,\n",
    "                uvdviz_im2=uvdviz_im2)\n",
    "\n",
    "\n",
    "class KITTISF15Factory(FlowPairFactoryBase):\n",
    "    DATASET = 'kitti_sf15'\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_all_scenes(cls):\n",
    "        from oarphpy import util as oputil\n",
    "        KITTI_SF15_ALL_FLOW_OCC = [\n",
    "            os.path.basename(p)\n",
    "            for p in oputil.all_files_recursive(\n",
    "                os.path.join(KITTI_SF15_DATA_ROOT, 'training/flow_occ'), pattern='*.png')\n",
    "        ]\n",
    "\n",
    "        KITTI_SF15_ALL_SCENES = [\n",
    "            {\n",
    "                \"ksf15.input\": 'training/image_2/%s' % fname,\n",
    "                \"ksf15.expected_out\": 'training/image_2/%s' % fname.replace('_10', '_11'),\n",
    "                \"ksf15.flow_gt\": 'training/flow_occ/%s' % fname,\n",
    "                \"ksf15.disp0\": 'training/disp_occ_0/%s' % fname,\n",
    "                \"ksf15.disp1\": 'training/disp_occ_1/%s' % fname,\n",
    "                \"ksf15.K\": 'training/calib_cam_to_cam/%s' % fname.replace('_10.png', '.txt'),\n",
    "            }\n",
    "            for fname in KITTI_SF15_ALL_FLOW_OCC\n",
    "        ]\n",
    "        return KITTI_SF15_ALL_SCENES\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def list_fp_uris(cls, spark):\n",
    "        scenes = cls._get_all_scenes()\n",
    "        return [\n",
    "            datum.URI(dataset=cls.DATASET, extra=scene)\n",
    "            for scene in scenes\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        uri_rdd = spark.sparkContext.parallelize(uris)\n",
    "        fp_rdd = uri_rdd.map(kittisf15_create_fp)\n",
    "        return fp_rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 15:39:03,569\toarph 723307 : Source has changed! Rebuilding Egg ...\n",
      "2021-04-28 15:39:03,570\toarph 723307 : Using source root /tmp/tmpcbogta6w_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis \n",
      "2021-04-28 15:39:03,571\toarph 723307 : Using source root /tmp/tmpcbogta6w_cheap_optical_flow_eval_analysis \n",
      "2021-04-28 15:39:03,572\toarph 723307 : Generating egg to /tmp/tmp7d55fgjb_oarphpy_eggbuild ...\n",
      "2021-04-28 15:39:03,578\toarph 723307 : ... done.  Egg at /tmp/tmp7d55fgjb_oarphpy_eggbuild/cheap_optical_flow_eval_analysis-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from cheap_optical_flow_eval_analysis.kittisf15 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 Kitti Scene Flow 2015 scenes\n"
     ]
    }
   ],
   "source": [
    "from psegs import datum\n",
    "\n",
    "# You have to ls flow_occ to get the paths\n",
    "KITTI_SF15_DEMO_URIS = [\n",
    "    datum.URI(dataset=KITTISF15Factory.DATASET, extra={\n",
    "        'ksf15.input': 'training/image_2/000000_10.png',\n",
    "        'ksf15.expected_out': 'training/image_2/000000_11.png',\n",
    "        'ksf15.flow_gt': 'training/flow_occ/000000_10.png',\n",
    "        'ksf15.disp0': 'training/disp_occ_0/000000_10.png',\n",
    "        'ksf15.disp1': 'training/disp_occ_1/000000_10.png',\n",
    "        'ksf15.K': 'training/calib_cam_to_cam/000000.txt',\n",
    "    }),\n",
    "    datum.URI(dataset=KITTISF15Factory.DATASET, extra={\n",
    "        'ksf15.input': 'training/image_2/000007_10.png',\n",
    "        'ksf15.expected_out': 'training/image_2/000007_11.png',\n",
    "        'ksf15.flow_gt': 'training/flow_occ/000007_10.png',\n",
    "        'ksf15.disp0': 'training/disp_occ_0/000007_10.png',\n",
    "        'ksf15.disp1': 'training/disp_occ_1/000007_10.png',\n",
    "        'ksf15.K': 'training/calib_cam_to_cam/000007.txt',\n",
    "    }),\n",
    "    datum.URI(dataset=KITTISF15Factory.DATASET, extra={\n",
    "        'ksf15.input': 'training/image_2/000023_10.png',\n",
    "        'ksf15.expected_out': 'training/image_2/000023_11.png',\n",
    "        'ksf15.flow_gt': 'training/flow_occ/000023_10.png',\n",
    "        'ksf15.disp0': 'training/disp_occ_0/000023_10.png',\n",
    "        'ksf15.disp1': 'training/disp_occ_1/000023_10.png',\n",
    "        'ksf15.K': 'training/calib_cam_to_cam/000023.txt',\n",
    "    }),\n",
    "    datum.URI(dataset=KITTISF15Factory.DATASET, extra={\n",
    "        'ksf15.input': 'training/image_2/000051_10.png',\n",
    "        'ksf15.expected_out': 'training/image_2/000051_11.png',\n",
    "        'ksf15.flow_gt': 'training/flow_occ/000051_10.png',\n",
    "        'ksf15.disp0': 'training/disp_occ_0/000051_10.png',\n",
    "        'ksf15.disp1': 'training/disp_occ_1/000051_10.png',\n",
    "        'ksf15.K': 'training/calib_cam_to_cam/000051.txt',\n",
    "    }),\n",
    "    datum.URI(dataset=KITTISF15Factory.DATASET, extra={\n",
    "        'ksf15.input': 'training/image_2/000003_10.png',\n",
    "        'ksf15.expected_out': 'training/image_2/000003_11.png',\n",
    "        'ksf15.flow_gt': 'training/flow_occ/000003_10.png',\n",
    "        'ksf15.disp0': 'training/disp_occ_0/000003_10.png',\n",
    "        'ksf15.disp1': 'training/disp_occ_1/000003_10.png',\n",
    "        'ksf15.K': 'training/calib_cam_to_cam/000003.txt',\n",
    "    }),\n",
    "]\n",
    "\n",
    "ALL_FP_FACTORY_CLSS.append(KITTISF15Factory)\n",
    "\n",
    "print(\"Found %s Kitti Scene Flow 2015 scenes\" % len(KITTISF15Factory.list_fp_uris(spark)))\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    fp_rdd = KITTISF15Factory.get_fp_rdd_for_uris(spark, KITTI_SF15_DEMO_URIS)\n",
    "    fps = fp_rdd.collect()\n",
    "    \n",
    "    for fp in fps:\n",
    "#         show_html(fp.to_html() + \"<br/><br/><br/>\")\n",
    "        print('fixme show html')\n",
    "        DEMO_FPS.append(fp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def kitti_sf15_create_fp(info):\n",
    "#      return OpticalFlowPair(\n",
    "#                 dataset=\"KITTI Scene Flow 2015\",\n",
    "#                 id1=scene['input'],\n",
    "#                 img1='file://' + os.path.join(KITTI_SF15_DATA_ROOT, scene['input']),\n",
    "#                 id2=scene['expected_out'],\n",
    "#                 img2='file://' + os.path.join(KITTI_SF15_DATA_ROOT, scene['expected_out']),\n",
    "#                 flow=KITTISF15LoadFlowFromPng(os.path.join(KITTI_SF15_DATA_ROOT, scene['flow_gt'])))\n",
    "\n",
    "# if SHOW_DEMO_OUTPUT:\n",
    "#     for scene in KITTI_SF15_DEMO_SCENES:\n",
    "#         p = kitti_sf15_create_fp(scene)\n",
    "#         show_html(p.to_html())\n",
    "#         DEMO_FPS.append(p)\n",
    "\n",
    "# if RUN_FULL_ANALYSIS:\n",
    "#     for scene in KITTI_SF15_ALL_SCENES:\n",
    "#         p = kitti_sf15_create_fp(scene)\n",
    "#         ALL_FPS.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSegs Synthetic Flow from Fused Lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEGS_SYNTHFLOW_PARQUET_ROOT = '/outer_root/media/rocket4q/psegs_flow_records_short'\n",
    "\n",
    "# from psegs.exp.fused_lidar_flow import FlowRecTable\n",
    "\n",
    "# T = FlowRecTable(spark, PSEGS_SYNTHFLOW_PARQUET_ROOT)\n",
    "# synthflow_record_uris = T.get_record_uris()\n",
    "# print(\"Found %s PSegs SynthFlow records\" % len(synthflow_record_uris))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fr_samp_rdd = T.get_records_with_samples_rdd(\n",
    "#                     record_uris=[PSEGS_SYNTHFLOW_DEMO_RECORD_URIS[0]],\n",
    "#                     include_cameras=False,\n",
    "#                     include_cuboids=False,\n",
    "#                     include_point_clouds=False)\n",
    "# flow_rec = fr_samp_rdd.take(1)[0][0]\n",
    "\n",
    "# print(\"Sample record:\")\n",
    "# show_html(flow_rec.to_html())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cheap_optical_flow_eval_analysis/psegs_synthflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cheap_optical_flow_eval_analysis/psegs_synthflow.py\n",
    "\n",
    "from psegs import datum\n",
    "from psegs.exp.fused_lidar_flow import FlowRecTable\n",
    "\n",
    "from cheap_optical_flow_eval_analysis.ofp import *\n",
    "\n",
    "from oarphpy.spark import CloudpickeledCallable\n",
    "\n",
    "\n",
    "# Please provide the PSegs synthetic flow Parquet directory root below:\n",
    "# PSEGS_SYNTHFLOW_PARQUET_ROOT = '/outer_root/media/rocket4q/psegs_flow_records_short_fixed'\n",
    "PSEGS_SYNTHFLOW_PARQUET_ROOT = '/outer_root/media/rocket4q/psegs_flow_records_FULL_fixed'\n",
    "\n",
    "def psegs_synthflow_flow_rec_to_fp(flow_rec, sample):\n",
    "  fr = flow_rec\n",
    "\n",
    "  uri_str_to_datum = sample.get_uri_str_to_datum()\n",
    "\n",
    "  # Find the camera_images associated with `flow_rec`\n",
    "  ci1_url_str = str(flow_rec.clouds[0].ci_uris[0])\n",
    "  ci1_sd = uri_str_to_datum[ci1_url_str]\n",
    "  ci1 = ci1_sd.camera_image\n",
    "\n",
    "  ci2_url_str = str(flow_rec.clouds[1].ci_uris[0])\n",
    "  ci2_sd = uri_str_to_datum[ci2_url_str]\n",
    "  ci2 = ci2_sd.camera_image\n",
    "\n",
    "  import numpy as np\n",
    "  world_T1 = ci1.ego_pose.translation\n",
    "  world_T2 = ci2.ego_pose.translation\n",
    "  translation_meters = np.linalg.norm(world_T2 - world_T1)\n",
    "\n",
    "  id1 = ci1_url_str + '&extra.psegs_flow_sids=' + str(fr.clouds[0].sample_id)\n",
    "  id2 = ci2_url_str + '&extra.psegs_flow_sids=' + str(fr.clouds[1].sample_id)\n",
    "\n",
    "  import urllib.parse\n",
    "  eval_uri = datum.URI(dataset=PSegsSynthFlowFactory.DATASET, extra={'pssf.ruri': urllib.parse.quote(str(fr.uri))})\n",
    "\n",
    "  uvdviz_im1 = flow_rec.clouds[0].uvdvis\n",
    "  uvdviz_im2 = flow_rec.clouds[1].uvdvis\n",
    "  K = ci1.K\n",
    "\n",
    "  fp = OpticalFlowPair(\n",
    "          uri=eval_uri,\n",
    "          dataset=\"PSegs SynthFlow for %s (%s)\" % (fr.uri.dataset, fr.uri.split),\n",
    "          id1=id1,\n",
    "          id2=id2,\n",
    "          img1=CloudpickeledCallable(lambda: ci1.image),\n",
    "          img2=CloudpickeledCallable(lambda: ci2.image),\n",
    "          flow=CloudpickeledCallable(lambda: fr.to_optical_flow()),\n",
    "\n",
    "          diff_time_sec=float(1e9 * abs(ci2_sd.uri.timestamp - ci1_sd.uri.timestamp)),\n",
    "          translation_meters=translation_meters,\n",
    "      \n",
    "          uvdviz_im1=uvdviz_im1,\n",
    "          uvdviz_im2=uvdviz_im2,\n",
    "          K=K)\n",
    "  return fp\n",
    "\n",
    "def psegs_synthflow_create_fps(\n",
    "        spark,\n",
    "        flow_record_pq_table_path,\n",
    "        record_uris,\n",
    "        include_cuboids=False,\n",
    "        include_point_clouds=False):\n",
    "\n",
    "  T = FlowRecTable(spark, flow_record_pq_table_path)\n",
    "  rec_sample_rdd = T.get_records_with_samples_rdd(\n",
    "                          record_uris=record_uris,\n",
    "                          include_cameras=True,\n",
    "                          include_cuboids=include_cuboids,\n",
    "                          include_point_clouds=include_point_clouds)\n",
    "\n",
    "  fps = [\n",
    "    flow_rec_to_fp(flow_rec, sample)\n",
    "    for flow_rec, sample in rec_sample_rdd.collect()\n",
    "  ]\n",
    "\n",
    "  return fps\n",
    "\n",
    "\n",
    "class PSegsSynthFlowFactory(FlowPairFactoryBase):\n",
    "    DATASET = 'psegs_synthflow'\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_frec_table(cls, spark):\n",
    "        if not hasattr(cls, '_frec_table'):\n",
    "            cls._frec_table = FlowRecTable(spark, PSEGS_SYNTHFLOW_PARQUET_ROOT)\n",
    "        return cls._frec_table\n",
    "    \n",
    "    @classmethod\n",
    "    def list_fp_uris(cls, spark):\n",
    "        import urllib.parse\n",
    "        T = cls._get_frec_table(spark)\n",
    "        ruris = T.get_record_uris()\n",
    "        fp_uris = [\n",
    "            datum.URI(dataset=cls.DATASET, extra={'pssf.ruri': urllib.parse.quote(str(ruri))})\n",
    "            for ruri in ruris\n",
    "        ]\n",
    "        return fp_uris\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        import urllib.parse\n",
    "        T = cls._get_frec_table(spark)\n",
    "        ruris = [urllib.parse.unquote(uri.extra['pssf.ruri']) for uri in uris]\n",
    "        rec_sample_rdd = T.get_records_with_samples_rdd(\n",
    "                          record_uris=ruris,\n",
    "                          include_cameras=True,\n",
    "                          include_cuboids=False,\n",
    "                          include_point_clouds=False)\n",
    "        fp_rdd = rec_sample_rdd.map(lambda fs: psegs_synthflow_flow_rec_to_fp(*fs))\n",
    "        return fp_rdd\n",
    "        \n",
    "\n",
    "\n",
    "# def psegs_synthflow_iter_fp_rdds(\n",
    "#         spark,\n",
    "#         flow_record_pq_table_path,\n",
    "#         fps_per_rdd=100,\n",
    "#         include_cuboids=False,\n",
    "#         include_point_clouds=False):\n",
    "  \n",
    "#   T = FlowRecTable(spark, flow_record_pq_table_path)\n",
    "#   ruris = T.get_record_uris()\n",
    "\n",
    "#   # Ensure a sort so that pairs from similar segments will load in the same\n",
    "#   # RDD -- that makes joins smaller and faster\n",
    "#   ruris = sorted(ruris)\n",
    "\n",
    "#   from oarphpy import util as oputil\n",
    "#   for ruri_chunk in oputil.ichunked(ruris, fps_per_rdd):\n",
    "#     frec_sample_rdd = T.get_records_with_samples_rdd(\n",
    "#                           record_uris=rids,\n",
    "#                           include_cuboids=include_cuboids,\n",
    "#                           include_point_clouds=include_point_clouds)\n",
    "#     fp_rdd = frec_sample_rdd.map(flow_rec_to_fp)\n",
    "#     yield fp_rdd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:11:57,017\toarph 1807391 : Source has changed! Rebuilding Egg ...\n",
      "2021-04-28 21:11:57,018\toarph 1807391 : Using source root /tmp/tmpekna3vk7_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis \n",
      "2021-04-28 21:11:57,018\toarph 1807391 : Using source root /tmp/tmpekna3vk7_cheap_optical_flow_eval_analysis \n",
      "2021-04-28 21:11:57,020\toarph 1807391 : Generating egg to /tmp/tmp9pgzm5jp_oarphpy_eggbuild ...\n",
      "2021-04-28 21:11:57,026\toarph 1807391 : ... done.  Egg at /tmp/tmp9pgzm5jp_oarphpy_eggbuild/cheap_optical_flow_eval_analysis-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from cheap_optical_flow_eval_analysis.psegs_synthflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 20:52:13,503\tps   723307 : FlowRecTable: Reading parquet from /outer_root/media/rocket4q/psegs_flow_records_FULL_fixed \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 PSegs SynthFlow scenes\n"
     ]
    }
   ],
   "source": [
    "from psegs import datum\n",
    "\n",
    "import urllib.parse\n",
    "\n",
    "PSEGS_SYNTHFLOW_DEMO_FPS_DO_CACHE = True\n",
    "PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH = '/tmp/psegs_synthflow_demo.pkl'\n",
    "\n",
    "PSEGS_SYNTHFLOW_DEMO_RECORD_RURIS = (\n",
    "  'psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0000_sync&extra.psegs_flow_sids=4340,4339',\n",
    "  'psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0000_sync&extra.psegs_flow_sids=11219,11269',\n",
    "\n",
    "  'psegs://dataset=nuscenes&split=train_track&segment_id=scene-0501&extra.psegs_flow_sids=40009,40010',\n",
    "  'psegs://dataset=nuscenes&split=train_track&segment_id=scene-0501&extra.psegs_flow_sids=50013,50014',\n",
    "\n",
    "#   'psegs://dataset=kitti-360-fused&split=train&segment_id=2013_05_28_drive_0000_sync&extra.psegs_flow_sids=11103,11104',\n",
    "#   'psegs://dataset=kitti-360-fused&split=train&segment_id=2013_05_28_drive_0000_sync&extra.psegs_flow_sids=1181,1182',\n",
    "\n",
    "#   'psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0002&extra.psegs_flow_sids=10016,10017',\n",
    "#   'psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0582&extra.psegs_flow_sids=60035,60036',\n",
    "\n",
    "#   'psegs://dataset=nuscenes&split=train_track&segment_id=scene-0393&extra.psegs_flow_sids=50017,50018',\n",
    "#   'psegs://dataset=nuscenes&split=train_track&segment_id=scene-0501&extra.psegs_flow_sids=40019,40020',\n",
    "    #  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D30009%2C30010%26sel_datums%3Dcamera%7CCAM_BACK_RIGHT%2C1535478534928113000%2Ccamera%7CCAM_BACK_RIGHT%2C1535478535428113000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D50016%2C50017%26sel_datums%3Dcamera%7CCAM_FRONT_LEFT%2C1535478538404799000%2Ccamera%7CCAM_FRONT_LEFT%2C1535478538904799000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D50018%2C50019%26sel_datums%3Dcamera%7CCAM_FRONT_LEFT%2C1535478539504799000%2Ccamera%7CCAM_FRONT_LEFT%2C1535478540004799000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D30031%2C30032%26sel_datums%3Dcamera%7CCAM_BACK_RIGHT%2C1535478546028113000%2Ccamera%7CCAM_BACK_RIGHT%2C1535478546528113000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D50023%2C50024%26sel_datums%3Dcamera%7CCAM_FRONT_LEFT%2C1535478541904799000%2Ccamera%7CCAM_FRONT_LEFT%2C1535478542504811000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D20006%2C20007%26sel_datums%3Dcamera%7CCAM_BACK_LEFT%2C1535478533447405000%2Ccamera%7CCAM_BACK_LEFT%2C1535478533947405000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D10019%2C10020%26sel_datums%3Dcamera%7CCAM_BACK%2C1535478540037558000%2Ccamera%7CCAM_BACK%2C1535478540537558000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D30023%2C30024%26sel_datums%3Dcamera%7CCAM_BACK_RIGHT%2C1535478541928113000%2Ccamera%7CCAM_BACK_RIGHT%2C1535478542528113000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D50007%2C50008%26sel_datums%3Dcamera%7CCAM_FRONT_LEFT%2C1535478533904799000%2Ccamera%7CCAM_FRONT_LEFT%2C1535478534404799000',\n",
    " 'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D60014%2C60015%26sel_datums%3Dcamera%7CCAM_FRONT_RIGHT%2C1535478537420482000%2Ccamera%7CCAM_FRONT_RIGHT%2C1535478537870482000',\n",
    " 'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dkitti-360%26split%3Dtrain%26segment_id%3D2013_05_28_drive_0004_sync%26extra.psegs_flow_sids%3D10412%2C10413%26sel_datums%3Dcamera%7Cright_rect%2C1369736347374754304%2Ccamera%7Cright_rect%2C1369736347479072256',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D50002%2C50003%26sel_datums%3Dcamera%7CCAM_FRONT_LEFT%2C1535478531354799000%2Ccamera%7CCAM_FRONT_LEFT%2C1535478531854807000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D50027%2C50028%26sel_datums%3Dcamera%7CCAM_FRONT_LEFT%2C1535478543904799000%2Ccamera%7CCAM_FRONT_LEFT%2C1535478544404799000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dkitti-360%26split%3Dtrain%26segment_id%3D2013_05_28_drive_0004_sync%26extra.psegs_flow_sids%3D10412%2C10413%26sel_datums%3Dcamera%7Cleft_rect%2C1369736347374744320%2Ccamera%7Cleft_rect%2C1369736347479187968',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D10027%2C10028%26sel_datums%3Dcamera%7CCAM_BACK%2C1535478543937558000%2Ccamera%7CCAM_BACK%2C1535478544437558000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D10035%2C10036%26sel_datums%3Dcamera%7CCAM_BACK%2C1535478548187558000%2Ccamera%7CCAM_BACK%2C1535478548687558000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D40005%2C40006%26sel_datums%3Dcamera%7CCAM_FRONT%2C1535478532912404000%2Ccamera%7CCAM_FRONT%2C1535478533412404000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D40002%2C40003%26sel_datums%3Dcamera%7CCAM_FRONT%2C1535478531362404000%2Ccamera%7CCAM_FRONT%2C1535478531862404000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D30034%2C30035%26sel_datums%3Dcamera%7CCAM_BACK_RIGHT%2C1535478547628113000%2Ccamera%7CCAM_BACK_RIGHT%2C1535478548178113000',\n",
    "#  'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dkitti-360%26split%3Dtrain%26segment_id%3D2013_05_28_drive_0000_sync%26extra.psegs_flow_sids%3D4340%2C4339%26sel_datums%3Dcamera%7Cleft_rect%2C1369731215809577984%2Ccamera%7Cleft_rect%2C1369731215914083072'\n",
    "\n",
    ")\n",
    "\n",
    "PSEGS_SYNTHFLOW_DEMO_FP_URIS = [\n",
    "    datum.URI.from_str(s)\n",
    "    for s in (\n",
    "         'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dnuscenes%26split%3Dtrain_track%26segment_id%3Dscene-0501%26extra.psegs_flow_sids%3D60014%2C60015%26sel_datums%3Dcamera%7CCAM_FRONT_RIGHT%2C1535478537420482000%2Ccamera%7CCAM_FRONT_RIGHT%2C1535478537870482000',\n",
    "         'psegs://dataset=psegs_synthflow&extra.pssf.ruri=psegs%3A//dataset%3Dkitti-360%26split%3Dtrain%26segment_id%3D2013_05_28_drive_0004_sync%26extra.psegs_flow_sids%3D10412%2C10413%26sel_datums%3Dcamera%7Cright_rect%2C1369736347374754304%2Ccamera%7Cright_rect%2C1369736347479072256',\n",
    "    )\n",
    "]\n",
    "\n",
    "ALL_FP_FACTORY_CLSS.append(PSegsSynthFlowFactory)\n",
    "\n",
    "psegs_synthflow_all_rids = PSegsSynthFlowFactory.list_fp_uris(spark)\n",
    "print(\"Found %s PSegs SynthFlow scenes\" % len(psegs_synthflow_all_rids))\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    if os.path.exists(PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH):\n",
    "        print(\"Loading demo FlowPairs from %s\" % PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH)\n",
    "        import pickle\n",
    "        fps = pickle.load(open(PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH, 'rb'))\n",
    "    else:\n",
    "        print(\"Building Demo FlowPairs, this might take a while ....\")\n",
    "        fp_rdd = PSegsSynthFlowFactory.get_fp_rdd_for_uris(spark, PSEGS_SYNTHFLOW_DEMO_FP_URIS)\n",
    "        fps = fp_rdd.collect()\n",
    "        if PSEGS_SYNTHFLOW_DEMO_FPS_DO_CACHE:\n",
    "            print(\"Saving demo FlowPairs to %s ...\" % PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH)\n",
    "            import pickle\n",
    "            with open(PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH, 'wb') as f:\n",
    "                pickle.dump(fps, f, protocol=4)\n",
    "    \n",
    "    for fp in fps:\n",
    "        print('fixme html')\n",
    "        continue\n",
    "        show_html(fp.to_html())\n",
    "        DEMO_FPS.append(fp)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     import urllib.parse\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     for fp in fps:\n",
    "#         show_html(fp.to_html() + \"<br/><br/><br/>\")\n",
    "#         DEMO_FPS.append(fp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if SHOW_DEMO_OUTPUT:\n",
    "#     if os.path.exists(PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH):\n",
    "#         print(\"Loading demo FlowPairs from %s\" % PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH)\n",
    "#         import pickle\n",
    "#         fps = pickle.load(open(PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH, 'rb'))\n",
    "#     else:\n",
    "#         print(\"Building Demo FlowPairs, this might take a while ....\")\n",
    "#         fps = psegs_synthflow_create_fps(spark, PSEGS_SYNTHFLOW_PARQUET_ROOT, PSEGS_SYNTHFLOW_DEMO_RECORD_URIS)\n",
    "#         if PSEGS_SYNTHFLOW_DEMO_FPS_DO_CACHE:\n",
    "#             print(\"Saving demo FlowPairs to %s ...\" % PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH)\n",
    "#             import pickle\n",
    "#             with open(PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH, 'wb') as f:\n",
    "#                 pickle.dump(fps, f, protocol=4)\n",
    "    \n",
    "#     for fp in fps:\n",
    "#         show_html(fp.to_html())\n",
    "#         DEMO_FPS.append(fp)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction via Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Reconstruction via Optical Flow\n",
    "\n",
    "def zero_flow(flow):\n",
    "    return (flow[:, :, :2] == np.array([0, 0])).all(axis=-1)\n",
    "\n",
    "def warp_flow_backwards(img, flow):\n",
    "    \"\"\"Given an image, apply the inverse of `flow`\"\"\"\n",
    "    h, w = flow.shape[:2]\n",
    "    flow = -flow\n",
    "    flow[:,:,0] += np.arange(w)\n",
    "    flow[:,:,1] += np.arange(h)[:,np.newaxis]\n",
    "    res = cv2.remap(img, flow.astype(np.float32), None, cv2.INTER_LINEAR)\n",
    "    return res\n",
    "    \n",
    "def warp_flow_forwards(img, flow):\n",
    "    \"\"\"Given an image, apply the given optical flow `flow`.  Returns not only the warped\n",
    "    image, but a `mask` indicating warped pixels (i.e. there was non-zero flow *into* these pixels ).\n",
    "    With some help from https://stackoverflow.com/questions/41703210/inverting-a-real-valued-index-grid/46009462#46009462\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    pts = flow.copy()\n",
    "    pts[:, :, 0] += np.arange(w)\n",
    "    pts[:, :, 1] += np.arange(h)[:, np.newaxis]\n",
    "    exclude = zero_flow(flow)\n",
    "    if exclude.all():\n",
    "        # No flow anywhere!\n",
    "        return img.copy(), np.zeros((h, w)).astype(np.bool)\n",
    "    else:\n",
    "        inpts = pts[~exclude]\n",
    "    \n",
    "    from scipy.interpolate import griddata\n",
    "    inpts = np.reshape(inpts, [-1, 2])\n",
    "    grid_y, grid_x = np.mgrid[:h, :w]\n",
    "    chan_out = []\n",
    "    for ch in range(img.shape[-1]):\n",
    "        spts = img[:, :, ch][~exclude].reshape([-1, 1])\n",
    "        mapped = griddata(inpts, spts, (grid_x, grid_y), method='linear')\n",
    "        chan_out.append(mapped.astype(img.dtype))\n",
    "    out = np.stack(chan_out, axis=-1)\n",
    "    out = out.reshape([h, w, len(chan_out)])\n",
    "\n",
    "    mask = np.reshape(inpts, [-1, 2])\n",
    "    mask = np.rint(mask).astype(np.int)\n",
    "    mask = mask[np.where((mask[:, 0] >= 0) & (mask[:, 0] < w) & (mask[:, 1] >= 0) & (mask[:, 1] < h))]\n",
    "    valid_mask = np.zeros((h, w))\n",
    "    valid_mask[mask[:, 1], mask[:, 0]] = 1\n",
    "    \n",
    "    return out, valid_mask.astype(np.bool)\n",
    "\n",
    "# @attr.s(slots=True, eq=False, weakref_slot=False)\n",
    "class FlowReconstructedImagePair(object):\n",
    "    \"\"\"A pair of reconstructed images using an input pair of images and optical\n",
    "    flow field (i.e. an `OpticalFlowPair` instance).\"\"\"\n",
    "\n",
    "    slots = (\n",
    "        'opair',\n",
    "        'img2_recon_fwd',\n",
    "        'img2_recon_fwd_valid',\n",
    "        'img1_recon_bkd',\n",
    "        'img1_recon_bkd_valid'\n",
    "    )\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        for k in self.slots:\n",
    "            setattr(self, k, kwargs.get(k))\n",
    "    \n",
    "#     opair = attr.ib(default=OpticalFlowPair())\n",
    "#     \"\"\"The original `OpticalFlowPair` with the source of the data for this reconstruction result.\"\"\"\n",
    "    \n",
    "#     img2_recon_fwd = attr.ib(default=np.array([]))\n",
    "#     \"\"\"A Numpy image containing the result of FORWARDS-WARPING OpticalFlowPair::img1\n",
    "#     via OpticalFlowPair::flow to reconstruct OpticalFlowPair::img2\"\"\"\n",
    "\n",
    "#     img2_recon_fwd_valid = attr.ib(default=np.array([]))\n",
    "#     \"\"\"A Numpy boolean mask indicating which pixels of `img2_recon_fwd` were modified via non-zero flow\"\"\"\n",
    "    \n",
    "#     img1_recon_bkd = attr.ib(default=np.array([]))\n",
    "#     \"\"\"A Numpy image containing the result of BACKWARDS-WARPING OpticalFlowPair::img2\n",
    "#     via OpticalFlowPair::flow to reconstruct OpticalFlowPair::img1\"\"\"\n",
    "\n",
    "#     img1_recon_bkd_valid = attr.ib(default=np.array([]))\n",
    "#     \"\"\"A Numpy boolean mask indicating which pixels of `img1_recon_bkd` were modified via non-zero flow\"\"\"\n",
    "        \n",
    "    @classmethod\n",
    "    def create_from(cls, oflow_pair: OpticalFlowPair):\n",
    "        flow = oflow_pair.get_flow()\n",
    "        \n",
    "        # Forward Warp\n",
    "        fwarped, fvalid = warp_flow_forwards(oflow_pair.get_img1(), flow)\n",
    "\n",
    "        # Backwards Warp\n",
    "        exclude = zero_flow(flow)\n",
    "        bwarped = warp_flow_backwards(oflow_pair.get_img2(), -flow[:, :, :2])\n",
    "        bvalid = ~exclude\n",
    "        \n",
    "        return FlowReconstructedImagePair(\n",
    "                opair=oflow_pair,\n",
    "                img2_recon_fwd=fwarped,\n",
    "                img2_recon_fwd_valid=fvalid,\n",
    "                img1_recon_bkd=bwarped,\n",
    "                img1_recon_bkd_valid=bvalid)\n",
    "    \n",
    "    def to_html(self):\n",
    "        # We use pixels from the destination image in order to make the reconstruction \n",
    "        # easier to interpret; we'll fade them in intensity so that they are more\n",
    "        # conspicuous.        \n",
    "        FADE_UNTOUCHED_PIXELS = 0.3\n",
    "        \n",
    "        viz_fwd = self.img2_recon_fwd.copy().astype(np.float32)\n",
    "        im2 = self.opair.get_img2()\n",
    "        if (~self.img2_recon_fwd_valid).any():\n",
    "            viz_fwd[~self.img2_recon_fwd_valid] = im2[~self.img2_recon_fwd_valid]\n",
    "            viz_fwd[~self.img2_recon_fwd_valid] *= FADE_UNTOUCHED_PIXELS\n",
    "        else:\n",
    "            # viz_fwd = im2.copy() * FADE_UNTOUCHED_PIXELS\n",
    "            print('no invalids forward!')\n",
    "        \n",
    "        viz_bkd = self.img1_recon_bkd.copy().astype(np.float32)\n",
    "        im1 = self.opair.get_img1()\n",
    "        if (~self.img1_recon_bkd_valid).any():\n",
    "            viz_bkd[~self.img1_recon_bkd_valid] = im1[~self.img1_recon_bkd_valid]\n",
    "            viz_bkd[~self.img1_recon_bkd_valid] *= FADE_UNTOUCHED_PIXELS\n",
    "        else:\n",
    "            # viz_bkd = im1.copy() * FADE_UNTOUCHED_PIXELS\n",
    "            print('no invalids backwards!')\n",
    "        \n",
    "        html = \"\"\"\n",
    "            <table>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Forwards Warped <i>(dark pixels unwarped)</i></b></td></tr>\n",
    "            <tr><td><img src=\"{viz_fwd}\" width=\"100%\" /></td></tr>\n",
    "\n",
    "            <tr><td style=\"text-align:left\"><b>Backwards Warped <i>(dark pixels unwarped)</i></b></td></tr>\n",
    "            <tr><td><img src=\"{viz_bkd}\" width=\"100%\" /></td></tr>\n",
    "\n",
    "            </table>\n",
    "        \"\"\".format(\n",
    "                viz_fwd=img_to_data_uri(viz_fwd.astype(np.uint8)),\n",
    "                viz_bkd=img_to_data_uri(viz_bkd.astype(np.uint8)))\n",
    "        return html\n",
    "\n",
    "        \n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    DEMO_RECONS = []\n",
    "    for p in DEMO_FPS:\n",
    "        print('fixme html')\n",
    "        continue\n",
    "        recon = FlowReconstructedImagePair.create_from(p)\n",
    "#         show_html(recon.to_html() + \"</br></br></br>\")\n",
    "        DEMO_RECONS.append(recon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analysis Utils\n",
    "\n",
    "def mse(i1, i2, valid):\n",
    "    return np.mean((i1[valid] - i2[valid]) ** 2)\n",
    "\n",
    "def rmse(i1, i2, valid):\n",
    "    return math.sqrt(mse(i1, i2, valid))\n",
    "\n",
    "def psnr(i1, i2, valid):\n",
    "    return 20 * math.log10(255) - 10 * math.log10(max((mse(i1, i2, valid), 1e-12)))\n",
    "\n",
    "def ssim(i1, i2, valid):\n",
    "    # Some variance out there ...\n",
    "    # https://github.com/scikit-image/scikit-image/blob/master/skimage/metrics/_structural_similarity.py#L12-L232\n",
    "    # https://github.com/nianticlabs/monodepth2/blob/13200ab2f29f2f10dec3aa5db29c32a23e29d376/layers.py#L218\n",
    "    # https://cvnote.ddlee.cn/2019/09/12/psnr-ssim-python\n",
    "    # We will just use SKImage for now ...\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    mssim, S = ssim(i1, i2, win_size=11, multichannel=True, full=True)\n",
    "    return np.mean(S[valid])\n",
    "\n",
    "def to_edge_im(img):\n",
    "    return np.stack([\n",
    "        cv2.Laplacian(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), cv2.CV_32F, ksize=1),\n",
    "        cv2.Sobel(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), cv2.CV_32F, 1, 0, ksize=3),\n",
    "        cv2.Sobel(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), cv2.CV_32F, 0, 1, ksize=3),\n",
    "    ], axis=-1)\n",
    "\n",
    "def edges_mse(i1, i2, valid):\n",
    "    return mse(to_edge_im(i1), to_edge_im(i2), valid)\n",
    "\n",
    "\n",
    "def oflow_coverage(valid):\n",
    "    return valid.sum() / (valid.shape[0] * valid.shape[1])\n",
    "\n",
    "def oflow_magnitude_hist(flow, valid, bins=50):\n",
    "    flow_l2s = np.sqrt( flow[valid][:, 0] ** 2 + flow[valid][:, 1] ** 2 )\n",
    "    bin_counts, bin_edges = np.histogram(flow_l2s, bins=bins)\n",
    "    return bin_edges, bin_counts\n",
    "\n",
    "\n",
    "# Analysis Data Model\n",
    "\n",
    "class OFlowReconErrors(object):\n",
    "    \"\"\"Various measures of reconstruction error for a `FlowReconstructedImagePair` instance.\n",
    "    Encapsulated as two dictionaries of stats for easy interop with Spark SQL.\"\"\"\n",
    "\n",
    "    RECONSTRUCTION_ERR_METRICS = {\n",
    "        'SSIM': ssim,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'PSNR': psnr,\n",
    "        'Edges_MSE': edges_mse,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, recon_pair: FlowReconstructedImagePair):\n",
    "        im2 = recon_pair.opair.get_img2()\n",
    "        img2_recon_fwd = recon_pair.img2_recon_fwd\n",
    "        img2_recon_fwd_valid = recon_pair.img2_recon_fwd_valid\n",
    "        self.forward_stats = dict(\n",
    "            (name, func(im2, img2_recon_fwd, img2_recon_fwd_valid))\n",
    "            for name, func in self.RECONSTRUCTION_ERR_METRICS.items())\n",
    "        \n",
    "        im1 = recon_pair.opair.get_img1()\n",
    "        img1_recon_fwd = recon_pair.img1_recon_bkd\n",
    "        img1_recon_fwd_valid = recon_pair.img1_recon_bkd_valid\n",
    "        self.backward_stats = dict(\n",
    "            (name, func(im1, img1_recon_fwd, img1_recon_fwd_valid))\n",
    "            for name, func in self.RECONSTRUCTION_ERR_METRICS.items())\n",
    "\n",
    "    def to_html(self):\n",
    "        stat_names = self.RECONSTRUCTION_ERR_METRICS.keys()\n",
    "\n",
    "        rows = [\n",
    "            \"\"\"\n",
    "            <tr>\n",
    "              <td style=\"text-align:left\"><b>{name}</b></td>\n",
    "              <td style=\"text-align:left\">{fwd:.2f}</td>\n",
    "              <td style=\"text-align:left\">{bkd:.2f}</td>\n",
    "            </tr>\n",
    "            \"\"\".format(name=name, fwd=self.forward_stats[name], bkd=self.backward_stats[name])\n",
    "            for name in stat_names\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        html = \"\"\"\n",
    "            <table>\n",
    "              <tr>\n",
    "                  <th></th> <th><b>Forwards Warp</b></th> <th><b>Backwards Warp</b></th>\n",
    "              </tr>\n",
    "\n",
    "              {table_rows}\n",
    "\n",
    "            </table>\n",
    "        \"\"\".format(table_rows=\"\".join(rows))\n",
    "        \n",
    "        return html\n",
    "            \n",
    "# @attr.s(slots=True, eq=False, weakref_slot=False)\n",
    "class OFlowStats(object):\n",
    "    \"\"\"Stats on the optical flow of a `OpticalFlowPair` instance\"\"\"\n",
    "\n",
    "    slots = (\n",
    "        'opair',\n",
    "        'coverage',\n",
    "        'magnitude_hist',\n",
    "    )\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        for k in self.slots:\n",
    "            setattr(self, k, kwargs.get(k))\n",
    "    \n",
    "#     opair = attr.ib(default=OpticalFlowPair())\n",
    "#     \"\"\"The original `OpticalFlowPair` with the source of the data for this reconstruction result.\"\"\"\n",
    "    \n",
    "#     coverage = attr.ib(default=0)\n",
    "#     \"\"\"Fraction of the image with valid flow\"\"\"\n",
    "    \n",
    "#     magnitude_hist = attr.ib(default=[np.array([]), np.array([])])\n",
    "#     \"\"\"Histogram [bin edges, bin counts] of flow magnitudes\"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def create_from(cls, oflow_pair: OpticalFlowPair):\n",
    "        flow = oflow_pair.get_flow()\n",
    "        valid = ~zero_flow(flow)\n",
    "        return OFlowStats(\n",
    "                 opair=oflow_pair,\n",
    "                 coverage=oflow_coverage(valid),\n",
    "                 magnitude_hist=oflow_magnitude_hist(flow, valid))\n",
    "                 \n",
    "    def to_html(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig = plt.figure()\n",
    "        bin_edges, bin_counts = self.magnitude_hist\n",
    "        plt.bar(bin_edges[:-1], bin_counts)\n",
    "        plt.title(\"Histogram of Flow Magnitudes\")\n",
    "        plt.xlabel('Flow Magnitude (pixels)')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        hist_img = matplotlib_fig_to_img(fig)\n",
    "        \n",
    "        html = \"\"\"\n",
    "            <table>           \n",
    "            <tr><td style=\"text-align:left\"><b>Flow Coverage:</b> {coverage:.2f}% </td></tr>\n",
    "            <tr><td><img src=\"{flow_hist}\" width=\"100%\" /></td></tr>\n",
    "            </table>\n",
    "        \"\"\".format(\n",
    "                coverage=100. * self.coverage,\n",
    "                flow_hist=img_to_data_uri(matplotlib_fig_to_img(hist_img)))\n",
    "        return html\n",
    "\n",
    "\n",
    "# Misc\n",
    "\n",
    "def matplotlib_fig_to_img(fig):\n",
    "    import io\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    im = Image.open(buf)\n",
    "    im.show()\n",
    "    buf.seek(0)\n",
    "\n",
    "    import imageio\n",
    "    hist_img = imageio.imread(buf)\n",
    "    buf.close()\n",
    "    return hist_img\n",
    "\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    %matplotlib agg\n",
    "    for recon in DEMO_RECONS:\n",
    "        p = recon.opair\n",
    "        errors = OFlowReconErrors(recon)\n",
    "        err_html = errors.to_html()  \n",
    "            \n",
    "        fstats = OFlowStats.create_from(p)\n",
    "        stats_html = fstats.to_html()\n",
    "            \n",
    "        title = \"<b>{dataset} {id1} -> {id2}</b>\".format(dataset=p.dataset, id1=p.id1, id2=p.id2)\n",
    "        \n",
    "        show_html(title + stats_html + err_html + \"</br></br></br>\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene Flow Analysis (where depth and intrinsics are available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " * for psegs, we have uvd and K\n",
    " * for kitti tracking, we'll be able to have uvd and K\n",
    " * for deepdeform, the intrinsics are in each seq.  also a mask for maybe the images of interest?\n",
    " * for kitti sf, we can get K (P) from kitti-like file.  !!! kitti has obj_map colors image!!  \n",
    "     need to figure out depth meters from disparity ...  derrrp to get the raw velodynes we have to cross-ref\n",
    "     with odometry dataset. let's talk to yiyi about that...\n",
    " * !!! do a test where you use nearest neighbor correspondence on raw clouds for OFlow. then can see how bad\n",
    "     the pairing is sometimes\n",
    " \n",
    " * metrics: end-pt-error for NN forward; same for backward; then also do a chamfer distance metric\n",
    " * (do all this again but first do an ICP on the raw depths-- the rigid background should probably align, right?\n",
    "     use the ICP's RT to pose raw and \n",
    " * a common class for all these is background / foreground.  want to break down chamfer dist etc bucket by at least\n",
    "      background / foreground\n",
    " * debug image: surface pairs of points with end pt error larger than E and plot on the image\n",
    " \n",
    " * another good test: (1) train self-sup SF on raw clouds.  then test on large displacement pair \n",
    "     (walk a prediction forward many time steps). then can see how well that holds up vs our \"GT\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# def nn_distance(xyz_src, xyz_target):\n",
    "#     import numpy as np\n",
    "#     import open3d as o3d\n",
    "#     pcds = o3d.geometry.PointCloud()\n",
    "#     pcds.points = o3d.utility.Vector3dVector(xyz_src)\n",
    "#     pcdt = o3d.geometry.PointCloud()\n",
    "#     pcdt.points = o3d.utility.Vector3dVector(xyz_target)\n",
    "#     dists = pcds.compute_point_cloud_distance(pcdt)\n",
    "#     dists = np.asarray(dists)\n",
    "#     return dists\n",
    "\n",
    "def get_icp_results(xyz_src, xyz_target):\n",
    "    if xyz_target.shape[0] == 0 or xyz_src.shape[0] == 0:\n",
    "        return 0.0, -1.\n",
    "    \n",
    "    import numpy as np\n",
    "    import open3d as o3d\n",
    "    pcds = o3d.geometry.PointCloud()\n",
    "    pcds.points = o3d.utility.Vector3dVector(xyz_src)\n",
    "    pcdt = o3d.geometry.PointCloud()\n",
    "    pcdt.points = o3d.utility.Vector3dVector(xyz_target)\n",
    "    \n",
    "    threshold = 0.01\n",
    "    trans_init = np.eye(4, 4)\n",
    "    reg = o3d.pipelines.registration.registration_icp(\n",
    "                    pcds, pcdt, threshold, trans_init,\n",
    "                    o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "                    o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=100))\n",
    "    return reg.fitness, reg.inlier_rmse\n",
    "\n",
    "def get_nearest_neighbors(xyz_src, xyz_target):\n",
    "    if xyz_target.shape[0] == 0 or xyz_src.shape[0] == 0:\n",
    "        return np.zeros((0, 3), dtype=np.float32)\n",
    "    \n",
    "    # We need to use scipy KDTree because open3d doesn't provide an API \n",
    "    # for efficiently querying for more than one point\n",
    "    \n",
    "    import open3d as o3d\n",
    "    pcdt = o3d.geometry.PointCloud()\n",
    "    pcdt.points = o3d.utility.Vector3dVector(xyz_target)\n",
    "    pcd_tree = o3d.geometry.KDTreeFlann(pcdt)\n",
    "    \n",
    "    pcds = o3d.geometry.PointCloud()\n",
    "    pcds.points = o3d.utility.Vector3dVector(xyz_src)\n",
    "    \n",
    "    found = np.zeros(xyz_src.shape[0], dtype=np.int64)\n",
    "    print(xyz_src.shape[0])\n",
    "    for i in range(xyz_src.shape[0]):\n",
    "        k, idx, dist = pcd_tree.search_hybrid_vector_3d(pcds.points[i], float('inf'), 1)\n",
    "        found[i] = idx[0]\n",
    "    return xyz_target[found]\n",
    "    \n",
    "#     from scipy.spatial import KDTree\n",
    "#     print('fixme try open3d...')\n",
    "#     print('tree size', xyz_target.shape[0])\n",
    "#     tree = KDTree(xyz_target)\n",
    "#     print('query size', xyz_src.shape[0])\n",
    "#     dists, idx = tree.query(xyz_src)\n",
    "#     return xyz_target[idx]\n",
    "\n",
    "\n",
    "class SFlowStats(object):\n",
    "    \"\"\"Stats on the scene flow of a `OpticalFlowPair` instance (that has scene flow data)\"\"\"\n",
    "\n",
    "    slots = (\n",
    "        'sf_norm_hist',\n",
    "           # Histogram [bin edges, bin counts] of scene flow vector L2 norms\n",
    "        'sf_norm_var',\n",
    "           # Variance of Scene Flow displacements\n",
    "        \n",
    "        'fwd_nn_xyz',\n",
    "        'fwd_nn_hist',\n",
    "\n",
    "        'fwd_nn_dist_to_sf_dist',\n",
    "        \n",
    "        'fwd_nnepe_mean',\n",
    "        'fwd_nnepe_sum',\n",
    "        'fwd_nnepe_50th',\n",
    "        'fwd_nnepe_75th',\n",
    "        'fwd_nnepe_95th',\n",
    "        \n",
    "        'fwd_nnepe_hist',\n",
    "            # Histogram [bin edges, bin counts] of forward nearest-neighbor end-point-errors\n",
    "        \n",
    "        'bkd_nnepe_mean',\n",
    "        'bkd_nnepe_sum',\n",
    "#         'icp_fwd_nn_end_point_error',\n",
    "#         'icp_bkd_nn_end_point_error',\n",
    "#         'icp_chamfer_distance',\n",
    "        \n",
    "#         'chamfer_distance',\n",
    "#         'icp_chamfer_distance',\n",
    "        \n",
    "\n",
    "        \n",
    "        'icp_fitness',\n",
    "        'icp_inlier_rmse',\n",
    "        \n",
    "        'opair',\n",
    "    )\n",
    "    \n",
    "    def get_rowdata(self):\n",
    "        KEYS = (\n",
    "            'sf_norm_var',\n",
    "            'fwd_nn_dist_to_sf_dist',\n",
    "        \n",
    "            'fwd_nnepe_mean',\n",
    "            'fwd_nnepe_50th',\n",
    "            'fwd_nnepe_75th',\n",
    "            'fwd_nnepe_95th',\n",
    "            \n",
    "            'bkd_nnepe_mean',\n",
    "        \n",
    "            'icp_fitness',\n",
    "            'icp_inlier_rmse',\n",
    "        )\n",
    "        rowdata = dict(\n",
    "            (k, getattr(self, k, None) or float('nan'))\n",
    "            for k in KEYS)\n",
    "        return rowdata\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        for k in self.slots:\n",
    "            setattr(self, k, kwargs.get(k))\n",
    "    \n",
    "#     opair = attr.ib(default=OpticalFlowPair())\n",
    "#     \"\"\"The original `OpticalFlowPair` with the source of the data for this reconstruction result.\"\"\"\n",
    "    \n",
    "#     coverage = attr.ib(default=0)\n",
    "#     \"\"\"Fraction of the image with valid flow\"\"\"\n",
    "    \n",
    "#     fwd_nnepes = attr.ib(default=[np.array([]), np.array([])])\n",
    "#     \"\"\"\"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def create_from(cls, oflow_pair: OpticalFlowPair):\n",
    "        import numpy as np\n",
    "        fp = oflow_pair\n",
    "        \n",
    "        import time\n",
    "        start = time.time()\n",
    "        print('start', fp.uri)\n",
    "\n",
    "        uvd1 = fp.uvdviz_im1[:, :3]\n",
    "        uvd2 = fp.uvdviz_im2[:, :3]\n",
    "        print('uvd1 shape', uvd1.shape[0])\n",
    "        print('uvd2 shape', uvd2.shape[0])\n",
    "#         visible_either = ((uvd1[:, -1] == 1) | (uvd2[:, -1] == 1))\n",
    "        xyz1 = uvd_to_xyzrgb(uvd1, fp.K)[:, :3]\n",
    "        xyz2 = uvd_to_xyzrgb(uvd2, fp.K)[:, :3]\n",
    "        fwd_nn = get_nearest_neighbors(xyz1, xyz2)\n",
    "        print('got fwd nn', fwd_nn.shape)\n",
    "        fwd_nn_dist = np.linalg.norm(fwd_nn - xyz1, axis=1)\n",
    "        fwd_sf_dist = np.linalg.norm(xyz2 - xyz1, axis=1)\n",
    "        fwd_nn_dist_to_sf_dist = fwd_nn_dist.sum() / fwd_sf_dist.sum()\n",
    "        fwd_nn_end_point_error = np.linalg.norm(fwd_nn - xyz2, axis=1)\n",
    "        \n",
    "        bkd_nn = get_nearest_neighbors(xyz2, xyz1)\n",
    "        bkd_nn_end_point_error = np.linalg.norm(bkd_nn - xyz1, axis=1)\n",
    "        \n",
    "        fwd_nnepes = sorted(fwd_nn_end_point_error.tolist())\n",
    "        def percentile(slst, p):\n",
    "            idx = int(p * len(slst))\n",
    "            return slst[idx]\n",
    "        \n",
    "        bin_counts, bin_edges = np.histogram(fwd_nn_end_point_error, bins=1000)\n",
    "        fwd_nnepe_hist = bin_edges, bin_counts\n",
    "        \n",
    "        bin_counts, bin_edges = np.histogram(fwd_nn_dist, bins=1000)\n",
    "        fwd_nn_hist = bin_edges, bin_counts\n",
    "        \n",
    "        bin_counts, bin_edges = np.histogram(fwd_sf_dist, bins=1000)\n",
    "        sf_norm_hist = bin_edges, bin_counts\n",
    "        sf_norm_var = np.var(fwd_sf_dist)\n",
    "        \n",
    "        icp_fitness, icp_inlier_rmse = get_icp_results(xyz1, xyz2)\n",
    "        \n",
    "        print('end', fp.uri, time.time() - start)\n",
    "        return SFlowStats(\n",
    "                 opair=oflow_pair,\n",
    "                 sf_norm_hist=sf_norm_hist,\n",
    "                 sf_norm_var=sf_norm_var,\n",
    "\n",
    "                 fwd_nn_xyz=fwd_nn,\n",
    "                 fwd_nn_hist=fwd_nn_hist,\n",
    "                 \n",
    "                 fwd_nn_dist_to_sf_dist=fwd_nn_dist_to_sf_dist,\n",
    "                 \n",
    "                 fwd_nnepe_mean=np.mean(fwd_nn_end_point_error),\n",
    "                 fwd_nnepe_sum=np.sum(fwd_nn_end_point_error),\n",
    "                 fwd_nnepe_50th=percentile(fwd_nnepes, 0.5),\n",
    "                 fwd_nnepe_75th=percentile(fwd_nnepes, 0.75),\n",
    "                 fwd_nnepe_95th=percentile(fwd_nnepes, 0.95),\n",
    "                 \n",
    "                 bkd_nnepe_mean=np.mean(bkd_nn_end_point_error),\n",
    "                 bkd_nnepe_sum=np.sum(bkd_nn_end_point_error),\n",
    "            \n",
    "                 fwd_nnepe_hist=fwd_nnepe_hist,\n",
    "            \n",
    "                 icp_fitness=icp_fitness,\n",
    "                 icp_inlier_rmse=icp_inlier_rmse)\n",
    "                 \n",
    "    def to_html(self):\n",
    "        import numpy as np\n",
    "        \n",
    "        fwd_nn_xyz = self.fwd_nn_xyz\n",
    "        fp = self.opair\n",
    "        uvd2 = fp.uvdviz_im2[:, :3]\n",
    "        xyzrgb2 = uvd_to_xyzrgb(uvd2, fp.K, imgs=[fp.get_img2()])\n",
    "        fwd_nn_xyzrgb = np.ones((fwd_nn_xyz.shape[0] + xyzrgb2.shape[0], 3 + 3)) * 110.\n",
    "        fwd_nn_xyzrgb[:fwd_nn_xyz.shape[0], :3] = fwd_nn_xyz[:, :3]\n",
    "        fwd_nn_xyzrgb[fwd_nn_xyz.shape[0]:, :6] = xyzrgb2[:, :6]\n",
    "        \n",
    "        fwd_nn_html = create_xyzrgb_3d_plot_html(fwd_nn_xyzrgb)\n",
    "        \n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        fig = plt.figure()\n",
    "        bin_edges, bin_counts = self.sf_norm_hist\n",
    "        \n",
    "        plt.bar(bin_edges[:-1], bin_counts)\n",
    "        plt.xlim(left=0)\n",
    "        plt.title(\"Histogram of Scene Flow Vector L2 Norms\")\n",
    "        plt.xlabel('L2 Norm (meters)')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        sf_norm_hist_img = matplotlib_fig_to_img(fig)\n",
    "        sf_norm_hist_html = img_to_data_uri(matplotlib_fig_to_img(sf_norm_hist_img))\n",
    "                \n",
    "        \n",
    "        fig = plt.figure()\n",
    "        bin_edges, bin_counts = self.fwd_nn_hist\n",
    "        \n",
    "        plt.bar(bin_edges[:-1], bin_counts)\n",
    "        plt.xlim(left=0)\n",
    "        plt.title(\"Histogram of Nearest-Neighbor Distances (L2 Norms)\")\n",
    "        plt.xlabel('Distance (meters)')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        fwd_nn_hist_img = matplotlib_fig_to_img(fig)\n",
    "        fwd_nn_hist_html = img_to_data_uri(matplotlib_fig_to_img(fwd_nn_hist_img))\n",
    "        \n",
    "        \n",
    "        fig = plt.figure()\n",
    "        bin_edges, bin_counts = self.fwd_nnepe_hist\n",
    "        \n",
    "        plt.bar(bin_edges[:-1], bin_counts)\n",
    "        plt.xlim(left=0)\n",
    "        plt.title(\"Histogram of Forward Nearest-Neighbor End-Point-Errors\")\n",
    "        plt.xlabel('End Point Error (meters)')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        fepe_hist_img = matplotlib_fig_to_img(fig)\n",
    "        fepe_hist_html = img_to_data_uri(matplotlib_fig_to_img(fepe_hist_img))\n",
    "        \n",
    "        html = \"\"\"\n",
    "            <table>\n",
    "            <tr><td><img src=\"{sf_norm_hist}\" width=\"100%\" /></td></tr>\n",
    "            \n",
    "            <tr>\n",
    "              <td style=\"text-align:left\"><b>Variance of Scene Flow Displacements:</b> {sf_norm_var}</td>\n",
    "            </tr>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Forwards Nearest Neighbor End Point Error</b></td></tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:left\">\n",
    "                Ratio of total nearest-neighbor distance to total scene flow displacement:\n",
    "                  {fwd_nn_dist_to_sf_dist}\n",
    "                </td>\n",
    "            </tr>\n",
    "            <tr><td>\n",
    "                <img src=\"{fwd_nn_hist_html}\" width=\"100%\" /><br />\n",
    "                (Compare with Scene Flow Displacement histogram above)</td>\n",
    "            </tr>\n",
    "            <tr><td><img src=\"{fepe_hist}\" width=\"100%\" /></td></tr>\n",
    "            <tr>\n",
    "              <td style=\"text-align:left\">\n",
    "                Mean: {fwd_nnepe_mean}<br/>\n",
    "                Sum: {fwd_nnepe_sum}<br/>\n",
    "                75th percentile: {fwd_nnepe_50th}<br/>\n",
    "                50th percentile: {fwd_nnepe_75th}<br/>\n",
    "                95th percentile: {fwd_nnepe_95th}<br/>\n",
    "              </td>\n",
    "            <tr>\n",
    "            \n",
    "            <tr>\n",
    "                <td style=\"text-align:left\">\n",
    "                  Nearest Neighbor Cloud (Grey) vs Target Cloud (Colors)<br /> {fwd_nn_html}\n",
    "                </td>\n",
    "            </tr> \n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Backwards Nearest Neighbor End Point Error</b></td></tr>\n",
    "            <tr>\n",
    "              <td style=\"text-align:left\">\n",
    "                Mean: {bkd_nnepe_mean}<br/>\n",
    "                Sum: {bkd_nnepe_sum}<br/>\n",
    "              </td>\n",
    "            <tr>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>ICP (point-to-point) results:</b></td></tr>\n",
    "            <tr>\n",
    "              <td style=\"text-align:left\">\n",
    "                Fitness: {icp_fitness}<br/>\n",
    "                Inlier RMSE: {icp_inlier_rmse}<br/>\n",
    "              </td>\n",
    "            <tr>\n",
    "            \n",
    "            </table>\n",
    "        \"\"\".format(\n",
    "                sf_norm_hist=sf_norm_hist_html,\n",
    "                sf_norm_var=self.sf_norm_var,\n",
    "\n",
    "                fwd_nn_hist_html=fwd_nn_hist_html,\n",
    "                fwd_nn_dist_to_sf_dist=self.fwd_nn_dist_to_sf_dist,\n",
    "                fepe_hist=fepe_hist_html,\n",
    "                fwd_nnepe_mean=self.fwd_nnepe_mean, fwd_nnepe_sum=self.fwd_nnepe_sum,\n",
    "                fwd_nnepe_50th=self.fwd_nnepe_50th, fwd_nnepe_75th=self.fwd_nnepe_75th,\n",
    "                fwd_nnepe_95th=self.fwd_nnepe_95th,\n",
    "                fwd_nn_html=fwd_nn_html,\n",
    "                \n",
    "                bkd_nnepe_mean=self.bkd_nnepe_mean,\n",
    "                bkd_nnepe_sum=self.bkd_nnepe_sum,\n",
    "        \n",
    "                icp_fitness=self.icp_fitness,\n",
    "                icp_inlier_rmse=self.icp_inlier_rmse)\n",
    "        return html\n",
    "    \n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    %matplotlib agg\n",
    "    for fp in DEMO_FPS:\n",
    "        print('fixme html')\n",
    "        continue\n",
    "        sfstats = SFlowStats.create_from(fp)\n",
    "        show_html(fp.to_html() + \"<br />\" + sfstats.to_html() + \"</br></br></br>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on Full Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis_uris_full 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pyspark-3.0.1-py3.8.egg/pyspark/sql/session.py:401: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n",
      "2021-04-28 15:45:43,788\toarph 723307 : Progress for \n",
      "run_analysis [Pid:723307 Id:140496910224640]\n",
      "-----------------------  ---------------------------\n",
      "Thruput\n",
      "N thru                   200 (of 200)\n",
      "N chunks                 1\n",
      "Total time               6 minutes and 12.74 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       1\n",
      "Progress\n",
      "Percent Complete         100.000000\n",
      "Est. Time To Completion  0 seconds\n",
      "-----------------------  ---------------------------\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append('/opt/psegs')\n",
    "\n",
    "# from oarphpy.spark import NBSpark\n",
    "# NBSpark.SRC_ROOT = os.path.join(ALIB_SRC_DIR, 'cheap_optical_flow_eval_analysis')\n",
    "# NBSpark.CONF_KV.update({\n",
    "#     'spark.driver.maxResultSize': '2g',\n",
    "#     'spark.driver.memory': '16g',\n",
    "#   })\n",
    "# spark = NBSpark.getOrCreate()\n",
    "\n",
    "\n",
    "from oarphpy.spark import RowAdapter\n",
    "\n",
    "from pyspark import Row\n",
    "\n",
    "\n",
    "def flow_pair_to_full_row(fp):\n",
    "    from threadpoolctl import threadpool_limits\n",
    "    with threadpool_limits(limits=1, user_api='blas'):\n",
    "        recon = FlowReconstructedImagePair.create_from(fp)\n",
    "        fstats = OFlowStats.create_from(fp)\n",
    "        errors = OFlowReconErrors(recon)\n",
    "\n",
    "#         assert False, 'fixme look for nulls in diff_time_sec' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        rowdata = dict(\n",
    "                fp_dataset=fp.dataset,\n",
    "                fp_id1=fp.id1,\n",
    "                fp_id2=fp.id2,\n",
    "                fp_uri=str(fp.uri),\n",
    "                flow_coverage=fstats.coverage,\n",
    "                diff_time_sec=fp.diff_time_sec,\n",
    "                translation_meters=fp.translation_meters,\n",
    "        )\n",
    "        rowdata.update(\n",
    "            ('Forwards_' + k, float(v))\n",
    "            for k, v in errors.forward_stats.items())\n",
    "        rowdata.update(\n",
    "            ('Backwards_' + k, float(v))\n",
    "            for k, v in errors.backward_stats.items())\n",
    "        \n",
    "        if fp.has_scene_flow():\n",
    "            with threadpool_limits(limits=1, user_api='openmp'):\n",
    "                sflow = SFlowStats.create_from(fp)\n",
    "        else:\n",
    "            sflow = SFlowStats()\n",
    "        rowdata['has_scene_flow'] = fp.has_scene_flow()\n",
    "        rowdata.update(\n",
    "            ('SceneFlow_' + k, float(v))\n",
    "            for k, v in sflow.get_rowdata().items())\n",
    "        \n",
    "        return RowAdapter.to_row(rowdata)\n",
    "\n",
    "\n",
    "# analysis_uris_demo = MiddFactory.list_fp_uris(spark) + PSEGS_SYNTHFLOW_DEMO_URIS + KITTI_SF15_DEMO_URIS + DD_DEMO_URIS\n",
    "\n",
    "\n",
    "class UnionFactory(FlowPairUnionFactory):\n",
    "    #FACTORIES = ALL_FP_FACTORY_CLSS\n",
    "    FACTORIES = [KITTISF15Factory]\n",
    "\n",
    "analysis_uris_full = UnionFactory.list_fp_uris(spark)\n",
    "# analysis_uris_full = analysis_uris_full[4900:]\n",
    "print('analysis_uris_full', len(analysis_uris_full))\n",
    "\n",
    "ANALYSIS_FIXTURE_PATH = '/outer_root/media/rocket4q/SFTEST_222_flow_pq_eval_test.parquet'\n",
    "\n",
    "from oarphpy import util as oputil\n",
    "thru = oputil.ThruputObserver(name='run_analysis', n_total=len(analysis_uris_full))\n",
    "for uri_chunk in oputil.ichunked(analysis_uris_full, 500):\n",
    "    thru.start_block()\n",
    "    fp_rdd = UnionFactory.get_fp_rdd_for_uris(spark, uri_chunk)\n",
    "    fp_rdd = fp_rdd.coalesce(len(uri_chunk))\n",
    "    result_rdd = fp_rdd.map(flow_pair_to_full_row)\n",
    "    df = spark.createDataFrame(result_rdd)\n",
    "    df.write.save(\n",
    "            mode='append',\n",
    "            path=ANALYSIS_FIXTURE_PATH,\n",
    "            format='parquet',\n",
    "            compression='lz4')\n",
    "    thru.stop_block(n=len(uri_chunk))\n",
    "    thru.maybe_log_progress(every_n=1)\n",
    "\n",
    "\n",
    "# if True:#RUN_FULL_ANALYSIS:\n",
    "# #     spark = Spark.getOrCreate()\n",
    "    \n",
    "# #     for p in ALL_FPS:\n",
    "# #         import cloudpickle\n",
    "# #         try:\n",
    "# #             cloudpickle.dumps(p)\n",
    "# #         except Exception:\n",
    "# #             assert False, p\n",
    "# #     print('all good')\n",
    "    \n",
    "#     import pickle\n",
    "#     fp_rdd = spark.sparkContext.parallelize(ALL_FPS, numSlices=200)\n",
    "# #     print(fp_rdd.count())\n",
    "#     df = spark.createDataFrame(fp_rdd.map(flow_pair_to_full_row)).persist()\n",
    "\n",
    "#     print(df.count())\n",
    "#     df.show(10)\n",
    "#     df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+-------------------+------------------------+--------------------------------+------------------------+------------------------+------------------------+------------------------+---------------------+-------------------------+---------------------+-------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+------------------+\n",
      "|Backwards_Edges_MSE|     Backwards_MSE|    Backwards_PSNR|    Backwards_RMSE|     Backwards_SSIM|Forwards_Edges_MSE|      Forwards_MSE|     Forwards_PSNR|     Forwards_RMSE|      Forwards_SSIM|SceneFlow_bkd_nnepe_mean|SceneFlow_fwd_nn_dist_to_sf_dist|SceneFlow_fwd_nnepe_50th|SceneFlow_fwd_nnepe_75th|SceneFlow_fwd_nnepe_95th|SceneFlow_fwd_nnepe_mean|SceneFlow_icp_fitness|SceneFlow_icp_inlier_rmse|SceneFlow_sf_norm_var|diff_time_sec|      flow_coverage|          fp_dataset|              fp_id1|              fp_id2|              fp_uri|has_scene_flow|translation_meters|\n",
      "+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+-------------------+------------------------+--------------------------------+------------------------+------------------------+------------------------+------------------------+---------------------+-------------------------+---------------------+-------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+------------------+\n",
      "|  811.1008911132812| 9.284229254872242| 38.45334504946084| 3.047003323738299| 0.8823270396357886| 515.6905517578125| 9.222926339982596| 38.48211620761724| 3.036927121282728| 0.8722998622459163|    0.023502385567686242|              0.4516372489822675|    0.014841745615961136|    0.024736306419134813|     0.05026109576087432|    0.028856742108864318|   0.8067577140409549|     0.004805716307626801| 0.010959513758672133|         -1.0|0.19759440104166667|DeepDeform Semi-S...|train/seq068/colo...|train/seq068/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|     2964.068359375| 39.99987510927938| 32.11021725525835|6.3245454468506574| 0.7057834568107098| 820.1018676757812| 46.10823565958159| 31.49301856691616|  6.79030453364071| 0.7448881323281962|     0.16218178381959694|             0.31058247480887224|     0.16187639188822683|      0.2163551324582083|     0.37717035943338906|     0.18744157253820032|  0.18164106406893968|      0.00529975903075521| 0.018885630223170274|         -1.0|     0.130322265625|DeepDeform Semi-S...|train/seq068/colo...|train/seq068/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|   841.026123046875|12.563004591241588| 37.13986842378698| 3.544432901218697| 0.8649651113808204| 301.0058898925781| 13.87650966183575| 36.70800118468567| 3.725118744662477| 0.8638773591682978|      0.0416027806325623|              0.3427769611977106|     0.03977029917456017|     0.05434765927010286|     0.07864340424807686|       0.048838723266916|   0.7085797031495112|     0.005095339620014852| 0.010854373824340754|         -1.0|0.18883138020833334|DeepDeform Semi-S...|train/seq068/colo...|train/seq068/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|  1218.396240234375| 20.62229952301895|34.987432705083414| 4.541178208683177| 0.7975913597542976| 424.5088195800781|18.048768225238813| 35.56632792921845| 4.248384189929015| 0.8384507638352278|     0.16479757296645556|              0.3563456133831688|     0.15643705211157988|     0.19412013346111456|      1.1270000000000002|     0.20365815534450135|  0.30726539647402956|     0.005464643926824096|  0.05375319144929807|         -1.0|0.20587565104166666|DeepDeform Semi-S...|train/seq068/colo...|train/seq068/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|  1160.928955078125|15.126672039495359| 36.33336969781971|  3.88930225612453| 0.8488052477374051|373.55743408203125|16.481696752920136|35.960784415833615|4.0597656031992955| 0.8549989854192298|     0.06307232934447705|             0.35707238217651704|     0.05673035821949333|     0.08148884110857045|      0.1374408793957658|     0.07083283699413889|  0.48496875142557366|     0.005062560968521223| 0.010254700555098145|         -1.0|     0.214072265625|DeepDeform Semi-S...|train/seq068/colo...|train/seq068/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "| 1232.4525146484375| 17.28339321171024| 35.75451350245138| 4.157330058067346| 0.8213380732564264|407.11614990234375|16.061909416748126|36.072831885513104| 4.007731205650914| 0.8379804186744038|     0.13428528596140207|             0.31016282011084756|     0.13859244222132086|      0.1688770567699868|                   1.124|     0.19660213566418516|  0.23232810239980092|     0.005520291301752163|  0.06399654538920176|         -1.0|0.20930013020833332|DeepDeform Semi-S...|train/seq068/colo...|train/seq068/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|   2938.28369140625| 38.97537996964865| 32.22290003064316|  6.24302650720375| 0.7136208460719717| 539.3043212890625|  45.2356259267324|31.575997568276566| 6.725743522223577| 0.7455977243336881|      0.1607040935800729|             0.31927145888526687|     0.17201262714590435|     0.21648267078678415|     0.36462003935020865|     0.19686560577746462|  0.13873032747105452|     0.005664573355736924| 0.026858167818412396|         -1.0|0.14085611979166668|DeepDeform Semi-S...|train/seq068/colo...|train/seq068/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|  795.0248413085938|   8.0046786389414|39.097364597326354|2.8292540781876414| 0.8875598688394287| 493.8609924316406|  8.89937916734674| 38.63720650133104|2.9831827244315323| 0.8865101394120161|    0.047291449265690004|             0.28268190621942585|    0.046239654106448834|     0.05383414539624179|     0.06329266511259705|     0.06432518773485871|   0.9445179584120983|     0.004211624207165231| 0.023595913685847434|         -1.0|        0.206640625|DeepDeform Semi-S...|train/seq068/colo...|train/seq068/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "| 1131.6527099609375| 39.44406089105714| 32.17098740303875|   6.2804506917145| 0.7675057374982521|   727.43115234375| 32.44271829652387| 33.01963125325038| 5.695850972113287| 0.7718167834219257|      0.1455562253904747|             0.24662527761763414|      0.1661670005873874|     0.20609033672308022|                   1.133|     0.22566200209285456|  0.13541978548406983|     0.005351537087237936|  0.06683862344720685|         -1.0|0.18482747395833332|DeepDeform Semi-S...|train/seq068/colo...|train/seq068/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|   3216.49951171875| 67.74295022431104| 29.82216254502043| 8.230610562061058|  0.685408356189637|1905.8311767578125| 67.27773191074982| 29.85209018911129| 8.202300403591044| 0.6966224374466261|      0.1688901095044044|              0.2719197900886001|     0.16141998256836712|       0.173826047890295|     0.21791458536344246|     0.17920375398565777|   0.2561952574236274|     0.005668034228763772| 0.034557186135361885|         -1.0|0.12190104166666667|DeepDeform Semi-S...|train/seq069/colo...|train/seq069/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|    6519.4736328125| 74.08821127411912|  29.4333125121382|   8.6074509161609|0.48458539947756063|    3023.216796875|  73.5401947957552| 29.46555584959631| 8.575557987428876| 0.5136199172522997|      0.1910381140827987|              0.2710407557278911|     0.12251644728962513|     0.16203463758251307|      0.7300688520687051|     0.19273014934306956|  0.30956105912566045|      0.00584906694422502|  0.06448818590919994|         -1.0|0.11027669270833333|DeepDeform Semi-S...|train/seq069/colo...|train/seq069/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|   7788.15771484375| 88.44043186895011| 28.66429506225775| 9.404277317739526|0.19300380281828838|  4099.98095703125| 91.44351723072168|28.519274387068776| 9.562610377439922|0.09829187815518842|     0.13658909124346802|              0.5391316954564738|     0.36354717915720386|      0.4566319732861115|      0.5944895163735789|     0.38178959271812757|                  NaN|                      NaN|  0.11459052623464928|         -1.0|      0.02623046875|DeepDeform Semi-S...|train/seq069/colo...|train/seq069/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|    9436.3056640625| 91.74412416851442|28.505021018174233| 9.578315309516304|0.30051534452024053|  3246.80126953125| 91.09141025641026|28.536029350927706| 9.544182010859299| 0.3147208946682977|     0.20335464775531636|             0.49674938686159087|     0.24995580062352463|      0.3092917718256536|      0.5407073359690687|      0.3009189066546911| 0.019733924611973392|     0.006005780245235893|  0.08598015133721634|         -1.0|      0.04404296875|DeepDeform Semi-S...|train/seq069/colo...|train/seq069/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|    3839.8916015625| 79.55337361530715| 29.12421759393853| 8.919269791597692|0.49169733508235275|  2455.31396484375|  80.1065876549823| 29.09412128621797| 8.950228357700283|0.46999112793603814|     0.12212842181062031|              0.6909344527962076|     0.04993886356411133|     0.08510125378226202|      0.2765094936303424|      0.0884927925560589|  0.40334515521695347|     0.005107325498483401|  0.03553895376990518|         -1.0|      0.14869140625|DeepDeform Semi-S...|train/seq069/colo...|train/seq069/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|      4418.88671875| 88.88582903040735|28.642478334687173| 9.427928140922976|0.38070384152906617|     2897.48828125|  90.9848260016275| 28.54111391989162| 9.538596647391454| 0.3402721466383742|     0.24684961944770897|              0.5655846371817075|     0.14840460308037087|     0.24910934050285977|      0.5145269709949031|      0.2147309809650449|  0.11087420042643924|     0.005920711904998024|  0.07170448630484601|         -1.0|0.12671549479166666|DeepDeform Semi-S...|train/seq069/colo...|train/seq069/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|  3796.372314453125| 82.39300515158745| 28.97190017532608| 9.077059278840666| 0.5105871529312278|  2179.34619140625| 82.60027051613521|  28.9609889122946| 9.088469096395455| 0.5166696991870318|      0.1042429189892928|             0.45681108907227674|     0.08879959268648942|     0.12880484054410712|      0.2793154777670644|     0.14152953557241993|   0.3157755215650929|     0.005482685391001725| 0.057774115408553976|         -1.0|0.12279622395833334|DeepDeform Semi-S...|train/seq069/colo...|train/seq069/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|  3857.495849609375|  72.2214663643235| 29.54414058894673| 8.498321385092677| 0.5897218015370079|  2612.37060546875| 70.68163544895117|29.637737711050896| 8.407237087709087| 0.5828058539238902|     0.19763100518199742|              0.2726893416854092|     0.15324284221907497|      0.2936212418189004|      0.6647001189281929|      0.2468005671292193|  0.22448979591836735|      0.00588505825366651|  0.08314662919092253|         -1.0|      0.12345703125|DeepDeform Semi-S...|train/seq070/colo...|train/seq070/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|    4106.9931640625| 75.43478699472405|  29.3550869250729| 8.685320201047515| 0.5555172081135071| 3242.009521484375| 74.27530868956302|29.422358954525215| 8.618312403804065| 0.5811231667996389|     0.09316757027690378|              0.3289237338531239|     0.06748676030639669|     0.10039278300272479|      0.5777851362480886|      0.1551991860686198|  0.38800956329631087|     0.005674513951334...|  0.10457658125305022|         -1.0|0.10756184895833333|DeepDeform Semi-S...|train/seq070/colo...|train/seq070/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|       5265.4140625|  77.7496444091306|29.223819494263413| 8.817575880542826| 0.5451489078405483| 3410.913818359375| 76.29038751233702| 29.30610539994793| 8.734436874369006| 0.5593972845410912|     0.21756224158711382|             0.29214377003642267|     0.14948533856885693|      0.2821403276061806|      0.8665706260469409|     0.26773200653237783|   0.2988728995615197|     0.005845719541346...|   0.0969860672916381|         -1.0|     0.110615234375|DeepDeform Semi-S...|train/seq070/colo...|train/seq070/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "|   4437.24169921875|   81.190022453797|29.035776993546147| 9.010550618791118| 0.5852403691867897|        2196.78125| 79.28582702020202|29.138848002468457| 8.904258925941114| 0.6019345513446139|      0.1698272598806962|              0.3029748979407533|     0.12957951603405538|     0.19861782079604023|      0.3357093955491163|     0.17143672923380832|   0.2523461339167482|       0.0059681274930809| 0.045513160452832706|         -1.0|0.11307942708333334|DeepDeform Semi-S...|train/seq070/colo...|train/seq070/colo...|psegs://dataset=d...|          true|              -1.0|\n",
      "+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+-------------------+------------------------+--------------------------------+------------------------+------------------------+------------------------+------------------------+---------------------+-------------------------+---------------------+-------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Backwards_Edges_MSE': 811.1008911132812,\n",
      " 'Backwards_MSE': 9.284229254872242,\n",
      " 'Backwards_PSNR': 38.45334504946084,\n",
      " 'Backwards_RMSE': 3.047003323738299,\n",
      " 'Backwards_SSIM': 0.8823270396357886,\n",
      " 'Forwards_Edges_MSE': 515.6905517578125,\n",
      " 'Forwards_MSE': 9.222926339982596,\n",
      " 'Forwards_PSNR': 38.48211620761724,\n",
      " 'Forwards_RMSE': 3.036927121282728,\n",
      " 'Forwards_SSIM': 0.8722998622459163,\n",
      " 'SceneFlow_bkd_nnepe_mean': 0.023502385567686242,\n",
      " 'SceneFlow_fwd_nn_dist_to_sf_dist': 0.4516372489822675,\n",
      " 'SceneFlow_fwd_nnepe_50th': 0.014841745615961136,\n",
      " 'SceneFlow_fwd_nnepe_75th': 0.024736306419134813,\n",
      " 'SceneFlow_fwd_nnepe_95th': 0.05026109576087432,\n",
      " 'SceneFlow_fwd_nnepe_mean': 0.028856742108864318,\n",
      " 'SceneFlow_icp_fitness': 0.8067577140409549,\n",
      " 'SceneFlow_icp_inlier_rmse': 0.004805716307626801,\n",
      " 'SceneFlow_sf_norm_var': 0.010959513758672133,\n",
      " 'diff_time_sec': -1.0,\n",
      " 'flow_coverage': 0.19759440104166667,\n",
      " 'fp_dataset': 'DeepDeform Semi-Synthetic Optical Flow',\n",
      " 'fp_id1': 'train/seq068/color/000200.jpg',\n",
      " 'fp_id2': 'train/seq068/color/000400.jpg',\n",
      " 'fp_uri': 'psegs://dataset=deep_deform&extra.dd.K=train/seq068/scene_flow/../intrinsics.txt&extra.dd.expected_out=train/seq068/color/000400.jpg&extra.dd.flow_gt=train/seq068/optical_flow/Jacket2_000200_000400.oflow&extra.dd.input=train/seq068/color/000200.jpg&extra.dd.sf_gt=train/seq068/scene_flow/Jacket2_000200_000400.sflow',\n",
      " 'has_scene_flow': True,\n",
      " 'translation_meters': -1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5474"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results_df = spark.read.parquet(ANALYSIS_FIXTURE_PATH)\n",
    "\n",
    "# from oarphpy import util as oputil\n",
    "# ANALYSIS_FIXTURE_PATH = '/outer_root/media/rocket4q/SFTEST_222_flow_pq_eval_test.parquet'\n",
    "results_dfs = []\n",
    "for path in oputil.all_files_recursive(ANALYSIS_FIXTURE_PATH, pattern='*.lz4.parquet'):\n",
    "    df = spark.read.parquet(path)\n",
    "    df = df.withColumn('diff_time_sec', df.diff_time_sec.cast('float'))\n",
    "    results_dfs.append(df)\n",
    "\n",
    "from oarphpy import spark as S\n",
    "results_df = S.union_dfs(*results_dfs)\n",
    "\n",
    "# def add_dataset(row):\n",
    "#     from psegs import datum\n",
    "#     row = row.asDict()\n",
    "#     uri = datum.URI.from_str(row['fp_uri'])\n",
    "#     row['fp_dataset'] = uri.dataset\n",
    "#     return row\n",
    "\n",
    "# results_df = spark.createDataFrame(results_df.rdd.map(add_dataset))\n",
    "results_df = results_df.persist()\n",
    "\n",
    "results_df.show()\n",
    "import pprint\n",
    "pprint.pprint(results_df.take(1)[0].asDict())\n",
    "results_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering 22 histograms\n",
      "Working on Backwards_Edges_MSE\n",
      "total chosen_fp_uris, added 512 512\n",
      "Working on Backwards_MSE\n",
      "total chosen_fp_uris, added 708 631\n",
      "Working on Backwards_PSNR\n",
      "total chosen_fp_uris, added 738 565\n",
      "Working on Backwards_RMSE\n",
      "total chosen_fp_uris, added 752 622\n",
      "Working on Backwards_SSIM\n",
      "total chosen_fp_uris, added 760 636\n",
      "Working on Forwards_Edges_MSE\n",
      "total chosen_fp_uris, added 762 543\n",
      "Working on Forwards_MSE\n",
      "total chosen_fp_uris, added 762 630\n",
      "Working on Forwards_PSNR\n",
      "total chosen_fp_uris, added 762 570\n",
      "Working on Forwards_RMSE\n",
      "total chosen_fp_uris, added 762 620\n",
      "Working on Forwards_SSIM\n",
      "total chosen_fp_uris, added 762 649\n",
      "Working on SceneFlow_bkd_nnepe_mean\n",
      "total chosen_fp_uris, added 762 451\n",
      "Working on SceneFlow_fwd_nn_dist_to_sf_dist\n",
      "total chosen_fp_uris, added 762 491\n",
      "Working on SceneFlow_fwd_nnepe_50th\n",
      "total chosen_fp_uris, added 762 407\n",
      "Working on SceneFlow_fwd_nnepe_75th\n",
      "total chosen_fp_uris, added 762 386\n",
      "Working on SceneFlow_fwd_nnepe_95th\n",
      "total chosen_fp_uris, added 762 394\n",
      "Working on SceneFlow_fwd_nnepe_mean\n",
      "total chosen_fp_uris, added 762 433\n",
      "Working on SceneFlow_icp_fitness\n",
      "total chosen_fp_uris, added 762 353\n",
      "Working on SceneFlow_icp_inlier_rmse\n",
      "total chosen_fp_uris, added 762 319\n",
      "Working on SceneFlow_sf_norm_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:17:03,177\tps   1807391 : FlowRecTable: Reading parquet from /outer_root/media/rocket4q/psegs_flow_records_FULL_fixed \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chosen_fp_uris, added 762 136\n",
      "Working on diff_time_sec\n",
      "total chosen_fp_uris, added 764 188\n",
      "Working on flow_coverage\n",
      "total chosen_fp_uris, added 764 624\n",
      "Working on translation_meters\n",
      "total chosen_fp_uris, added 764 523\n",
      "Rendering 764 histogram bucket pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:17:05,517\tps   1807391 : FlowRecTable: Have 3 StampedDatumTables\n",
      "2021-04-28 21:24:57,221\tps   1807391 : Building union DF for 42 segments ...\n",
      "2021-04-28 21:24:57,334\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0283\n",
      "2021-04-28 21:24:58,330\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:25:03,059\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:25:03,063\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:25:07,867\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:25:07,867\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:25:07,868\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:25:12,344\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0283\n",
      "2021-04-28 21:25:12,354\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  ----------------------------\n",
      "Thruput\n",
      "N thru                   1 (of 42)\n",
      "N chunks                 1\n",
      "Total time               15.12 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         2.380952\n",
      "Est. Time To Completion  10 minutes and 20.01 seconds\n",
      "-----------------------  ----------------------------\n",
      "2021-04-28 21:25:12,420\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-1013\n",
      "2021-04-28 21:25:13,329\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:25:17,818\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:25:17,822\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:25:32,082\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:25:32,083\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:25:32,083\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:25:36,240\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-1013\n",
      "2021-04-28 21:25:36,258\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  ---------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   2 (of 42)\n",
      "N chunks                 2\n",
      "Total time               39.01 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         4.761905\n",
      "Est. Time To Completion  13 minutes and 0.16 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      19 seconds, 503 milliseconds, 955 microseconds and 364.23 nanoseconds\n",
      "p50                      19 seconds, 503 milliseconds, 955 microseconds and 364.23 nanoseconds\n",
      "p95                      23 seconds, 447 milliseconds, 626 microseconds and 161.58 nanoseconds\n",
      "p99                      23 seconds, 798 milliseconds, 174 microseconds and 676.9 nanoseconds\n",
      "-----------------------  ---------------------------------------------------------------------\n",
      "2021-04-28 21:25:36,324\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0901\n",
      "2021-04-28 21:25:37,171\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:25:41,319\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:25:41,322\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:25:58,093\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:25:58,093\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:25:58,094\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:26:02,335\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0901\n",
      "2021-04-28 21:26:02,341\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  ---------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   3 (of 42)\n",
      "N chunks                 3\n",
      "Total time               1 minute and 5.08 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         7.142857\n",
      "Est. Time To Completion  14 minutes and 6.1 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      21 seconds, 694 milliseconds, 882 microseconds and 710.77 nanoseconds\n",
      "p50                      23 seconds, 885 milliseconds, 811 microseconds and 805.73 nanoseconds\n",
      "p95                      25 seconds, 857 milliseconds, 644 microseconds and 844.06 nanoseconds\n",
      "p99                      26 seconds, 32 milliseconds, 918 microseconds and 891.91 nanoseconds\n",
      "-----------------------  ---------------------------------------------------------------------\n",
      "2021-04-28 21:26:02,411\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=val&segment_id=scene-1069\n",
      "2021-04-28 21:26:03,256\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:26:07,405\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:26:07,408\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:26:30,429\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:26:30,429\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:26:30,430\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:26:34,811\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=val&segment_id=scene-1069\n",
      "2021-04-28 21:26:34,816\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  ---------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   4 (of 42)\n",
      "N chunks                 4\n",
      "Total time               1 minute and 37.55 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         9.523810\n",
      "Est. Time To Completion  15 minutes and 26.77 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      24 seconds, 388 milliseconds, 715 microseconds and 386.39 nanoseconds\n",
      "p50                      24 seconds, 981 milliseconds, 274 microseconds and 604.8 nanoseconds\n",
      "p95                      31 seconds, 511 milliseconds, 192 microseconds and 11.83 nanoseconds\n",
      "p99                      32 seconds, 278 milliseconds, 409 microseconds and 132.96 nanoseconds\n",
      "-----------------------  ---------------------------------------------------------------------\n",
      "2021-04-28 21:26:34,865\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0683\n",
      "2021-04-28 21:26:35,690\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:26:39,896\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:26:39,899\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:27:08,593\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:27:08,594\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:27:08,594\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:27:13,081\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0683\n",
      "2021-04-28 21:27:13,087\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   5 (of 42)\n",
      "N chunks                 5\n",
      "Total time               2 minutes and 15.82 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         11.904762\n",
      "Est. Time To Completion  16 minutes and 45.07 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      27 seconds, 164 milliseconds, 24 microseconds and 162.29 nanoseconds\n",
      "p50                      26 seconds, 76 milliseconds, 737 microseconds and 403.87 nanoseconds\n",
      "p95                      37 seconds, 106 milliseconds, 250 microseconds and 95.37 nanoseconds\n",
      "p99                      38 seconds, 33 milliseconds, 457 microseconds and 431.79 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:27:13,141\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0174\n",
      "2021-04-28 21:27:14,012\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:27:18,281\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:27:18,285\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:28:00,110\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:28:00,110\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:28:00,111\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:28:04,564\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0174\n",
      "2021-04-28 21:28:04,568\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  ---------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   6 (of 42)\n",
      "N chunks                 6\n",
      "Total time               3 minutes and 7.3 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         14.285714\n",
      "Est. Time To Completion  18 minutes and 43.78 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      31 seconds, 216 milliseconds, 205 microseconds and 477.71 nanoseconds\n",
      "p50                      29 seconds, 273 milliseconds, 475 microseconds and 408.55 nanoseconds\n",
      "p95                      48 seconds, 174 milliseconds, 148 microseconds and 857.59 nanoseconds\n",
      "p99                      50 seconds, 816 milliseconds, 519 microseconds and 415.38 nanoseconds\n",
      "-----------------------  ---------------------------------------------------------------------\n",
      "2021-04-28 21:28:04,614\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0582\n",
      "2021-04-28 21:28:05,483\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:28:09,587\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:28:09,591\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:29:00,658\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:29:00,659\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:29:00,659\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:29:05,054\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0582\n",
      "2021-04-28 21:29:05,058\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  ---------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   7 (of 42)\n",
      "N chunks                 7\n",
      "Total time               4 minutes and 7.78 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         16.666667\n",
      "Est. Time To Completion  20 minutes and 38.91 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      35 seconds, 397 milliseconds, 565 microseconds and 398.9 nanoseconds\n",
      "p50                      32 seconds, 470 milliseconds, 213 microseconds and 413.24 nanoseconds\n",
      "p95                      57 seconds, 783 milliseconds, 141 microseconds and 64.64 nanoseconds\n",
      "p99                      59 seconds, 945 milliseconds, 208 microseconds and 153.72 nanoseconds\n",
      "-----------------------  ---------------------------------------------------------------------\n",
      "2021-04-28 21:29:05,059\tps   1807391 : Building DF for psegs://dataset=kitti-360-fused&split=train&segment_id=2013_05_28_drive_0005_sync\n",
      "2021-04-28 21:29:05,902\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:29:10,093\tps   1807391 : Creating datums for KITTI-360 ...\n",
      "2021-04-28 21:29:10,094\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:29:14,391\tps   1807391 : ... seq 2013_05_28_drive_0005_sync has 71770 URIs spanning 705 sec, creating 1121 slices ...\n",
      "2021-04-28 21:29:14,785\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:31:40,748\tps   1807391 : ... partitioned datums into 0 RDDs.\n",
      "2021-04-28 21:31:40,749\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:31:40,749\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:31:45,170\tps   1807391 : Added DF for psegs://dataset=kitti-360-fused&split=train&segment_id=2013_05_28_drive_0005_sync\n",
      "2021-04-28 21:31:45,175\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   8 (of 42)\n",
      "N chunks                 8\n",
      "Total time               6 minutes and 47.89 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         19.047619\n",
      "Est. Time To Completion  28 minutes and 53.55 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      50 seconds, 986 milliseconds, 830 microseconds and 711.36 nanoseconds\n",
      "p50                      35 seconds, 367 milliseconds, 736 microseconds and 339.57 nanoseconds\n",
      "p95                      2 minutes, 5 seconds, 242 milliseconds, 600 microseconds and 858.21 nanoseconds\n",
      "p99                      2 minutes, 33 seconds, 137 milliseconds, 870 microseconds and 490.55 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:31:45,226\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0997\n",
      "2021-04-28 21:31:46,099\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:31:50,288\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:31:50,291\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:32:58,737\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:32:58,738\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:32:58,738\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:33:02,962\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0997\n",
      "2021-04-28 21:33:02,967\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   9 (of 42)\n",
      "N chunks                 9\n",
      "Total time               8 minutes and 5.68 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         21.428571\n",
      "Est. Time To Completion  29 minutes and 40.83 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      53 seconds, 964 milliseconds, 628 microseconds and 828.9 nanoseconds\n",
      "p50                      38 seconds, 265 milliseconds, 259 microseconds and 265.9 nanoseconds\n",
      "p95                      2 minutes, 7 seconds, 181 milliseconds, 818 microseconds and 246.84 nanoseconds\n",
      "p99                      2 minutes, 33 seconds, 525 milliseconds, 713 microseconds and 968.28 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:33:03,015\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0704\n",
      "2021-04-28 21:33:03,859\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:33:07,966\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:33:07,970\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:34:26,467\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:34:26,468\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:34:26,468\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:34:30,924\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0704\n",
      "2021-04-28 21:34:30,929\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   10 (of 42)\n",
      "N chunks                 10\n",
      "Total time               9 minutes and 33.64 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         23.809524\n",
      "Est. Time To Completion  30 minutes and 35.65 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      57 seconds, 363 milliseconds, 916 microseconds and 683.2 nanoseconds\n",
      "p50                      44 seconds, 871 milliseconds, 185 microseconds and 660.36 nanoseconds\n",
      "p95                      2 minutes, 7 seconds, 642 milliseconds, 306 microseconds and 661.61 nanoseconds\n",
      "p99                      2 minutes, 33 seconds, 617 milliseconds, 811 microseconds and 651.23 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:34:30,974\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0716\n",
      "2021-04-28 21:34:31,778\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:34:36,035\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:34:36,038\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:34:57,956\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:34:57,957\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:34:57,957\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:35:02,093\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0716\n",
      "2021-04-28 21:35:02,098\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   11 (of 42)\n",
      "N chunks                 11\n",
      "Total time               10 minutes and 4.8 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         26.190476\n",
      "Est. Time To Completion  28 minutes and 24.45 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      54 seconds, 982 milliseconds, 100 microseconds and 876.89 nanoseconds\n",
      "p50                      38 seconds, 265 milliseconds, 259 microseconds and 265.9 nanoseconds\n",
      "p95                      2 minutes, 4 seconds, 34 milliseconds, 597 microseconds and 635.27 nanoseconds\n",
      "p99                      2 minutes, 32 seconds, 896 milliseconds, 269 microseconds and 845.96 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:35:02,143\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=val&segment_id=scene-0270\n",
      "2021-04-28 21:35:02,982\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:35:07,217\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:35:07,221\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:35:11,265\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:35:11,265\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:35:11,266\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:35:15,507\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=val&segment_id=scene-0270\n",
      "2021-04-28 21:35:15,514\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  ------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   12 (of 42)\n",
      "N chunks                 12\n",
      "Total time               10 minutes and 18.21 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         28.571429\n",
      "Est. Time To Completion  25 minutes and 45.53 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      51 seconds, 517 milliseconds, 712 microseconds and 255.32 nanoseconds\n",
      "p50                      35 seconds, 367 milliseconds, 736 microseconds and 339.57 nanoseconds\n",
      "p95                      2 minutes, 426 milliseconds, 888 microseconds and 608.93 nanoseconds\n",
      "p99                      2 minutes, 32 seconds, 174 milliseconds, 728 microseconds and 40.7 nanoseconds\n",
      "-----------------------  ------------------------------------------------------------------------------\n",
      "2021-04-28 21:35:15,562\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0129\n",
      "2021-04-28 21:35:16,365\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:35:20,618\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:35:20,622\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:35:24,749\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:35:24,750\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:35:24,750\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:35:28,838\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0129\n",
      "2021-04-28 21:35:28,842\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   13 (of 42)\n",
      "N chunks                 13\n",
      "Total time               10 minutes and 31.54 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         30.952381\n",
      "Est. Time To Completion  23 minutes and 28.81 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      48 seconds, 579 milliseconds, 754 microseconds and 242.53 nanoseconds\n",
      "p50                      32 seconds, 470 milliseconds, 213 microseconds and 413.24 nanoseconds\n",
      "p95                      1 minute, 56 seconds, 819 milliseconds, 179 microseconds and 582.6 nanoseconds\n",
      "p99                      2 minutes, 31 seconds, 453 milliseconds, 186 microseconds and 235.43 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:35:28,888\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0514\n",
      "2021-04-28 21:35:29,723\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:35:33,736\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:35:33,739\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:35:37,918\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:35:37,919\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:35:37,919\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:35:42,156\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0514\n",
      "2021-04-28 21:35:42,161\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   14 (of 42)\n",
      "N chunks                 14\n",
      "Total time               10 minutes and 44.85 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         33.333333\n",
      "Est. Time To Completion  21 minutes and 29.7 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      46 seconds, 60 milliseconds, 741 microseconds and 748.13 nanoseconds\n",
      "p50                      31 seconds, 817 milliseconds, 78 microseconds and 113.56 nanoseconds\n",
      "p95                      1 minute, 53 seconds, 211 milliseconds, 470 microseconds and 556.26 nanoseconds\n",
      "p99                      2 minutes, 30 seconds, 731 milliseconds, 644 microseconds and 430.16 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:35:42,205\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0998\n",
      "2021-04-28 21:35:43,008\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:35:47,109\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:35:47,113\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:35:51,434\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:35:51,435\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:35:51,435\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:35:55,579\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0998\n",
      "2021-04-28 21:35:55,584\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  -------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   15 (of 42)\n",
      "N chunks                 15\n",
      "Total time               10 minutes and 58.27 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         35.714286\n",
      "Est. Time To Completion  19 minutes and 44.88 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      43 seconds, 884 milliseconds, 610 microseconds and 748.29 nanoseconds\n",
      "p50                      31 seconds, 163 milliseconds, 942 microseconds and 813.87 nanoseconds\n",
      "p95                      1 minute, 49 seconds, 603 milliseconds, 761 microseconds and 529.92 nanoseconds\n",
      "p99                      2 minutes, 30 seconds, 10 milliseconds, 102 microseconds and 624.89 nanoseconds\n",
      "-----------------------  -------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:35:55,679\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0393\n",
      "2021-04-28 21:35:56,461\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:36:00,444\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:36:00,447\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:36:04,484\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:36:04,485\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:36:04,485\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:36:08,445\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0393\n",
      "2021-04-28 21:36:08,450\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   16 (of 42)\n",
      "N chunks                 16\n",
      "Total time               11 minutes and 11.13 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         38.095238\n",
      "Est. Time To Completion  18 minutes and 10.59 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      41 seconds, 945 milliseconds, 633 microseconds and 515.72 nanoseconds\n",
      "p50                      28 seconds, 620 milliseconds, 340 microseconds and 108.87 nanoseconds\n",
      "p95                      1 minute, 45 seconds, 996 milliseconds, 52 microseconds and 503.59 nanoseconds\n",
      "p99                      2 minutes, 29 seconds, 288 milliseconds, 560 microseconds and 819.63 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:36:08,495\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0248\n",
      "2021-04-28 21:36:09,254\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:36:13,281\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:36:13,285\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:36:17,075\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:36:17,075\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:36:17,075\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:36:21,123\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0248\n",
      "2021-04-28 21:36:21,129\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  -------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   17 (of 42)\n",
      "N chunks                 17\n",
      "Total time               11 minutes and 23.8 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         40.476190\n",
      "Est. Time To Completion  16 minutes and 45.59 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      40 seconds, 223 milliseconds, 721 microseconds and 588.36 nanoseconds\n",
      "p50                      26 seconds, 76 milliseconds, 737 microseconds and 403.87 nanoseconds\n",
      "p95                      1 minute, 42 seconds, 388 milliseconds, 343 microseconds and 477.25 nanoseconds\n",
      "p99                      2 minutes, 28 seconds, 567 milliseconds, 19 microseconds and 14.36 nanoseconds\n",
      "-----------------------  -------------------------------------------------------------------------------\n",
      "2021-04-28 21:36:21,130\tps   1807391 : Building DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0007_sync\n",
      "2021-04-28 21:36:22,003\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:36:25,954\tps   1807391 : Creating datums for KITTI-360 ...\n",
      "2021-04-28 21:36:25,955\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:36:30,257\tps   1807391 : ... seq 2013_05_28_drive_0007_sync has 29299 URIs spanning 355 sec, creating 114 slices ...\n",
      "2021-04-28 21:36:30,406\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:36:38,133\tps   1807391 : ... partitioned datums into 0 RDDs.\n",
      "2021-04-28 21:36:38,134\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:36:38,134\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:36:42,330\tps   1807391 : Added DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0007_sync\n",
      "2021-04-28 21:36:42,334\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   18 (of 42)\n",
      "N chunks                 18\n",
      "Total time               11 minutes and 45 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         42.857143\n",
      "Est. Time To Completion  15 minutes and 40.01 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      39 seconds, 166 milliseconds, 892 microseconds and 329.85 nanoseconds\n",
      "p50                      24 seconds, 981 milliseconds, 274 microseconds and 604.8 nanoseconds\n",
      "p95                      1 minute, 38 seconds, 780 milliseconds, 634 microseconds and 450.91 nanoseconds\n",
      "p99                      2 minutes, 27 seconds, 845 milliseconds, 477 microseconds and 209.09 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:36:42,380\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0073\n",
      "2021-04-28 21:36:43,160\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:36:47,387\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:36:47,390\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:36:51,165\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:36:51,165\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:36:51,165\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:36:55,154\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0073\n",
      "2021-04-28 21:36:55,159\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   19 (of 42)\n",
      "N chunks                 19\n",
      "Total time               11 minutes and 57.82 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         45.238095\n",
      "Est. Time To Completion  14 minutes and 28.94 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      37 seconds, 780 milliseconds, 195 microseconds and 863.62 nanoseconds\n",
      "p50                      23 seconds, 885 milliseconds, 811 microseconds and 805.73 nanoseconds\n",
      "p95                      1 minute, 35 seconds, 172 milliseconds, 925 microseconds and 424.58 nanoseconds\n",
      "p99                      2 minutes, 27 seconds, 123 milliseconds, 935 microseconds and 403.82 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:36:55,159\tps   1807391 : Building DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0002_sync\n",
      "2021-04-28 21:36:55,951\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:36:59,963\tps   1807391 : Creating datums for KITTI-360 ...\n",
      "2021-04-28 21:36:59,963\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:37:06,930\tps   1807391 : ... seq 2013_05_28_drive_0002_sync has 153214 URIs spanning 2013 sec, creating 598 slices ...\n",
      "2021-04-28 21:37:07,712\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:37:42,287\tps   1807391 : ... partitioned datums into 0 RDDs.\n",
      "2021-04-28 21:37:42,287\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:37:42,288\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:37:46,828\tps   1807391 : Added DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0002_sync\n",
      "2021-04-28 21:37:46,833\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   20 (of 42)\n",
      "N chunks                 20\n",
      "Total time               12 minutes and 49.49 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         47.619048\n",
      "Est. Time To Completion  14 minutes and 6.44 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      38 seconds, 474 milliseconds, 642 microseconds and 348.29 nanoseconds\n",
      "p50                      24 seconds, 981 milliseconds, 274 microseconds and 604.8 nanoseconds\n",
      "p95                      1 minute, 31 seconds, 565 milliseconds, 216 microseconds and 398.24 nanoseconds\n",
      "p99                      2 minutes, 26 seconds, 402 milliseconds, 393 microseconds and 598.56 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:37:46,879\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0029\n",
      "2021-04-28 21:37:47,715\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:37:51,699\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:37:51,702\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:37:55,703\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:37:55,704\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:37:55,704\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:37:59,894\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0029\n",
      "2021-04-28 21:37:59,898\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   21 (of 42)\n",
      "N chunks                 21\n",
      "Total time               13 minutes and 2.55 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         50.000000\n",
      "Est. Time To Completion  13 minutes and 2.55 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      37 seconds, 264 milliseconds, 479 microseconds and 671.21 nanoseconds\n",
      "p50                      23 seconds, 885 milliseconds, 811 microseconds and 805.73 nanoseconds\n",
      "p95                      1 minute, 27 seconds, 957 milliseconds, 507 microseconds and 371.9 nanoseconds\n",
      "p99                      2 minutes, 25 seconds, 680 milliseconds, 851 microseconds and 793.29 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:37:59,944\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0583\n",
      "2021-04-28 21:38:00,761\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:38:04,851\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:38:04,854\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:38:08,783\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:38:08,783\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:38:08,784\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:38:12,920\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0583\n",
      "2021-04-28 21:38:12,926\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   22 (of 42)\n",
      "N chunks                 22\n",
      "Total time               13 minutes and 15.58 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         52.380952\n",
      "Est. Time To Completion  12 minutes and 3.25 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      36 seconds, 162 milliseconds, 542 microseconds and 386.49 nanoseconds\n",
      "p50                      22 seconds, 543 milliseconds, 303 microseconds and 370.48 nanoseconds\n",
      "p95                      1 minute, 27 seconds, 448 milliseconds, 982 microseconds and 691.76 nanoseconds\n",
      "p99                      2 minutes, 24 seconds, 959 milliseconds, 309 microseconds and 988.02 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:38:12,927\tps   1807391 : Building DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0003_sync\n",
      "2021-04-28 21:38:13,708\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:38:17,796\tps   1807391 : Creating datums for KITTI-360 ...\n",
      "2021-04-28 21:38:17,797\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:38:18,509\tps   1807391 : ... seq 2013_05_28_drive_0003_sync has 9175 URIs spanning 108 sec, creating 35 slices ...\n",
      "2021-04-28 21:38:18,556\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:38:21,157\tps   1807391 : ... partitioned datums into 0 RDDs.\n",
      "2021-04-28 21:38:21,157\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:38:21,158\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:38:25,059\tps   1807391 : Added DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0003_sync\n",
      "2021-04-28 21:38:25,064\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   23 (of 42)\n",
      "N chunks                 23\n",
      "Total time               13 minutes and 27.71 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         54.761905\n",
      "Est. Time To Completion  11 minutes and 7.24 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      35 seconds, 117 milliseconds, 760 microseconds and 347.28 nanoseconds\n",
      "p50                      21 seconds, 200 milliseconds, 794 microseconds and 935.23 nanoseconds\n",
      "p95                      1 minute, 26 seconds, 940 milliseconds, 458 microseconds and 11.63 nanoseconds\n",
      "p99                      2 minutes, 24 seconds, 237 milliseconds, 768 microseconds and 182.75 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:38:25,108\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0259\n",
      "2021-04-28 21:38:25,902\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:38:29,812\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:38:29,815\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:38:33,697\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:38:33,698\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:38:33,698\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:38:37,786\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0259\n",
      "2021-04-28 21:38:37,791\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   24 (of 42)\n",
      "N chunks                 24\n",
      "Total time               13 minutes and 40.43 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         57.142857\n",
      "Est. Time To Completion  10 minutes and 15.32 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      34 seconds, 184 milliseconds, 624 microseconds and 532.86 nanoseconds\n",
      "p50                      18 seconds, 161 milliseconds, 446 microseconds and 928.98 nanoseconds\n",
      "p95                      1 minute, 26 seconds, 431 milliseconds, 933 microseconds and 331.49 nanoseconds\n",
      "p99                      2 minutes, 23 seconds, 516 milliseconds, 226 microseconds and 377.49 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:38:37,835\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=val&segment_id=scene-0105\n",
      "2021-04-28 21:38:38,597\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:38:42,635\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:38:42,638\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:38:46,860\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:38:46,861\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:38:46,861\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:38:50,798\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=val&segment_id=scene-0105\n",
      "2021-04-28 21:38:50,802\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   25 (of 42)\n",
      "N chunks                 25\n",
      "Total time               13 minutes and 53.44 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         59.523810\n",
      "Est. Time To Completion  9 minutes and 26.74 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      33 seconds, 337 milliseconds, 527 microseconds and 74.81 nanoseconds\n",
      "p50                      15 seconds, 122 milliseconds, 98 microseconds and 922.73 nanoseconds\n",
      "p95                      1 minute, 25 seconds, 923 milliseconds, 408 microseconds and 651.35 nanoseconds\n",
      "p99                      2 minutes, 22 seconds, 794 milliseconds, 684 microseconds and 572.22 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:38:50,803\tps   1807391 : Building DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0004_sync\n",
      "2021-04-28 21:38:51,560\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:38:55,452\tps   1807391 : Creating datums for KITTI-360 ...\n",
      "2021-04-28 21:38:55,452\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:39:00,203\tps   1807391 : ... seq 2013_05_28_drive_0004_sync has 99660 URIs spanning 1211 sec, creating 389 slices ...\n",
      "2021-04-28 21:39:00,714\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:39:23,976\tps   1807391 : ... partitioned datums into 0 RDDs.\n",
      "2021-04-28 21:39:23,977\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:39:23,977\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:39:28,251\tps   1807391 : Added DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0004_sync\n",
      "2021-04-28 21:39:28,256\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  -------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   26 (of 42)\n",
      "N chunks                 26\n",
      "Total time               14 minutes and 30.89 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         61.904762\n",
      "Est. Time To Completion  8 minutes and 55.93 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      33 seconds, 495 milliseconds, 650 microseconds and 181.4 nanoseconds\n",
      "p50                      18 seconds, 161 milliseconds, 446 microseconds and 928.98 nanoseconds\n",
      "p95                      1 minute, 25 seconds, 414 milliseconds, 883 microseconds and 971.21 nanoseconds\n",
      "p99                      2 minutes, 22 seconds, 73 milliseconds, 142 microseconds and 766.95 nanoseconds\n",
      "-----------------------  -------------------------------------------------------------------------------\n",
      "2021-04-28 21:39:28,302\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0002\n",
      "2021-04-28 21:39:29,119\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:39:33,299\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:39:33,302\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:39:37,299\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:39:37,300\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:39:37,300\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:39:41,433\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0002\n",
      "2021-04-28 21:39:41,438\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   27 (of 42)\n",
      "N chunks                 27\n",
      "Total time               14 minutes and 44.06 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         64.285714\n",
      "Est. Time To Completion  8 minutes and 11.15 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      32 seconds, 743 milliseconds, 113 microseconds and 685.54 nanoseconds\n",
      "p50                      15 seconds, 122 milliseconds, 98 microseconds and 922.73 nanoseconds\n",
      "p95                      1 minute, 24 seconds, 906 milliseconds, 359 microseconds and 291.08 nanoseconds\n",
      "p99                      2 minutes, 21 seconds, 351 milliseconds, 600 microseconds and 961.69 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:39:41,483\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=val&segment_id=scene-0276\n",
      "2021-04-28 21:39:42,253\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:39:46,276\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:39:46,279\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:39:50,503\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:39:50,503\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:39:50,504\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:39:54,498\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=val&segment_id=scene-0276\n",
      "2021-04-28 21:39:54,503\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  -------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   28 (of 42)\n",
      "N chunks                 28\n",
      "Total time               14 minutes and 57.12 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         66.666667\n",
      "Est. Time To Completion  7 minutes and 28.56 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      32 seconds, 40 milliseconds, 156 microseconds and 628.4 nanoseconds\n",
      "p50                      14 seconds, 270 milliseconds, 437 microseconds and 836.65 nanoseconds\n",
      "p95                      1 minute, 24 seconds, 397 milliseconds, 834 microseconds and 610.94 nanoseconds\n",
      "p99                      2 minutes, 20 seconds, 630 milliseconds, 59 microseconds and 156.42 nanoseconds\n",
      "-----------------------  -------------------------------------------------------------------------------\n",
      "2021-04-28 21:39:54,549\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=val&segment_id=scene-0273\n",
      "2021-04-28 21:39:55,327\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:39:59,214\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:39:59,217\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:40:03,073\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:40:03,074\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:40:03,074\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:40:07,083\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=val&segment_id=scene-0273\n",
      "2021-04-28 21:40:07,088\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   29 (of 42)\n",
      "N chunks                 29\n",
      "Total time               15 minutes and 9.7 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         69.047619\n",
      "Est. Time To Completion  6 minutes and 47.8 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      31 seconds, 369 milliseconds, 105 microseconds and 347.27 nanoseconds\n",
      "p50                      13 seconds, 418 milliseconds, 776 microseconds and 750.56 nanoseconds\n",
      "p95                      1 minute, 23 seconds, 889 milliseconds, 309 microseconds and 930.8 nanoseconds\n",
      "p99                      2 minutes, 19 seconds, 908 milliseconds, 517 microseconds and 351.15 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:40:07,133\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=val&segment_id=scene-0094\n",
      "2021-04-28 21:40:07,936\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:40:11,864\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:40:11,867\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:40:15,815\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:40:15,816\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:40:15,817\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:40:19,850\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=val&segment_id=scene-0094\n",
      "2021-04-28 21:40:19,855\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   30 (of 42)\n",
      "N chunks                 30\n",
      "Total time               15 minutes and 22.47 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         71.428571\n",
      "Est. Time To Completion  6 minutes and 8.99 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      30 seconds, 748 milliseconds, 883 microseconds and 716.27 nanoseconds\n",
      "p50                      13 seconds, 414 milliseconds, 107 microseconds and 84.27 nanoseconds\n",
      "p95                      1 minute, 23 seconds, 380 milliseconds, 785 microseconds and 250.66 nanoseconds\n",
      "p99                      2 minutes, 19 seconds, 186 milliseconds, 975 microseconds and 545.88 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:40:19,900\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0067\n",
      "2021-04-28 21:40:20,728\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:40:24,742\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:40:24,745\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:40:28,226\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:40:28,227\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:40:28,227\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:40:32,354\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0067\n",
      "2021-04-28 21:40:32,359\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   31 (of 42)\n",
      "N chunks                 31\n",
      "Total time               15 minutes and 34.97 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         73.809524\n",
      "Est. Time To Completion  5 minutes and 31.76 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      30 seconds, 160 milliseconds, 165 microseconds and 102.25 nanoseconds\n",
      "p50                      13 seconds, 409 milliseconds, 437 microseconds and 417.98 nanoseconds\n",
      "p95                      1 minute, 22 seconds, 872 milliseconds, 260 microseconds and 570.53 nanoseconds\n",
      "p99                      2 minutes, 18 seconds, 465 milliseconds, 433 microseconds and 740.62 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:40:32,404\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0397\n",
      "2021-04-28 21:40:33,197\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:40:37,203\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:40:37,211\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:40:41,026\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:40:41,027\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:40:41,027\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:40:45,084\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0397\n",
      "2021-04-28 21:40:45,089\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   32 (of 42)\n",
      "N chunks                 32\n",
      "Total time               15 minutes and 47.69 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         76.190476\n",
      "Est. Time To Completion  4 minutes and 56.15 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      29 seconds, 615 milliseconds, 337 microseconds and 878.47 nanoseconds\n",
      "p50                      13 seconds, 366 milliseconds, 847 microseconds and 753.52 nanoseconds\n",
      "p95                      1 minute, 22 seconds, 363 milliseconds, 735 microseconds and 890.39 nanoseconds\n",
      "p99                      2 minutes, 17 seconds, 743 milliseconds, 891 microseconds and 935.35 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:40:45,134\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0373\n",
      "2021-04-28 21:40:45,889\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:40:50,046\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:40:50,050\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:40:54,159\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:40:54,159\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:40:54,160\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:40:58,023\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0373\n",
      "2021-04-28 21:40:58,028\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  -------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   33 (of 42)\n",
      "N chunks                 33\n",
      "Total time               16 minutes and 0.63 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         78.571429\n",
      "Est. Time To Completion  4 minutes and 21.99 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      29 seconds, 109 milliseconds, 855 microseconds and 536.26 nanoseconds\n",
      "p50                      13 seconds, 324 milliseconds, 258 microseconds and 89.07 nanoseconds\n",
      "p95                      1 minute, 21 seconds, 855 milliseconds, 211 microseconds and 210.25 nanoseconds\n",
      "p99                      2 minutes, 17 seconds, 22 milliseconds, 350 microseconds and 130.08 nanoseconds\n",
      "-----------------------  -------------------------------------------------------------------------------\n",
      "2021-04-28 21:40:58,029\tps   1807391 : Building DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0000_sync\n",
      "2021-04-28 21:40:58,777\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:41:02,756\tps   1807391 : Creating datums for KITTI-360 ...\n",
      "2021-04-28 21:41:02,756\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:41:09,409\tps   1807391 : ... seq 2013_05_28_drive_0000_sync has 101669 URIs spanning 1204 sec, creating 397 slices ...\n",
      "2021-04-28 21:41:09,906\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:41:35,528\tps   1807391 : ... partitioned datums into 0 RDDs.\n",
      "2021-04-28 21:41:35,530\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:41:35,531\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:41:39,747\tps   1807391 : Added DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0000_sync\n",
      "2021-04-28 21:41:39,752\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   34 (of 42)\n",
      "N chunks                 34\n",
      "Total time               16 minutes and 42.34 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         80.952381\n",
      "Est. Time To Completion  3 minutes and 55.85 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      29 seconds, 480 milliseconds, 720 microseconds and 884.66 nanoseconds\n",
      "p50                      13 seconds, 366 milliseconds, 847 microseconds and 753.52 nanoseconds\n",
      "p95                      1 minute, 21 seconds, 346 milliseconds, 686 microseconds and 530.11 nanoseconds\n",
      "p99                      2 minutes, 16 seconds, 300 milliseconds, 808 microseconds and 324.81 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:41:39,752\tps   1807391 : Building DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0006_sync\n",
      "2021-04-28 21:41:40,563\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:41:44,618\tps   1807391 : Creating datums for KITTI-360 ...\n",
      "2021-04-28 21:41:44,619\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:41:50,030\tps   1807391 : ... seq 2013_05_28_drive_0006_sync has 85875 URIs spanning 1014 sec, creating 335 slices ...\n",
      "2021-04-28 21:41:50,489\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:42:10,376\tps   1807391 : ... partitioned datums into 0 RDDs.\n",
      "2021-04-28 21:42:10,376\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:42:10,377\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:42:14,789\tps   1807391 : Added DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0006_sync\n",
      "2021-04-28 21:42:14,794\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   35 (of 42)\n",
      "N chunks                 35\n",
      "Total time               17 minutes and 17.38 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         83.333333\n",
      "Est. Time To Completion  3 minutes and 27.48 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      29 seconds, 639 milliseconds, 469 microseconds and 930.1 nanoseconds\n",
      "p50                      13 seconds, 409 milliseconds, 437 microseconds and 417.98 nanoseconds\n",
      "p95                      1 minute, 20 seconds, 838 milliseconds, 161 microseconds and 849.98 nanoseconds\n",
      "p99                      2 minutes, 15 seconds, 579 milliseconds, 266 microseconds and 519.55 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:42:14,795\tps   1807391 : Building DF for psegs://dataset=kitti-360-fused&split=train&segment_id=2013_05_28_drive_0000_sync\n",
      "2021-04-28 21:42:15,601\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:42:19,646\tps   1807391 : Creating datums for KITTI-360 ...\n",
      "2021-04-28 21:42:19,647\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:42:27,116\tps   1807391 : ... seq 2013_05_28_drive_0000_sync has 122635 URIs spanning 1204 sec, creating 1916 slices ...\n",
      "2021-04-28 21:42:27,753\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:44:10,573\tps   1807391 : ... partitioned datums into 0 RDDs.\n",
      "2021-04-28 21:44:10,574\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:44:10,574\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:44:14,858\tps   1807391 : Added DF for psegs://dataset=kitti-360-fused&split=train&segment_id=2013_05_28_drive_0000_sync\n",
      "2021-04-28 21:44:14,863\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  ------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   36 (of 42)\n",
      "N chunks                 36\n",
      "Total time               19 minutes and 17.45 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         85.714286\n",
      "Est. Time To Completion  3 minutes and 12.91 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      32 seconds, 151 milliseconds, 266 microseconds and 819.9 nanoseconds\n",
      "p50                      13 seconds, 414 milliseconds, 107 microseconds and 84.27 nanoseconds\n",
      "p95                      1 minute, 35 seconds, 984 milliseconds, 170 microseconds and 19.63 nanoseconds\n",
      "p99                      2 minutes, 26 seconds, 95 milliseconds, 52 microseconds and 421.09 nanoseconds\n",
      "-----------------------  ------------------------------------------------------------------------------\n",
      "2021-04-28 21:44:14,864\tps   1807391 : Building DF for psegs://dataset=kitti-360-fused&split=train&segment_id=2013_05_28_drive_0004_sync\n",
      "2021-04-28 21:44:15,716\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:44:19,953\tps   1807391 : Creating datums for KITTI-360 ...\n",
      "2021-04-28 21:44:19,953\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:44:25,535\tps   1807391 : ... seq 2013_05_28_drive_0004_sync has 115934 URIs spanning 1211 sec, creating 1811 slices ...\n",
      "2021-04-28 21:44:26,152\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:46:06,936\tps   1807391 : ... partitioned datums into 0 RDDs.\n",
      "2021-04-28 21:46:06,937\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:46:06,937\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:46:11,326\tps   1807391 : Added DF for psegs://dataset=kitti-360-fused&split=train&segment_id=2013_05_28_drive_0004_sync\n",
      "2021-04-28 21:46:11,331\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   37 (of 42)\n",
      "N chunks                 37\n",
      "Total time               21 minutes and 13.91 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         88.095238\n",
      "Est. Time To Completion  2 minutes and 52.15 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      34 seconds, 429 milliseconds, 972 microseconds and 725.95 nanoseconds\n",
      "p50                      13 seconds, 418 milliseconds, 776 microseconds and 750.56 nanoseconds\n",
      "p95                      1 minute, 57 seconds, 183 milliseconds, 539 microseconds and 867.4 nanoseconds\n",
      "p99                      2 minutes, 25 seconds, 694 milliseconds, 577 microseconds and 121.73 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:46:11,376\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0669\n",
      "2021-04-28 21:46:12,190\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:46:16,273\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:46:16,276\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:46:20,619\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:46:20,619\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:46:20,620\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:46:24,766\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0669\n",
      "2021-04-28 21:46:24,771\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   38 (of 42)\n",
      "N chunks                 38\n",
      "Total time               21 minutes and 27.34 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         90.476190\n",
      "Est. Time To Completion  2 minutes and 15.51 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      33 seconds, 877 milliseconds, 464 microseconds and 181.5 nanoseconds\n",
      "p50                      13 seconds, 426 milliseconds, 712 microseconds and 393.76 nanoseconds\n",
      "p95                      1 minute, 57 seconds, 3 milliseconds, 501 microseconds and 236.44 nanoseconds\n",
      "p99                      2 minutes, 25 seconds, 294 milliseconds, 101 microseconds and 822.38 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:46:24,815\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0255\n",
      "2021-04-28 21:46:25,642\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:46:29,719\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:46:29,723\tps   1807391 : ... checking existing URIs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 21:46:33,465\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:46:33,466\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:46:33,466\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:46:37,587\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0255\n",
      "2021-04-28 21:46:37,592\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   39 (of 42)\n",
      "N chunks                 39\n",
      "Total time               21 minutes and 40.16 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         92.857143\n",
      "Est. Time To Completion  1 minute and 40.01 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      33 seconds, 337 milliseconds, 440 microseconds and 87.25 nanoseconds\n",
      "p50                      13 seconds, 418 milliseconds, 776 microseconds and 750.56 nanoseconds\n",
      "p95                      1 minute, 56 seconds, 823 milliseconds, 462 microseconds and 605.48 nanoseconds\n",
      "p99                      2 minutes, 24 seconds, 893 milliseconds, 626 microseconds and 523.02 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:46:37,593\tps   1807391 : Building DF for psegs://dataset=kitti-360-fused&split=train&segment_id=2013_05_28_drive_0002_sync\n",
      "2021-04-28 21:46:38,364\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:46:42,411\tps   1807391 : Creating datums for KITTI-360 ...\n",
      "2021-04-28 21:46:42,411\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:46:50,075\tps   1807391 : ... seq 2013_05_28_drive_0002_sync has 174992 URIs spanning 2013 sec, creating 2734 slices ...\n",
      "2021-04-28 21:46:50,957\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:49:22,243\tps   1807391 : ... partitioned datums into 0 RDDs.\n",
      "2021-04-28 21:49:22,243\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:49:22,244\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:49:26,515\tps   1807391 : Added DF for psegs://dataset=kitti-360-fused&split=train&segment_id=2013_05_28_drive_0002_sync\n",
      "2021-04-28 21:49:26,520\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   40 (of 42)\n",
      "N chunks                 40\n",
      "Total time               24 minutes and 29.08 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         95.238095\n",
      "Est. Time To Completion  1 minute and 13.45 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      36 seconds, 727 milliseconds, 68 microseconds and 823.58 nanoseconds\n",
      "p50                      13 seconds, 426 milliseconds, 712 microseconds and 393.76 nanoseconds\n",
      "p95                      2 minutes, 2 seconds, 66 milliseconds, 534 microseconds and 459.59 nanoseconds\n",
      "p99                      2 minutes, 45 seconds, 486 milliseconds, 337 microseconds and 900.16 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:49:26,566\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0501\n",
      "2021-04-28 21:49:27,407\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:49:31,647\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:49:31,651\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:49:41,056\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:49:41,056\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:49:41,057\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:49:45,352\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_track&segment_id=scene-0501\n",
      "2021-04-28 21:49:45,357\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   41 (of 42)\n",
      "N chunks                 41\n",
      "Total time               24 minutes and 47.92 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         97.619048\n",
      "Est. Time To Completion  36.29 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      36 seconds, 290 milliseconds, 615 microseconds and 302.76 nanoseconds\n",
      "p50                      13 seconds, 434 milliseconds, 648 microseconds and 36.96 nanoseconds\n",
      "p95                      2 minutes, 64 milliseconds, 157 microseconds and 962.8 nanoseconds\n",
      "p99                      2 minutes, 45 seconds, 398 milliseconds, 228 microseconds and 883.74 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:49:45,402\tps   1807391 : Building DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0767\n",
      "2021-04-28 21:49:46,222\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:49:50,401\tps   1807391 : Filtering to only 1 segments\n",
      "2021-04-28 21:49:50,405\tps   1807391 : ... checking existing URIs ...\n",
      "2021-04-28 21:49:54,402\tps   1807391 : ... all datums already exist, skipping this chunk ...\n",
      "2021-04-28 21:49:54,402\tps   1807391 : Going to write in 0 chunks ...\n",
      "2021-04-28 21:49:54,402\tps   1807391 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-28 21:49:58,594\tps   1807391 : Added DF for psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0767\n",
      "2021-04-28 21:49:58,599\toarph 1807391 : Progress for \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   42 (of 42)\n",
      "N chunks                 42\n",
      "Total time               25 minutes and 1.15 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         100.000000\n",
      "Est. Time To Completion  0 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      35 seconds, 741 milliseconds, 724 microseconds and 752.24 nanoseconds\n",
      "p50                      13 seconds, 426 milliseconds, 712 microseconds and 393.76 nanoseconds\n",
      "p95                      1 minute, 59 seconds, 884 milliseconds, 119 microseconds and 331.84 nanoseconds\n",
      "p99                      2 minutes, 45 seconds, 310 milliseconds, 119 microseconds and 867.32 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "2021-04-28 21:49:58,599\tps   1807391 : ... done building union DF for 42 segments\n",
      "2021-04-28 21:50:04,255\toarph 1807391 : \n",
      "get_or_build_datum_dfs [Pid:1807391 Id:140553816379056]\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   42 (of 42)\n",
      "N chunks                 42\n",
      "Total time               25 minutes and 1.15 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         100.000000\n",
      "Est. Time To Completion  0 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      35 seconds, 741 milliseconds, 724 microseconds and 752.24 nanoseconds\n",
      "p50                      13 seconds, 426 milliseconds, 712 microseconds and 393.76 nanoseconds\n",
      "p95                      1 minute, 59 seconds, 884 milliseconds, 119 microseconds and 331.84 nanoseconds\n",
      "p99                      2 minutes, 45 seconds, 310 milliseconds, 119 microseconds and 867.32 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def fp_uri_to_fname(fp_uri):\n",
    "    fp_uri = str(fp_uri)\n",
    "    import urllib.parse\n",
    "    fname = urllib.parse.quote(fp_uri)\n",
    "    from slugify import slugify\n",
    "    fname = slugify(fname)\n",
    "    \n",
    "    # Ubuntu seems to limit at 255 or so ...\n",
    "    if len(fname) > 150:\n",
    "        from oarphpy.util import stable_hash\n",
    "        fname_hash = stable_hash(fname)\n",
    "        fname = fname[:150] + str(fname_hash)\n",
    "    \n",
    "    return fname\n",
    "\n",
    "def extract_fp_uris_from_html(html):\n",
    "    import re\n",
    "    matches = list(set(re.findall(r'alt=\\\\\"(.*?)\\\\\"', html)))\n",
    "    import html\n",
    "    return set(html.unescape(s) for s in matches)\n",
    "\n",
    "FLOW_EVAL_REPORT_BASEDIR = '/opt/psegs/flow_eval_hists_222/'\n",
    "from oarphpy import util as oputil\n",
    "oputil.mkdir(FLOW_EVAL_REPORT_BASEDIR)\n",
    "\n",
    "from oarphpy import plotting as pl\n",
    "class Plotter(pl.HistogramWithExamplesPlotter):\n",
    "    NUM_BINS = 50\n",
    "    ROWS_TO_DISPLAY_PER_BUCKET = 5\n",
    "    SUB_PIVOT_COL = 'fp_dataset'\n",
    "\n",
    "    def display_bucket(self, sub_pivot, bucket_id, irows):\n",
    "        from oarphpy.spark import RowAdapter\n",
    "        from psegs import datum\n",
    "        \n",
    "        # Sample from irows using reservior sampling\n",
    "        import random\n",
    "        rows = []\n",
    "        for i, row in enumerate(irows):\n",
    "            r = random.randint(0, i)\n",
    "            if r < self.ROWS_TO_DISPLAY_PER_BUCKET:\n",
    "                if i < self.ROWS_TO_DISPLAY_PER_BUCKET:\n",
    "                    rows.insert(r, row)\n",
    "                else:\n",
    "                    rows[r] = row\n",
    "        \n",
    "        # Now render each row to HTML\n",
    "        row_htmls = []\n",
    "        for row in rows:\n",
    "            rowdata = RowAdapter.from_row(row)\n",
    "            \n",
    "            fp_datset = rowdata['fp_dataset']\n",
    "            fp_uri_str = rowdata['fp_uri']\n",
    "            fp_uri = datum.URI.from_str(fp_uri_str)\n",
    "            fp_page_uri = fp_uri_to_fname(fp_uri_str) + '.html'\n",
    "            id1 = rowdata['fp_id1']\n",
    "            id2 = rowdata['fp_id2']\n",
    "            \n",
    "            row_html = f\"\"\"\n",
    "                <a href=\"{fp_page_uri}\" alt=\"{fp_uri_str}\">\n",
    "                    {fp_datset} {fp_uri.split} {fp_uri.segment_id} {id1} -> {id2}\n",
    "                </a><br />\"\"\"\n",
    "            row_htmls.append(row_html)\n",
    "        \n",
    "        HTML = \"\"\"\n",
    "        <b>Pivot: {spv} Bucket: {bucket_id} </b> <br/>\n",
    "        \n",
    "        {row_bodies}\n",
    "        \"\"\".format(\n",
    "              spv=sub_pivot,\n",
    "              bucket_id=bucket_id,\n",
    "              row_bodies=\"<br/><br/><br/>\".join(row_htmls))\n",
    "        \n",
    "        return bucket_id, HTML\n",
    "\n",
    "plotter = Plotter()\n",
    "\n",
    "chosen_fp_uris = set()\n",
    "histogram_htmls = []\n",
    "\n",
    "SKIP_COLS = (\n",
    "    'fp_uri',\n",
    "    'fp_dataset',\n",
    "    'fp_id1',\n",
    "    'fp_id2',\n",
    "    'has_scene_flow',\n",
    ")\n",
    "\n",
    "cols = [col for col in results_df.columns if col not in SKIP_COLS]\n",
    "print(\"Rendering %s histograms\" % len(cols))\n",
    "for col in cols:\n",
    "    print(\"Working on %s\" % col)\n",
    "    cur_df = results_df\n",
    "#     if col == 'diff_time_sec':\n",
    "#         # fixme hacks ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         cur_df = cur_df.filter(cur_df.diff_time_sec.isNotNull())\n",
    "    if 'SceneFlow' in col:\n",
    "        cur_df = cur_df.filter(cur_df.has_scene_flow == True)\n",
    "#     fig = plotter.run(cur_df, col)\n",
    "    dest = os.path.join(FLOW_EVAL_REPORT_BASEDIR, '%s.html' % col)\n",
    "#     pl.save_bokeh_fig(fig, dest)\n",
    "    \n",
    "    with open(dest, 'r') as f:\n",
    "        cur_chosen_fp_uris = extract_fp_uris_from_html(f.read())\n",
    "#     cur_chosen_fp_uris = set(u for u in cur_chosen_fp_uris if ('deep_deform' not in u and 'kitti_sf15' not in u))\n",
    "    \n",
    "    chosen_fp_uris |= cur_chosen_fp_uris\n",
    "    print('total chosen_fp_uris, added', len(chosen_fp_uris), len(cur_chosen_fp_uris))\n",
    "\n",
    "print(\"Rendering %s histogram bucket pages\" % len(chosen_fp_uris))\n",
    "class UnionFactory(FlowPairUnionFactory):\n",
    "    FACTORIES = ALL_FP_FACTORY_CLSS\n",
    "\n",
    "# analysis_uris_full = UnionFactory.list_fp_uris(spark)\n",
    "\n",
    "fp_rdd = UnionFactory.get_fp_rdd_for_uris(spark, chosen_fp_uris)\n",
    "def render_and_save(fp):\n",
    "    from threadpoolctl import threadpool_limits\n",
    "    with threadpool_limits(limits=1, user_api='blas'):\n",
    "        import os\n",
    "        recon = FlowReconstructedImagePair.create_from(fp)\n",
    "        fstats = OFlowStats.create_from(fp)\n",
    "        errors = OFlowReconErrors(recon)\n",
    "        \n",
    "        if fp.has_scene_flow():\n",
    "            with threadpool_limits(limits=1, user_api='openmp'):\n",
    "                sflow = SFlowStats.create_from(fp)\n",
    "            sflow_html = sflow.to_html()\n",
    "        else:\n",
    "            sflow_html = '(no SceneFlow data)'\n",
    "        \n",
    "        page_html = \"<br/>\".join(\n",
    "            (PLOTLY_INIT_HTML, fp.to_html(), recon.to_html(), fstats.to_html(), errors.to_html(), sflow_html))\n",
    "\n",
    "        dest = os.path.join(FLOW_EVAL_REPORT_BASEDIR, fp_uri_to_fname(fp.uri) + '.html')\n",
    "        with open(dest, 'w') as f:\n",
    "            f.write(page_html)\n",
    "\n",
    "fp_rdd.foreach(render_and_save)\n",
    "    \n",
    "    \n",
    "# from bokeh.io import output_notebook\n",
    "# output_notebook()\n",
    "# from bokeh.plotting import show\n",
    "# show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from oarphpy import util as oputil\n",
    "# paths = oputil.all_files_recursive('/opt/psegs/flow_eval_hists_222/')\n",
    "# for path in paths:\n",
    "#     if 'psegs-3a' in path:\n",
    "#         with open(path, 'r') as f:\n",
    "#             data = f.read()\n",
    "#         data = \"\"\"<script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\"\"\" + data\n",
    "#         with open(path, 'w') as f:\n",
    "#             f.write(data)\n",
    "#         print(path)\n",
    "        \n",
    "\n",
    "# PLOTLY_INIT_HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
