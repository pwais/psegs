{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheap Optical Flow: Is it Good? Does it Boost?\n",
    "\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "## Credits\n",
    "\n",
    "Some portions of this notebook adapted from:\n",
    " * [Middlebury Flow code by Johannes Oswald](https://github.com/Johswald/flow-code-python/blob/master/readFlowFile.py)\n",
    " * [DeepDeform Demo Code](https://github.com/AljazBozic/DeepDeform)\n",
    " * [OpticalFlowToolkit by RUOTENG LI](https://github.com/liruoteng/OpticalFlowToolkit)\n",
    " * [OpenCV Samples](https://github.com/opencv/opencv/blob/master/samples/python/opt_flow.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "SHOW_DEMO_OUTPUT = False\n",
    "DEMO_FPS = []\n",
    "\n",
    "RUN_FULL_ANALYSIS = False\n",
    "ALL_FP_FACTORY_CLSS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypng in /usr/local/lib/python3.8/dist-packages (0.0.20)\n",
      "Requirement already satisfied: scikit-image in /usr/lib/python3/dist-packages (0.16.2)\n",
      "fixme installs\n",
      "\n",
      "\n",
      "Putting analysis lib in /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis\n",
      "running clean\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing psegs.egg-info/PKG-INFO\n",
      "writing dependency_links to psegs.egg-info/dependency_links.txt\n",
      "writing top-level names to psegs.egg-info/top_level.txt\n",
      "reading manifest file 'psegs.egg-info/SOURCES.txt'\n",
      "writing manifest file 'psegs.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "copying psegs/exp/fused_lidar_flow.py -> build/lib/psegs/exp\n",
      "warning: build_py: byte-compiling is disabled, skipping.\n",
      "\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/psegs\n",
      "copying build/lib/psegs/dummyrun.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "creating build/bdist.linux-x86_64/egg/psegs/datasets\n",
      "copying build/lib/psegs/datasets/kitti.py -> build/bdist.linux-x86_64/egg/psegs/datasets\n",
      "copying build/lib/psegs/datasets/idsutil.py -> build/bdist.linux-x86_64/egg/psegs/datasets\n",
      "copying build/lib/psegs/datasets/kitti_360.py -> build/bdist.linux-x86_64/egg/psegs/datasets\n",
      "copying build/lib/psegs/datasets/__init__.py -> build/bdist.linux-x86_64/egg/psegs/datasets\n",
      "copying build/lib/psegs/datasets/nuscenes.py -> build/bdist.linux-x86_64/egg/psegs/datasets\n",
      "creating build/bdist.linux-x86_64/egg/psegs/util\n",
      "copying build/lib/psegs/util/__init__.py -> build/bdist.linux-x86_64/egg/psegs/util\n",
      "copying build/lib/psegs/util/misc.py -> build/bdist.linux-x86_64/egg/psegs/util\n",
      "copying build/lib/psegs/util/plotting.py -> build/bdist.linux-x86_64/egg/psegs/util\n",
      "creating build/bdist.linux-x86_64/egg/psegs/table\n",
      "copying build/lib/psegs/table/sd_db.py -> build/bdist.linux-x86_64/egg/psegs/table\n",
      "copying build/lib/psegs/table/sd_table.py -> build/bdist.linux-x86_64/egg/psegs/table\n",
      "copying build/lib/psegs/table/__init__.py -> build/bdist.linux-x86_64/egg/psegs/table\n",
      "copying build/lib/psegs/browser.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "creating build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/uri.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/bbox2d.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/datumutils.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/transform.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/stamped_datum.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/cuboid.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/frame.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/point_cloud.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/camera_image.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/datum/__init__.py -> build/bdist.linux-x86_64/egg/psegs/datum\n",
      "copying build/lib/psegs/spark.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "copying build/lib/psegs/conf.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "copying build/lib/psegs/dsutil.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "copying build/lib/psegs/__init__.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "creating build/bdist.linux-x86_64/egg/psegs/exp\n",
      "copying build/lib/psegs/exp/semantic_kitti.py -> build/bdist.linux-x86_64/egg/psegs/exp\n",
      "copying build/lib/psegs/exp/fused_lidar_flow.py -> build/bdist.linux-x86_64/egg/psegs/exp\n",
      "copying build/lib/psegs/exp/__init__.py -> build/bdist.linux-x86_64/egg/psegs/exp\n",
      "copying build/lib/psegs/ros.py -> build/bdist.linux-x86_64/egg/psegs\n",
      "warning: install_lib: byte-compiling is disabled, skipping.\n",
      "\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying psegs.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying psegs.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying psegs.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying psegs.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/psegs-0.0.1-py3.8.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 02:01:27,469\toarph 3501100 : Using source root /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis \n",
      "2021-04-15 02:01:27,470\toarph 3501100 : Using source root /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis \n",
      "2021-04-15 02:01:27,514\toarph 3501100 : Generating egg to /tmp/tmp7ftnd9ii_oarphpy_eggbuild ...\n",
      "2021-04-15 02:01:27,525\toarph 3501100 : ... done.  Egg at /tmp/tmp7ftnd9ii_oarphpy_eggbuild/cheap_optical_flow_eval_analysis-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "## Setup\n",
    "\n",
    "!pip3 install pypng scikit-image\n",
    "print('fixme installs')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import copy\n",
    "import imageio\n",
    "import IPython.display\n",
    "import math\n",
    "import os\n",
    "import PIL.Image\n",
    "import six\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "\n",
    "## General Notebook Utilities\n",
    "    \n",
    "def imshow(x):\n",
    "    IPython.display.display(PIL.Image.fromarray(x))\n",
    "\n",
    "def show_html(x):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(x))\n",
    "\n",
    "\n",
    "## Create a random temporary directory for analysis library (for Spark-enabled full analysis mode)\n",
    "old_cwd = os.getcwd()\n",
    "tempdir = tempfile.TemporaryDirectory(suffix='_cheap_optical_flow_eval_analysis')\n",
    "ALIB_SRC_DIR = tempdir.name\n",
    "print(\"Putting analysis lib in %s\" % ALIB_SRC_DIR)\n",
    "os.chdir(ALIB_SRC_DIR)\n",
    "!mkdir -p cheap_optical_flow_eval_analysis\n",
    "!touch cheap_optical_flow_eval_analysis/__init__.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append(ALIB_SRC_DIR)\n",
    "\n",
    "\n",
    "## Prepare a build of local psegs for inclusion\n",
    "!cd /opt/psegs && python3 setup.py clean bdist_egg\n",
    "PSEGS_EGG_PATH = '/opt/psegs/dist/psegs-0.0.1-py3.8.egg'\n",
    "assert os.path.exists(PSEGS_EGG_PATH), \"Build failed?\"\n",
    "sys.path.append('/opt/psegs')\n",
    "import psegs\n",
    "\n",
    "\n",
    "## Prepare Spark session with local PSegs and local Analysis Lib\n",
    "from psegs.spark import NBSpark\n",
    "NBSpark.SRC_ROOT = os.path.join(ALIB_SRC_DIR, 'cheap_optical_flow_eval_analysis')\n",
    "NBSpark.CONF_KV.update({\n",
    "    'spark.driver.maxResultSize': '2g',\n",
    "    'spark.driver.memory': '16g',\n",
    "    'spark.submit.pyFiles': PSEGS_EGG_PATH,\n",
    "  })\n",
    "spark = NBSpark.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cheap_optical_flow_eval_analysis/ofp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cheap_optical_flow_eval_analysis/ofp.py\n",
    "\n",
    "## Data Model & Utility Code\n",
    "\n",
    "import attr\n",
    "import cv2\n",
    "import imageio\n",
    "import math\n",
    "import os\n",
    "import PIL.Image\n",
    "import six\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from psegs import datum\n",
    "\n",
    "from oarphpy import plotting as op_plt\n",
    "from oarphpy.spark import CloudpickeledCallable\n",
    "img_to_data_uri = lambda x: op_plt.img_to_data_uri(x, format='png')\n",
    "\n",
    "@attr.s(slots=True, eq=False, weakref_slot=False)\n",
    "class OpticalFlowPair(object):\n",
    "    \"\"\"A flyweight for a pair of images with an optical flow field.\n",
    "    Supports lazy-loading of large data attributes.\"\"\"\n",
    "    \n",
    "    ## Core Attributes (Required for All Datasets)\n",
    "    \n",
    "    dataset = attr.ib(type=str, default='')\n",
    "    \"\"\"(Display name) To which dataset does this pair belong?\"\"\"\n",
    "    \n",
    "    id1 = attr.ib(type=str, default='')\n",
    "    \"\"\"Identifier or URI for the first image\"\"\"\n",
    "    \n",
    "    id2 = attr.ib(type=str, default='')\n",
    "    \"\"\"Identifier or URI for the second image\"\"\"\n",
    "    \n",
    "    img1 = attr.ib(default=None)\n",
    "    \"\"\"URI or numpy array or CloudPickleCallable for the first image (source image)\"\"\"\n",
    "\n",
    "    img2 = attr.ib(default=None)\n",
    "    \"\"\"URI or numpy array or CloudpickeledCallable for the second image (target image)\"\"\"\n",
    "    \n",
    "    flow = attr.ib(default=None)\n",
    "    \"\"\"A numpy array or callable or CloudpickeledCallable representing optical flow from img1 -> img2\"\"\"\n",
    "    \n",
    "    uri = attr.ib(type=datum.URI, default=None, converter=datum.URI.from_str)\n",
    "    \"\"\"A URI addressing this pair; to make dynamic construction of the pair easier\"\"\"\n",
    "    \n",
    "    \n",
    "    ## Optional Attributes (For Select Datasets)\n",
    "    \n",
    "    diff_time_sec = attr.ib(type=float, default=0.0)\n",
    "    \"\"\"Difference in time (in seconds) between the views / poses depicted in `img1` and `img2`.\"\"\"\n",
    "    \n",
    "    translation_meters = attr.ib(type=float, default=0.0)\n",
    "    \"\"\"Difference in ego translation (in meters) between the views / poses depicted in `img1` and `img2`.\"\"\"\n",
    "    \n",
    "    uvdviz_im1 = attr.ib(default=None)\n",
    "    \"\"\"An nx4 numpy array representing UVD-visible points for `img1`\"\"\"\n",
    "    \n",
    "    uvdviz_im2 = attr.ib(default=None)\n",
    "    \"\"\"An nx4 numpy array representing UVD-visible points for `img2`\"\"\"\n",
    "    \n",
    "    K = attr.ib(default=None)\n",
    "    \"\"\"A 3x3 numpy array representing the camera matrix K for both views\"\"\"\n",
    "    \n",
    "    # to add:\n",
    "    # semantic image for frame 1, frame 2 [could be painted by cuboids]\n",
    "    # instance images for frame 1, frame 2 [could be painted by cuboids]\n",
    "    #   -- for colored images, at first just pivot all oflow metrics by colors\n",
    "    # get uvdviz1 uvdviz2 (scene flow)\n",
    "    #   * for deepeform, their load_flow will work\n",
    "    #   * for kitti, we have to read their disparity images\n",
    "    # get uvd1 uvd2 (lidar for nearest neighbor stuff)\n",
    "    # depth image for frame 1, frame 2 [could be interpolated by cuboids]\n",
    "    #   -- at first bucket the depth coarsely and pivot al oflow by colors\n",
    "    \n",
    "    def get_img1(self):\n",
    "        if isinstance(self.img1, CloudpickeledCallable):\n",
    "            self.img1 = self.img1()\n",
    "        if isinstance(self.img1, six.string_types):\n",
    "            self.img1 = imageio.imread(self.img1)\n",
    "        return self.img1\n",
    "    \n",
    "    def get_img2(self):\n",
    "        if isinstance(self.img2, CloudpickeledCallable):\n",
    "            self.img2 = self.img2()\n",
    "        if isinstance(self.img2, six.string_types):\n",
    "            self.img2 = imageio.imread(self.img2)\n",
    "        return self.img2\n",
    "    \n",
    "    def get_flow(self):\n",
    "        if not isinstance(self.flow, (np.ndarray, np.generic)):\n",
    "            self.flow = self.flow()\n",
    "        return self.flow\n",
    "    \n",
    "    def has_scene_flow(self):\n",
    "        return (self.uvdviz_im1 is not None and uvdviz_im2 is not None and self.K is not None)\n",
    "    \n",
    "    def get_sf_viz_html(self):\n",
    "        uvd1 = self.uvdviz_im1[self.uvdviz_im1[:, -1] == 1, :2]\n",
    "        uvd2 = self.uvdviz_im2[self.uvdviz_im2[:, -1] == 1, :2]\n",
    "        xyzrgb1 = uvd_to_xyzrgb(uvd1, self.K, imgs=[self.get_img1()])\n",
    "        xyzrgb2 = uvd_to_xyzrgb(uvd2, self.K, imgs=[self.get_img2()])\n",
    "        html1 = create_xyzrgb_3d_plot_html(xyzrgb1)\n",
    "        html2 = create_xyzrgb_3d_plot_html(xyzrgb2)\n",
    "        \n",
    "        html = \"View 1:<br />%s<br /><br />View 2:<br />%s\" % (html1, html2)\n",
    "        return html\n",
    "    \n",
    "    def to_html(self):\n",
    "        im1 = self.get_img1()\n",
    "        im2 = self.get_img2()\n",
    "        flow = self.get_flow()\n",
    "        fviz = draw_flow(im1, flow)\n",
    "        \n",
    "        sf_html = ''\n",
    "        if self.has_scene_flow():\n",
    "            sf_html = \"\"\"\n",
    "                <tr><td style=\"text-align:left\"><b>Scene Flow</b></td></tr>\n",
    "                <tr><td>{viz_html></td></tr>\n",
    "            \"\"\"format(viz_html=self.get_sf_viz_html())\n",
    "        \n",
    "        html = \"\"\"\n",
    "            <table>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Dataset:</b> {dataset}</td></tr>\n",
    "            <tr><td style=\"text-align:left\"><b>URI:</b> {uri}</td></tr>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Source Image:</b> {id1}</td></tr>\n",
    "            <tr><td><img src=\"{im1}\" /></td></tr>\n",
    "\n",
    "            <tr><td style=\"text-align:left\"><b>Target Image:</b> {id2}</td></tr>\n",
    "            <tr><td><img src=\"{im2}\" /></td></tr>\n",
    "\n",
    "            <tr><td style=\"text-align:left\"><b>Flow</b></td></tr>\n",
    "            <tr><td><img src=\"{fviz}\" /></td></tr>\n",
    "            \n",
    "            {sf_html}\n",
    "            </table>\n",
    "        \"\"\".format(\n",
    "                dataset=self.dataset,\n",
    "                uri=str(self.uri),\n",
    "                id1=self.id1, id2=self.id2,\n",
    "                im1=img_to_data_uri(im1), im2=img_to_data_uri(im2),\n",
    "                fviz=img_to_data_uri(fviz),\n",
    "                sf_html=sf_html)\n",
    "        return html\n",
    "\n",
    "def draw_flow(img, flow, step=8):\n",
    "    \"\"\"Based upon OpenCV sample: https://github.com/opencv/opencv/blob/master/samples/python/opt_flow.py\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n",
    "    fx, fy = flow[y,x].T\n",
    "    lines = np.vstack([x, y, x+fx, y+fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "    vis = img.copy()\n",
    "    cv2.polylines(vis, lines, 0, (0, 255, 0))\n",
    "    for (x1, y1), (_x2, _y2) in lines:\n",
    "        cv2.circle(vis, (x1, y1), 1, (0, 255, 0), -1)\n",
    "    return vis\n",
    "\n",
    "def uvd_to_xyzrgb(uvd, K, imgs=None):\n",
    "    import numpy as np\n",
    "    from psegs import datum\n",
    "\n",
    "    fx = K[0, 0]\n",
    "    cx = K[0, 2]\n",
    "    fy = K[1, 1]\n",
    "    cy = K[1, 2]\n",
    "    \n",
    "    xyz = np.zeros((uvd.shape[0], 3))\n",
    "    xyz[:, 0] = (uvd[:, 0] - cx) / fx\n",
    "    xyz[:, 1] = (uvd[:, 1] - cy) / fy\n",
    "    xyz[:, 1] = 1.\n",
    "    xyz = uvd[:, 2] * xyz / np.linalg.norm(xyz, axis=-1)[:, np.newaxis]\n",
    "    \n",
    "    from psegs import datum\n",
    "    pc = (cloud=xyz)\n",
    "    cis = [datum.CameraImage() for img in (imgs or [])]\n",
    "    xyzrgb = datum.PointCloud.paint_ego_cloud(xyz, camera_images=cis)\n",
    "    return xyzrgb\n",
    "\n",
    "def create_xyzrgb_3d_plot_html(xyzrgb, max_points=100000):\n",
    "    import plotly\n",
    "    import plotly.graph_objects as go\n",
    "    import pandas as pd\n",
    "\n",
    "    pcloud_df = pd.DataFrame(xyzrgb, columns=['x', 'y', 'z', 'r', 'g', 'b'])\n",
    "    pcloud_df = pcloud_df.sample(n=min(xyzrgb.shape[0], max_points))\n",
    "    scatter = go.Scatter3d(\n",
    "                x=pcloud_df['x'], y=pcloud_df['y'], z=pcloud_df['z'],\n",
    "                mode='markers',\n",
    "                marker=dict(size=2, color=pcloud_df[['r', 'g', 'b']], opacity=0.9))\n",
    "    fig = go.Figure(data=[scatter])\n",
    "    fig.update_layout(\n",
    "          width=1000, height=700,\n",
    "          scene_aspectmode='data')\n",
    "    html = plotly.offline.plot(fig, output_type='div')\n",
    "    return html\n",
    "\n",
    "class FlowPairFactoryBase(object):\n",
    "    DATASET = ''\n",
    "\n",
    "    @classmethod\n",
    "    def list_fp_uris(cls, spark):\n",
    "        return []\n",
    "    \n",
    "    @classmethod\n",
    "    def get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        uris = [datum.URI.from_str(u) for u in uris]\n",
    "        uris = [u for u in uris if u.dataset == cls.DATASET]\n",
    "        if not uris:\n",
    "            return None\n",
    "        return cls._get_fp_rdd_for_uris(spark, uris)\n",
    "\n",
    "    @classmethod\n",
    "    def _get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        return None\n",
    "\n",
    "class FlowPairUnionFactory(FlowPairFactoryBase):\n",
    "    FACTORIES = []\n",
    "    \n",
    "    @classmethod\n",
    "    def list_fp_uris(cls, spark):\n",
    "        import itertools\n",
    "        return list(itertools.chain.from_iterable(F.list_fp_uris(spark) for F in cls.FACTORIES))\n",
    "    \n",
    "    @classmethod\n",
    "    def get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        rdds = []\n",
    "        for F in cls.FACTORIES:\n",
    "            rdd = F.get_fp_rdd_for_uris(spark, uris)\n",
    "            if rdd is not None:\n",
    "                rdds.append(rdd)\n",
    "        assert rdds, \"No RDDs for %s\" % uris\n",
    "        return spark.sparkContext.union(rdds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 02:01:31,714\toarph 3501100 : Source has changed! Rebuilding Egg ...\n",
      "2021-04-15 02:01:31,714\toarph 3501100 : Using source root /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis \n",
      "2021-04-15 02:01:31,715\toarph 3501100 : Using source root /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis \n",
      "2021-04-15 02:01:31,717\toarph 3501100 : Generating egg to /tmp/tmpcw0qhfcy_oarphpy_eggbuild ...\n",
      "2021-04-15 02:01:31,724\toarph 3501100 : ... done.  Egg at /tmp/tmpcw0qhfcy_oarphpy_eggbuild/cheap_optical_flow_eval_analysis-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from cheap_optical_flow_eval_analysis.ofp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Middlebury Optical Flow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO talk configs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cheap_optical_flow_eval_analysis/midd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cheap_optical_flow_eval_analysis/midd.py\n",
    "\n",
    "from psegs import datum\n",
    "\n",
    "from cheap_optical_flow_eval_analysis.ofp import *\n",
    "\n",
    "# Please unzip `other-color-allframes.zip` and `other-gt-flow.zip` to a directory and provide the target below:\n",
    "MIDD_DATA_ROOT = '/opt/psegs/ext_data/middlebury-flow/'\n",
    "\n",
    "# For the Middlebury Flow dataset, we only consider the real scenes\n",
    "MIDD_SCENES = [\n",
    "    {\n",
    "        'input': 'other-data/Dimetrodon/frame10.png',\n",
    "        'expected_out': 'other-data/Dimetrodon/frame11.png',\n",
    "        'flow_gt': 'other-gt-flow/Dimetrodon/flow10.flo',\n",
    "    },\n",
    "        {\n",
    "        'input': 'other-data/Hydrangea/frame10.png',\n",
    "        'expected_out': 'other-data/Hydrangea/frame11.png',\n",
    "        'flow_gt': 'other-gt-flow/Hydrangea/flow10.flo',\n",
    "    },\n",
    "        {\n",
    "        'input': 'other-data/RubberWhale/frame10.png',\n",
    "        'expected_out': 'other-data/RubberWhale/frame11.png',\n",
    "        'flow_gt': 'other-gt-flow/RubberWhale/flow10.flo',\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def midd_read_flow(path):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    # Based upon: https://github.com/Johswald/flow-code-python/blob/master/readFlowFile.py\n",
    "    # compute colored image to visualize optical flow file .flo\n",
    "    # Author: Johannes Oswald, Technical University Munich\n",
    "    # Contact: johannes.oswald@tum.de\n",
    "    # Date: 26/04/2017\n",
    "    # For more information, check http://vision.middlebury.edu/flow/ \n",
    "    assert os.path.exists(path) and path.endswith('.flo'), path\n",
    "    f = open(path, 'rb')\n",
    "    flo_number = np.fromfile(f, np.float32, count=1)[0]\n",
    "    TAG_FLOAT = 202021.25\n",
    "    assert flo_number == TAG_FLOAT, 'Flow number %r incorrect.' % flo_number\n",
    "    w = np.fromfile(f, np.int32, count=1)\n",
    "    h = np.fromfile(f, np.int32, count=1)\n",
    "\n",
    "    #if error try: data = np.fromfile(f, np.float32, count=2*w[0]*h[0])\n",
    "    data = np.fromfile(f, np.float32, count=int(2*w*h))\n",
    "\n",
    "    # Reshape data into 3D array (columns, rows, bands)\n",
    "    flow = np.resize(data, (int(h), int(w), 2))\t\n",
    "    f.close()\n",
    "\n",
    "    # We found that there are some invalid (?) (i.e. very large) flows, so we're going\n",
    "    # to ignore those for this experiment.\n",
    "    invalid = (flow >= 1666)\n",
    "    flow[invalid] = 0\n",
    "\n",
    "    return flow\n",
    "\n",
    "def midd_create_fp(uri):\n",
    "    scene_idx = int(uri.extra['midd.scene_idx'])\n",
    "    scene = MIDD_SCENES[scene_idx]\n",
    "    data_root = uri.extra['midd.dataroot']\n",
    "    return OpticalFlowPair(\n",
    "                uri=uri,\n",
    "                dataset=\"Middlebury Optical Flow\",\n",
    "                id1=scene['input'],\n",
    "                img1='file://' + os.path.join(data_root, scene['input']),\n",
    "                id2=scene['expected_out'],\n",
    "                img2='file://' + os.path.join(data_root, scene['expected_out']),\n",
    "                flow=CloudpickeledCallable(lambda: midd_read_flow(os.path.join(data_root, scene['flow_gt']))))\n",
    "    \n",
    "\n",
    "class MiddFactory(FlowPairFactoryBase):\n",
    "    DATASET = 'midd_oflow'\n",
    "    \n",
    "    @classmethod\n",
    "    def list_fp_uris(cls, spark):\n",
    "        return [\n",
    "            datum.URI(dataset=cls.DATASET, extra={'midd.scene_idx': i, 'midd.dataroot': MIDD_DATA_ROOT})\n",
    "            for i, scene in enumerate(MIDD_SCENES)\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        uri_rdd = spark.sparkContext.parallelize(uris)\n",
    "        fp_rdd = uri_rdd.map(midd_create_fp)\n",
    "        return fp_rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 02:01:31,835\toarph 3501100 : Source has changed! Rebuilding Egg ...\n",
      "2021-04-15 02:01:31,836\toarph 3501100 : Using source root /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis \n",
      "2021-04-15 02:01:31,837\toarph 3501100 : Using source root /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis \n",
      "2021-04-15 02:01:31,838\toarph 3501100 : Generating egg to /tmp/tmpnmfxezoj_oarphpy_eggbuild ...\n",
      "2021-04-15 02:01:31,845\toarph 3501100 : ... done.  Egg at /tmp/tmpnmfxezoj_oarphpy_eggbuild/cheap_optical_flow_eval_analysis-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from cheap_optical_flow_eval_analysis.midd import MiddFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 Midd scenes\n"
     ]
    }
   ],
   "source": [
    "ALL_FP_FACTORY_CLSS.append(MiddFactory)\n",
    "\n",
    "print(\"Found %s Midd scenes\" % len(MiddFactory.list_fp_uris(spark)))\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    demo_uris = MiddFactory.list_fp_uris(spark)\n",
    "    fp_rdd = MiddFactory.get_fp_rdd_for_uris(spark, demo_uris)\n",
    "    fps = fp_rdd.collect()\n",
    "    \n",
    "    for fp in fps:\n",
    "        show_html(fp.to_html() + \"<br/><br/><br/>\")\n",
    "        DEMO_FPS.append(fp)\n",
    "\n",
    "# for i, scene in enumerate(MIDD_SCENES):\n",
    "#     p = OpticalFlowPair(\n",
    "#             dataset=\"Middlebury Optical Flow\",\n",
    "#             id1=scene['input'],\n",
    "#             img1='file://' + os.path.join(MIDD_DATA_ROOT, scene['input']),\n",
    "#             id2=scene['expected_out'],\n",
    "#             img2='file://' + os.path.join(MIDD_DATA_ROOT, scene['expected_out']),\n",
    "#             flow=CloudpickeledCallable(lambda: midd_read_flow(os.path.join(MIDD_DATA_ROOT, scene['flow_gt']))))\n",
    "    \n",
    "#     if RUN_FULL_ANALYSIS:\n",
    "#         ALL_FPS.append(copy.deepcopy(p))\n",
    "    \n",
    "#     if SHOW_DEMO_OUTPUT:\n",
    "#         show_html(p.to_html() + \"<br/><br/><br/>\")\n",
    "#         DEMO_FPS.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepDeform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cheap_optical_flow_eval_analysis/deepdeform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cheap_optical_flow_eval_analysis/deepdeform.py\n",
    "\n",
    "from psegs import datum\n",
    "\n",
    "from cheap_optical_flow_eval_analysis.ofp import *\n",
    "\n",
    "# Please extract deepdeform_v1.7z to a directory and provide the target below:\n",
    "DD_DATA_ROOT = '/opt/psegs/ext_data/deepdeform_v1/'\n",
    "\n",
    "def dd_load_oflow(path):\n",
    "    # Based upon https://github.com/AljazBozic/DeepDeform/blob/master/utils.py#L1\n",
    "    import shutil\n",
    "    import struct\n",
    "    import os\n",
    "    import numpy as np\n",
    "\n",
    "    # Flow is stored row-wise in order [channels, height, width].\n",
    "    assert os.path.isfile(path)\n",
    "\n",
    "    flow_gt = None\n",
    "    with open(path, 'rb') as fin:\n",
    "        width = struct.unpack('I', fin.read(4))[0]\n",
    "        height = struct.unpack('I', fin.read(4))[0]\n",
    "        channels = struct.unpack('I', fin.read(4))[0]\n",
    "        n_elems = height * width * channels\n",
    "\n",
    "        flow = struct.unpack('f' * n_elems, fin.read(n_elems * 4))\n",
    "        flow_gt = np.asarray(flow, dtype=np.float32).reshape([channels, height, width])\n",
    "\n",
    "    # Match format used in this analysis\n",
    "    flow_gt = np.moveaxis(flow_gt, 0, -1) # (h, w, 2)\n",
    "    invalid_flow = flow_gt == -np.Inf\n",
    "    flow_gt[invalid_flow] = 0.0\n",
    "    return flow_gt\n",
    "\n",
    "def dd_create_fp(uri):\n",
    "    return OpticalFlowPair(\n",
    "                uri=uri,\n",
    "                dataset=\"DeepDeform Semi-Synthetic Optical Flow\",\n",
    "                id1=uri.extra['dd.input'],\n",
    "                img1='file://' + os.path.join(DD_DATA_ROOT, uri.extra['dd.input']),\n",
    "                id2=uri.extra['dd.expected_out'],\n",
    "                img2='file://' + os.path.join(DD_DATA_ROOT, uri.extra['dd.expected_out']),\n",
    "                flow=dd_load_oflow(os.path.join(DD_DATA_ROOT, uri.extra['dd.flow_gt'])))\n",
    "\n",
    "\n",
    "class DDFactory(FlowPairFactoryBase):\n",
    "    DATASET = 'deep_deform'\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_all_scenes(cls):\n",
    "        import json\n",
    "        DD_ALIGNMENTS = json.load(open(os.path.join(DD_DATA_ROOT, 'train_alignments.json')))\n",
    "        ALL_DD_SCENES = [\n",
    "            {\n",
    "                \"dd.input\": ascene['source_color'],\n",
    "                \"dd.expected_out\": ascene['target_color'],\n",
    "                \"dd.flow_gt\": ascene['optical_flow'],\n",
    "            }\n",
    "            for ascene in DD_ALIGNMENTS\n",
    "        ]\n",
    "        return ALL_DD_SCENES\n",
    "    \n",
    "    @classmethod\n",
    "    def list_fp_uris(cls, spark):\n",
    "        scenes = cls._get_all_scenes()\n",
    "        return [\n",
    "            datum.URI(dataset=cls.DATASET, extra=scene)\n",
    "            for scene in scenes\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        uri_rdd = spark.sparkContext.parallelize(uris)\n",
    "        fp_rdd = uri_rdd.map(dd_create_fp)\n",
    "        return fp_rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 02:01:31,903\toarph 3501100 : Source has changed! Rebuilding Egg ...\n",
      "2021-04-15 02:01:31,903\toarph 3501100 : Using source root /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis \n",
      "2021-04-15 02:01:31,904\toarph 3501100 : Using source root /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis \n",
      "2021-04-15 02:01:31,905\toarph 3501100 : Generating egg to /tmp/tmphp1vdjl9_oarphpy_eggbuild ...\n",
      "2021-04-15 02:01:31,912\toarph 3501100 : ... done.  Egg at /tmp/tmphp1vdjl9_oarphpy_eggbuild/cheap_optical_flow_eval_analysis-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from cheap_optical_flow_eval_analysis.deepdeform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4540 DeepDeform scenes\n"
     ]
    }
   ],
   "source": [
    "from psegs import datum\n",
    "\n",
    "DD_DEMO_URIS = [\n",
    "    datum.URI(dataset=DDFactory.DATASET, extra={\n",
    "        \"dd.input\": \"train/seq000/color/000000.jpg\",\n",
    "        \"dd.expected_out\": \"train/seq000/color/000200.jpg\",\n",
    "        \"dd.flow_gt\": \"train/seq000/optical_flow/blackdog_000000_000200.oflow\",\n",
    "    }),\n",
    "    datum.URI(dataset=DDFactory.DATASET, extra={\n",
    "        \"dd.input\": \"train/seq000/color/000000.jpg\",\n",
    "        \"dd.expected_out\": \"train/seq000/color/001200.jpg\",\n",
    "        \"dd.flow_gt\": \"train/seq000/optical_flow/blackdog_000000_001200.oflow\",\n",
    "    }),\n",
    "    datum.URI(dataset=DDFactory.DATASET, extra={\n",
    "        \"dd.input\": \"train/seq001/color/003400.jpg\",\n",
    "        \"dd.expected_out\": \"train/seq001/color/003600.jpg\",\n",
    "        \"dd.flow_gt\": \"train/seq001/optical_flow/lady_003400_003600.oflow\",\n",
    "    }),\n",
    "    datum.URI(dataset=DDFactory.DATASET, extra={\n",
    "        \"dd.input\": \"train/seq337/color/000050.jpg\",\n",
    "        \"dd.expected_out\": \"train/seq337/color/000350.jpg\",\n",
    "        \"dd.flow_gt\": \"train/seq337/optical_flow/adult_000050_000350.oflow\",\n",
    "    }),\n",
    "]\n",
    "\n",
    "ALL_FP_FACTORY_CLSS.append(DDFactory)\n",
    "\n",
    "print(\"Found %s DeepDeform scenes\" % len(DDFactory.list_fp_uris(spark)))\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    fp_rdd = DDFactory.get_fp_rdd_for_uris(spark, DD_DEMO_URIS)\n",
    "    fps = fp_rdd.collect()\n",
    "    \n",
    "    for fp in fps:\n",
    "        show_html(fp.to_html() + \"<br/><br/><br/>\")\n",
    "        DEMO_FPS.append(fp)\n",
    "\n",
    "\n",
    "\n",
    "# import json\n",
    "# DD_ALIGNMENTS = json.load(open(os.path.join(DD_DATA_ROOT, 'train_alignments.json')))\n",
    "# ALL_DD_SCENES = [\n",
    "#     {\n",
    "#         \"input\": ascene['source_color'],\n",
    "#         \"expected_out\": ascene['target_color'],\n",
    "#         \"flow_gt\": ascene['optical_flow'],\n",
    "#     }\n",
    "#     for ascene in DD_ALIGNMENTS\n",
    "# ]\n",
    "\n",
    "# print(\"Found %s DeepDeform scenes\" % len(ALL_DD_SCENES))\n",
    "# if SHOW_DEMO_OUTPUT:\n",
    "#     for scene in DD_DEMO_SCENES:\n",
    "#         p = dd_create_fp(scene)\n",
    "#         show_html(p.to_html())\n",
    "#         DEMO_FPS.append(p)\n",
    "\n",
    "# if RUN_FULL_ANALYSIS:\n",
    "#     for scene in ALL_DD_SCENES:\n",
    "#         p = dd_create_fp(scene)\n",
    "#         ALL_FPS.append(p)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kitti Scene Flow Benchmark (2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Please unzip `data_scene_flow.zip` and `data_scene_flow_calib.zip` to a directory and provide that target below:\n",
    "# KITTI_SF15_DATA_ROOT = '/opt/psegs/ext_data/kitti_scene_flow_2015/'\n",
    "\n",
    "\n",
    "\n",
    "# from oarphpy import util as oputil\n",
    "# KITTI_SF15_ALL_FLOW_OCC = [\n",
    "#     os.path.basename(p)\n",
    "#     for p in oputil.all_files_recursive(\n",
    "#         os.path.join(KITTI_SF15_DATA_ROOT, 'training/flow_occ'), pattern='*.png')\n",
    "# ]\n",
    "    \n",
    "# KITTI_SF15_ALL_SCENES = [\n",
    "#     {\n",
    "#         \"input\": 'training/image_2/%s' % fname,\n",
    "#         \"expected_out\": 'training/image_2/%s' % fname.replace('_10', '_11'),\n",
    "#         \"flow_gt\": 'training/flow_occ/%s' % fname,\n",
    "#     }\n",
    "#     for fname in KITTI_SF15_ALL_FLOW_OCC\n",
    "# ]\n",
    "# print(\"Found %s KITTI SceneFlow 2015 scenes\" % len(KITTI_SF15_ALL_SCENES))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cheap_optical_flow_eval_analysis/kittisf15.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cheap_optical_flow_eval_analysis/kittisf15.py\n",
    "\n",
    "from psegs import datum\n",
    "\n",
    "from cheap_optical_flow_eval_analysis.ofp import *\n",
    "\n",
    "# Please unzip `data_scene_flow.zip` and `data_scene_flow_calib.zip` to a directory and provide that target below:\n",
    "KITTI_SF15_DATA_ROOT = '/opt/psegs/ext_data/kitti_scene_flow_2015/'\n",
    "\n",
    "\n",
    "def kittisf15_load_flow(path):\n",
    "    # Based upon https://github.com/liruoteng/OpticalFlowToolkit/blob/master/lib/flowlib.py#L559\n",
    "    import png\n",
    "    import numpy as np\n",
    "    flow_object = png.Reader(filename=path)\n",
    "    flow_direct = flow_object.asDirect()\n",
    "    flow_data = list(flow_direct[2])\n",
    "    w, h = flow_direct[3]['size']\n",
    "    flow = np.zeros((h, w, 3), dtype=np.float64)\n",
    "    for i in range(len(flow_data)):\n",
    "        flow[i, :, 0] = flow_data[i][0::3]\n",
    "        flow[i, :, 1] = flow_data[i][1::3]\n",
    "        flow[i, :, 2] = flow_data[i][2::3]\n",
    "\n",
    "    invalid_idx = (flow[:, :, 2] == 0)\n",
    "    flow[:, :, 0:2] = (flow[:, :, 0:2] - 2 ** 15) / 64.0\n",
    "    flow[invalid_idx, 0] = 0\n",
    "    flow[invalid_idx, 1] = 0\n",
    "    return flow[:, :, :2]\n",
    "\n",
    "def kittisf15_create_fp(uri):\n",
    "    return OpticalFlowPair(\n",
    "                uri=uri,\n",
    "                dataset=\"KITTI Scene Flow 2015\",\n",
    "                id1=uri.extra['ksf15.input'],\n",
    "                img1='file://' + os.path.join(KITTI_SF15_DATA_ROOT, uri.extra['ksf15.input']),\n",
    "                id2=uri.extra['ksf15.expected_out'],\n",
    "                img2='file://' + os.path.join(KITTI_SF15_DATA_ROOT, uri.extra['ksf15.expected_out']),\n",
    "                flow=kittisf15_load_flow(os.path.join(KITTI_SF15_DATA_ROOT, uri.extra['ksf15.flow_gt'])))\n",
    "\n",
    "\n",
    "class KITTISF15Factory(FlowPairFactoryBase):\n",
    "    DATASET = 'kitti_sf15'\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_all_scenes(cls):\n",
    "        from oarphpy import util as oputil\n",
    "        KITTI_SF15_ALL_FLOW_OCC = [\n",
    "            os.path.basename(p)\n",
    "            for p in oputil.all_files_recursive(\n",
    "                os.path.join(KITTI_SF15_DATA_ROOT, 'training/flow_occ'), pattern='*.png')\n",
    "        ]\n",
    "\n",
    "        KITTI_SF15_ALL_SCENES = [\n",
    "            {\n",
    "                \"ksf15.input\": 'training/image_2/%s' % fname,\n",
    "                \"ksf15.expected_out\": 'training/image_2/%s' % fname.replace('_10', '_11'),\n",
    "                \"ksf15.flow_gt\": 'training/flow_occ/%s' % fname,\n",
    "            }\n",
    "            for fname in KITTI_SF15_ALL_FLOW_OCC\n",
    "        ]\n",
    "        return KITTI_SF15_ALL_SCENES\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def list_fp_uris(cls, spark):\n",
    "        scenes = cls._get_all_scenes()\n",
    "        return [\n",
    "            datum.URI(dataset=cls.DATASET, extra=scene)\n",
    "            for scene in scenes\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        uri_rdd = spark.sparkContext.parallelize(uris)\n",
    "        fp_rdd = uri_rdd.map(kittisf15_create_fp)\n",
    "        return fp_rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 02:01:32,060\toarph 3501100 : Source has changed! Rebuilding Egg ...\n",
      "2021-04-15 02:01:32,060\toarph 3501100 : Using source root /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis \n",
      "2021-04-15 02:01:32,061\toarph 3501100 : Using source root /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis \n",
      "2021-04-15 02:01:32,062\toarph 3501100 : Generating egg to /tmp/tmprdjz7bio_oarphpy_eggbuild ...\n",
      "2021-04-15 02:01:32,069\toarph 3501100 : ... done.  Egg at /tmp/tmprdjz7bio_oarphpy_eggbuild/cheap_optical_flow_eval_analysis-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from cheap_optical_flow_eval_analysis.kittisf15 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 Kitti Scene Flow 2015 scenes\n"
     ]
    }
   ],
   "source": [
    "from psegs import datum\n",
    "\n",
    "# You have to ls flow_occ to get the paths\n",
    "KITTI_SF15_DEMO_URIS = [\n",
    "    datum.URI(dataset=KITTISF15Factory.DATASET, extra={\n",
    "        'ksf15.input': 'training/image_2/000000_10.png',\n",
    "        'ksf15.expected_out': 'training/image_2/000000_11.png',\n",
    "        'ksf15.flow_gt': 'training/flow_occ/000000_10.png',\n",
    "    }),\n",
    "    datum.URI(dataset=KITTISF15Factory.DATASET, extra={\n",
    "        'ksf15.input': 'training/image_2/000007_10.png',\n",
    "        'ksf15.expected_out': 'training/image_2/000007_11.png',\n",
    "        'ksf15.flow_gt': 'training/flow_occ/000007_10.png',\n",
    "    }),\n",
    "    datum.URI(dataset=KITTISF15Factory.DATASET, extra={\n",
    "        'ksf15.input': 'training/image_2/000023_10.png',\n",
    "        'ksf15.expected_out': 'training/image_2/000023_11.png',\n",
    "        'ksf15.flow_gt': 'training/flow_occ/000023_10.png',\n",
    "    }),\n",
    "    datum.URI(dataset=KITTISF15Factory.DATASET, extra={\n",
    "        'ksf15.input': 'training/image_2/000051_10.png',\n",
    "        'ksf15.expected_out': 'training/image_2/000051_11.png',\n",
    "        'ksf15.flow_gt': 'training/flow_occ/000051_10.png',\n",
    "    }),\n",
    "    datum.URI(dataset=KITTISF15Factory.DATASET, extra={\n",
    "        'ksf15.input': 'training/image_2/000003_10.png',\n",
    "        'ksf15.expected_out': 'training/image_2/000003_11.png',\n",
    "        'ksf15.flow_gt': 'training/flow_occ/000003_10.png',\n",
    "    }),\n",
    "]\n",
    "\n",
    "ALL_FP_FACTORY_CLSS.append(KITTISF15Factory)\n",
    "\n",
    "print(\"Found %s Kitti Scene Flow 2015 scenes\" % len(KITTISF15Factory.list_fp_uris(spark)))\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    fp_rdd = KITTISF15Factory.get_fp_rdd_for_uris(spark, KITTI_SF15_DEMO_URIS)\n",
    "    fps = fp_rdd.collect()\n",
    "    \n",
    "    for fp in fps:\n",
    "        show_html(fp.to_html() + \"<br/><br/><br/>\")\n",
    "        DEMO_FPS.append(fp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def kitti_sf15_create_fp(info):\n",
    "#      return OpticalFlowPair(\n",
    "#                 dataset=\"KITTI Scene Flow 2015\",\n",
    "#                 id1=scene['input'],\n",
    "#                 img1='file://' + os.path.join(KITTI_SF15_DATA_ROOT, scene['input']),\n",
    "#                 id2=scene['expected_out'],\n",
    "#                 img2='file://' + os.path.join(KITTI_SF15_DATA_ROOT, scene['expected_out']),\n",
    "#                 flow=KITTISF15LoadFlowFromPng(os.path.join(KITTI_SF15_DATA_ROOT, scene['flow_gt'])))\n",
    "\n",
    "# if SHOW_DEMO_OUTPUT:\n",
    "#     for scene in KITTI_SF15_DEMO_SCENES:\n",
    "#         p = kitti_sf15_create_fp(scene)\n",
    "#         show_html(p.to_html())\n",
    "#         DEMO_FPS.append(p)\n",
    "\n",
    "# if RUN_FULL_ANALYSIS:\n",
    "#     for scene in KITTI_SF15_ALL_SCENES:\n",
    "#         p = kitti_sf15_create_fp(scene)\n",
    "#         ALL_FPS.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSegs Synthetic Flow from Fused Lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEGS_SYNTHFLOW_PARQUET_ROOT = '/outer_root/media/rocket4q/psegs_flow_records_short'\n",
    "\n",
    "# from psegs.exp.fused_lidar_flow import FlowRecTable\n",
    "\n",
    "# T = FlowRecTable(spark, PSEGS_SYNTHFLOW_PARQUET_ROOT)\n",
    "# synthflow_record_uris = T.get_record_uris()\n",
    "# print(\"Found %s PSegs SynthFlow records\" % len(synthflow_record_uris))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fr_samp_rdd = T.get_records_with_samples_rdd(\n",
    "#                     record_uris=[PSEGS_SYNTHFLOW_DEMO_RECORD_URIS[0]],\n",
    "#                     include_cameras=False,\n",
    "#                     include_cuboids=False,\n",
    "#                     include_point_clouds=False)\n",
    "# flow_rec = fr_samp_rdd.take(1)[0][0]\n",
    "\n",
    "# print(\"Sample record:\")\n",
    "# show_html(flow_rec.to_html())\n",
    "\n",
    "\n",
    "# PSEGS_SYNTHFLOW_DEMO_FPS_DO_CACHE = True\n",
    "# PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH = '/tmp/psegs_synthflow_demo.pkl'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cheap_optical_flow_eval_analysis/psegs_synthflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cheap_optical_flow_eval_analysis/psegs_synthflow.py\n",
    "\n",
    "from psegs import datum\n",
    "from psegs.exp.fused_lidar_flow import FlowRecTable\n",
    "\n",
    "from cheap_optical_flow_eval_analysis.ofp import *\n",
    "\n",
    "from oarphpy.spark import CloudpickeledCallable\n",
    "\n",
    "\n",
    "# Please provide the PSegs synthetic flow Parquet directory root below:\n",
    "PSEGS_SYNTHFLOW_PARQUET_ROOT = '/outer_root/media/rocket4q/psegs_flow_records_short_fixed'\n",
    "# PSEGS_SYNTHFLOW_PARQUET_ROOT = '/outer_root/media/rocket4q/psegs_synthflow.parquet'\n",
    "\n",
    "def psegs_synthflow_flow_rec_to_fp(flow_rec, sample):\n",
    "  fr = flow_rec\n",
    "\n",
    "  uri_str_to_datum = sample.get_uri_str_to_datum()\n",
    "\n",
    "  # Find the camera_images associated with `flow_rec`\n",
    "  ci1_url_str = str(flow_rec.clouds[0].ci_uris[0])\n",
    "  ci1_sd = uri_str_to_datum[ci1_url_str]\n",
    "  ci1 = ci1_sd.camera_image\n",
    "\n",
    "  ci2_url_str = str(flow_rec.clouds[1].ci_uris[0])\n",
    "  ci2_sd = uri_str_to_datum[ci2_url_str]\n",
    "  ci2 = ci2_sd.camera_image\n",
    "\n",
    "  import numpy as np\n",
    "  world_T1 = ci1.ego_pose.translation\n",
    "  world_T2 = ci2.ego_pose.translation\n",
    "  translation_meters = np.linalg.norm(world_T2 - world_T1)\n",
    "\n",
    "  id1 = ci1_url_str + '&extra.psegs_flow_sids=' + str(fr.clouds[0].sample_id)\n",
    "  id2 = ci2_url_str + '&extra.psegs_flow_sids=' + str(fr.clouds[1].sample_id)\n",
    "\n",
    "  import urllib.parse\n",
    "  eval_uri = datum.URI(dataset=PSegsSynthFlowFactory.DATASET, extra={'pssf.ruri': urllib.parse.quote(str(fr.uri))})\n",
    "\n",
    "  uvdviz_im1 = flow_rec.clouds[0].uvdvis\n",
    "  uvdviz_im2 = flow_rec.clouds[1].uvdvis\n",
    "  K = ci1.K\n",
    "\n",
    "  fp = OpticalFlowPair(\n",
    "          uri=eval_uri,\n",
    "          dataset=\"PSegs SynthFlow for %s (%s)\" % (fr.uri.dataset, fr.uri.split),\n",
    "          id1=id1,\n",
    "          id2=id2,\n",
    "          img1=CloudpickeledCallable(lambda: ci1.image),\n",
    "          img2=CloudpickeledCallable(lambda: ci2.image),\n",
    "          flow=CloudpickeledCallable(lambda: fr.to_optical_flow()),\n",
    "\n",
    "          diff_time_sec=abs(ci2_sd.uri.timestamp - ci1_sd.uri.timestamp),\n",
    "          translation_meters=translation_meters,\n",
    "      \n",
    "          uvdviz_im1=uvdviz_im1,\n",
    "          uvdviz_im2=uvdviz_im2,\n",
    "          K=K)\n",
    "  return fp\n",
    "\n",
    "def psegs_synthflow_create_fps(\n",
    "        spark,\n",
    "        flow_record_pq_table_path,\n",
    "        record_uris,\n",
    "        include_cuboids=False,\n",
    "        include_point_clouds=False):\n",
    "\n",
    "  T = FlowRecTable(spark, flow_record_pq_table_path)\n",
    "  rec_sample_rdd = T.get_records_with_samples_rdd(\n",
    "                          record_uris=record_uris,\n",
    "                          include_cameras=True,\n",
    "                          include_cuboids=include_cuboids,\n",
    "                          include_point_clouds=include_point_clouds)\n",
    "\n",
    "  fps = [\n",
    "    flow_rec_to_fp(flow_rec, sample)\n",
    "    for flow_rec, sample in rec_sample_rdd.collect()\n",
    "  ]\n",
    "\n",
    "  return fps\n",
    "\n",
    "\n",
    "class PSegsSynthFlowFactory(FlowPairFactoryBase):\n",
    "    DATASET = 'psegs_synthflow'\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_frec_table(cls, spark):\n",
    "        if not hasattr(cls, '_frec_table'):\n",
    "            cls._frec_table = FlowRecTable(spark, PSEGS_SYNTHFLOW_PARQUET_ROOT)\n",
    "        return cls._frec_table\n",
    "    \n",
    "    @classmethod\n",
    "    def list_fp_uris(cls, spark):\n",
    "        import urllib.parse\n",
    "        T = cls._get_frec_table(spark)\n",
    "        ruris = T.get_record_uris()\n",
    "        return [\n",
    "            datum.URI(dataset=cls.DATASET, extra={'pssf.ruri': urllib.parse.quote(str(ruri))})\n",
    "            for ruri in ruris\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_fp_rdd_for_uris(cls, spark, uris):\n",
    "        import urllib.parse\n",
    "        T = cls._get_frec_table(spark)\n",
    "        ruris = [urllib.parse.unquote(uri.extra['pssf.ruri']) for uri in uris]\n",
    "        rec_sample_rdd = T.get_records_with_samples_rdd(\n",
    "                          record_uris=ruris,\n",
    "                          include_cameras=True,\n",
    "                          include_cuboids=False,\n",
    "                          include_point_clouds=False)\n",
    "        fp_rdd = rec_sample_rdd.map(lambda fs: psegs_synthflow_flow_rec_to_fp(*fs))\n",
    "        return fp_rdd\n",
    "        \n",
    "\n",
    "\n",
    "# def psegs_synthflow_iter_fp_rdds(\n",
    "#         spark,\n",
    "#         flow_record_pq_table_path,\n",
    "#         fps_per_rdd=100,\n",
    "#         include_cuboids=False,\n",
    "#         include_point_clouds=False):\n",
    "  \n",
    "#   T = FlowRecTable(spark, flow_record_pq_table_path)\n",
    "#   ruris = T.get_record_uris()\n",
    "\n",
    "#   # Ensure a sort so that pairs from similar segments will load in the same\n",
    "#   # RDD -- that makes joins smaller and faster\n",
    "#   ruris = sorted(ruris)\n",
    "\n",
    "#   from oarphpy import util as oputil\n",
    "#   for ruri_chunk in oputil.ichunked(ruris, fps_per_rdd):\n",
    "#     frec_sample_rdd = T.get_records_with_samples_rdd(\n",
    "#                           record_uris=rids,\n",
    "#                           include_cuboids=include_cuboids,\n",
    "#                           include_point_clouds=include_point_clouds)\n",
    "#     fp_rdd = frec_sample_rdd.map(flow_rec_to_fp)\n",
    "#     yield fp_rdd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 02:01:32,337\toarph 3501100 : Source has changed! Rebuilding Egg ...\n",
      "2021-04-15 02:01:32,338\toarph 3501100 : Using source root /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis \n",
      "2021-04-15 02:01:32,339\toarph 3501100 : Using source root /tmp/tmp4376nzva_cheap_optical_flow_eval_analysis \n",
      "2021-04-15 02:01:32,340\toarph 3501100 : Generating egg to /tmp/tmphccawm3g_oarphpy_eggbuild ...\n",
      "2021-04-15 02:01:32,346\toarph 3501100 : ... done.  Egg at /tmp/tmphccawm3g_oarphpy_eggbuild/cheap_optical_flow_eval_analysis-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from cheap_optical_flow_eval_analysis.psegs_synthflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 191 PSegs SynthFlow scenes\n"
     ]
    }
   ],
   "source": [
    "from psegs import datum\n",
    "\n",
    "import urllib.parse\n",
    "\n",
    "PSEGS_SYNTHFLOW_DEMO_RECORD_RURIS = (\n",
    "  'psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0000_sync&extra.psegs_flow_sids=4340,4339',\n",
    "  'psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0000_sync&extra.psegs_flow_sids=11219,11269',\n",
    "\n",
    "  'psegs://dataset=nuscenes&split=train_track&segment_id=scene-0501&extra.psegs_flow_sids=40009,40010',\n",
    "  'psegs://dataset=nuscenes&split=train_track&segment_id=scene-0501&extra.psegs_flow_sids=50013,50014',\n",
    "\n",
    "#   'psegs://dataset=kitti-360-fused&split=train&segment_id=2013_05_28_drive_0000_sync&extra.psegs_flow_sids=11103,11104',\n",
    "#   'psegs://dataset=kitti-360-fused&split=train&segment_id=2013_05_28_drive_0000_sync&extra.psegs_flow_sids=1181,1182',\n",
    "\n",
    "#   'psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0002&extra.psegs_flow_sids=10016,10017',\n",
    "#   'psegs://dataset=nuscenes&split=train_detect&segment_id=scene-0582&extra.psegs_flow_sids=60035,60036',\n",
    "\n",
    "#   'psegs://dataset=nuscenes&split=train_track&segment_id=scene-0393&extra.psegs_flow_sids=50017,50018',\n",
    "#   'psegs://dataset=nuscenes&split=train_track&segment_id=scene-0501&extra.psegs_flow_sids=40019,40020',\n",
    ")\n",
    "\n",
    "PSEGS_SYNTHFLOW_DEMO_URIS = [\n",
    "    datum.URI(dataset=PSegsSynthFlowFactory.DATASET, extra={\n",
    "        'pssf.ruri': urllib.parse.quote(ruri_str)\n",
    "    })\n",
    "    for ruri_str in PSEGS_SYNTHFLOW_DEMO_RECORD_RURIS\n",
    "]\n",
    "\n",
    "ALL_FP_FACTORY_CLSS.append(PSegsSynthFlowFactory)\n",
    "\n",
    "print(\"Found %s PSegs SynthFlow scenes\" % len(PSegsSynthFlowFactory.list_fp_uris(spark)))\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    if os.path.exists(PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH):\n",
    "        print(\"Loading demo FlowPairs from %s\" % PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH)\n",
    "        import pickle\n",
    "        fps = pickle.load(open(PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH, 'rb'))\n",
    "    else:\n",
    "        print(\"Building Demo FlowPairs, this might take a while ....\")\n",
    "        fp_rdd = PSegsSynthFlowFactory.get_fp_rdd_for_uris(spark, PSEGS_SYNTHFLOW_DEMO_URIS)\n",
    "        fps = fp_rdd.collect()\n",
    "        if PSEGS_SYNTHFLOW_DEMO_FPS_DO_CACHE:\n",
    "            print(\"Saving demo FlowPairs to %s ...\" % PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH)\n",
    "            import pickle\n",
    "            with open(PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH, 'wb') as f:\n",
    "                pickle.dump(fps, f, protocol=4)\n",
    "    \n",
    "    for fp in fps:\n",
    "        show_html(fp.to_html())\n",
    "        DEMO_FPS.append(fp)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     import urllib.parse\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     for fp in fps:\n",
    "#         show_html(fp.to_html() + \"<br/><br/><br/>\")\n",
    "#         DEMO_FPS.append(fp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if SHOW_DEMO_OUTPUT:\n",
    "#     if os.path.exists(PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH):\n",
    "#         print(\"Loading demo FlowPairs from %s\" % PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH)\n",
    "#         import pickle\n",
    "#         fps = pickle.load(open(PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH, 'rb'))\n",
    "#     else:\n",
    "#         print(\"Building Demo FlowPairs, this might take a while ....\")\n",
    "#         fps = psegs_synthflow_create_fps(spark, PSEGS_SYNTHFLOW_PARQUET_ROOT, PSEGS_SYNTHFLOW_DEMO_RECORD_URIS)\n",
    "#         if PSEGS_SYNTHFLOW_DEMO_FPS_DO_CACHE:\n",
    "#             print(\"Saving demo FlowPairs to %s ...\" % PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH)\n",
    "#             import pickle\n",
    "#             with open(PSEGS_SYNTHFLOW_DEMO_FPS_CACHE_PATH, 'wb') as f:\n",
    "#                 pickle.dump(fps, f, protocol=4)\n",
    "    \n",
    "#     for fp in fps:\n",
    "#         show_html(fp.to_html())\n",
    "#         DEMO_FPS.append(fp)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction via Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Reconstruction via Optical Flow\n",
    "\n",
    "def zero_flow(flow):\n",
    "    return (flow[:, :, :2] == np.array([0, 0])).all(axis=-1)\n",
    "\n",
    "def warp_flow_backwards(img, flow):\n",
    "    \"\"\"Given an image, apply the inverse of `flow`\"\"\"\n",
    "    h, w = flow.shape[:2]\n",
    "    flow = -flow\n",
    "    flow[:,:,0] += np.arange(w)\n",
    "    flow[:,:,1] += np.arange(h)[:,np.newaxis]\n",
    "    res = cv2.remap(img, flow.astype(np.float32), None, cv2.INTER_LINEAR)\n",
    "    return res\n",
    "    \n",
    "def warp_flow_forwards(img, flow):\n",
    "    \"\"\"Given an image, apply the given optical flow `flow`.  Returns not only the warped\n",
    "    image, but a `mask` indicating warped pixels (i.e. there was non-zero flow *into* these pixels ).\n",
    "    With some help from https://stackoverflow.com/questions/41703210/inverting-a-real-valued-index-grid/46009462#46009462\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    pts = flow.copy()\n",
    "    pts[:, :, 0] += np.arange(w)\n",
    "    pts[:, :, 1] += np.arange(h)[:, np.newaxis]\n",
    "    exclude = zero_flow(flow)\n",
    "    if exclude.all():\n",
    "        # No flow anywhere!\n",
    "        return img.copy(), np.zeros((h, w)).astype(np.bool)\n",
    "    else:\n",
    "        inpts = pts[~exclude]\n",
    "    \n",
    "    from scipy.interpolate import griddata\n",
    "    inpts = np.reshape(inpts, [-1, 2])\n",
    "    grid_y, grid_x = np.mgrid[:h, :w]\n",
    "    chan_out = []\n",
    "    for ch in range(img.shape[-1]):\n",
    "        spts = img[:, :, ch][~exclude].reshape([-1, 1])\n",
    "        mapped = griddata(inpts, spts, (grid_x, grid_y), method='linear')\n",
    "        chan_out.append(mapped.astype(img.dtype))\n",
    "    out = np.stack(chan_out, axis=-1)\n",
    "    out = out.reshape([h, w, len(chan_out)])\n",
    "\n",
    "    mask = np.reshape(inpts, [-1, 2])\n",
    "    mask = np.rint(mask).astype(np.int)\n",
    "    mask = mask[np.where((mask[:, 0] >= 0) & (mask[:, 0] < w) & (mask[:, 1] >= 0) & (mask[:, 1] < h))]\n",
    "    valid_mask = np.zeros((h, w))\n",
    "    valid_mask[mask[:, 1], mask[:, 0]] = 1\n",
    "    \n",
    "    return out, valid_mask.astype(np.bool)\n",
    "\n",
    "# @attr.s(slots=True, eq=False, weakref_slot=False)\n",
    "class FlowReconstructedImagePair(object):\n",
    "    \"\"\"A pair of reconstructed images using an input pair of images and optical\n",
    "    flow field (i.e. an `OpticalFlowPair` instance).\"\"\"\n",
    "\n",
    "    slots = (\n",
    "        'opair',\n",
    "        'img2_recon_fwd',\n",
    "        'img2_recon_fwd_valid',\n",
    "        'img1_recon_bkd',\n",
    "        'img1_recon_bkd_valid'\n",
    "    )\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        for k in self.slots:\n",
    "            setattr(self, k, kwargs.get(k))\n",
    "    \n",
    "#     opair = attr.ib(default=OpticalFlowPair())\n",
    "#     \"\"\"The original `OpticalFlowPair` with the source of the data for this reconstruction result.\"\"\"\n",
    "    \n",
    "#     img2_recon_fwd = attr.ib(default=np.array([]))\n",
    "#     \"\"\"A Numpy image containing the result of FORWARDS-WARPING OpticalFlowPair::img1\n",
    "#     via OpticalFlowPair::flow to reconstruct OpticalFlowPair::img2\"\"\"\n",
    "\n",
    "#     img2_recon_fwd_valid = attr.ib(default=np.array([]))\n",
    "#     \"\"\"A Numpy boolean mask indicating which pixels of `img2_recon_fwd` were modified via non-zero flow\"\"\"\n",
    "    \n",
    "#     img1_recon_bkd = attr.ib(default=np.array([]))\n",
    "#     \"\"\"A Numpy image containing the result of BACKWARDS-WARPING OpticalFlowPair::img2\n",
    "#     via OpticalFlowPair::flow to reconstruct OpticalFlowPair::img1\"\"\"\n",
    "\n",
    "#     img1_recon_bkd_valid = attr.ib(default=np.array([]))\n",
    "#     \"\"\"A Numpy boolean mask indicating which pixels of `img1_recon_bkd` were modified via non-zero flow\"\"\"\n",
    "        \n",
    "    @classmethod\n",
    "    def create_from(cls, oflow_pair: OpticalFlowPair):\n",
    "        flow = oflow_pair.get_flow()\n",
    "        \n",
    "        # Forward Warp\n",
    "        fwarped, fvalid = warp_flow_forwards(oflow_pair.get_img1(), flow)\n",
    "\n",
    "        # Backwards Warp\n",
    "        exclude = zero_flow(flow)\n",
    "        bwarped = warp_flow_backwards(oflow_pair.get_img2(), -flow[:, :, :2])\n",
    "        bvalid = ~exclude\n",
    "        \n",
    "        return FlowReconstructedImagePair(\n",
    "                opair=oflow_pair,\n",
    "                img2_recon_fwd=fwarped,\n",
    "                img2_recon_fwd_valid=fvalid,\n",
    "                img1_recon_bkd=bwarped,\n",
    "                img1_recon_bkd_valid=bvalid)\n",
    "    \n",
    "    def to_html(self):\n",
    "        # We use pixels from the destination image in order to make the reconstruction \n",
    "        # easier to interpret; we'll fade them in intensity so that they are more\n",
    "        # conspicuous.        \n",
    "        FADE_UNTOUCHED_PIXELS = 0.3\n",
    "        \n",
    "        viz_fwd = self.img2_recon_fwd.copy().astype(np.float32)\n",
    "        im2 = self.opair.get_img2()\n",
    "        if (~self.img2_recon_fwd_valid).any():\n",
    "            viz_fwd[~self.img2_recon_fwd_valid] = im2[~self.img2_recon_fwd_valid]\n",
    "            viz_fwd[~self.img2_recon_fwd_valid] *= FADE_UNTOUCHED_PIXELS\n",
    "        else:\n",
    "            # viz_fwd = im2.copy() * FADE_UNTOUCHED_PIXELS\n",
    "            print('no invalids forward!')\n",
    "        \n",
    "        viz_bkd = self.img1_recon_bkd.copy().astype(np.float32)\n",
    "        im1 = self.opair.get_img1()\n",
    "        if (~self.img1_recon_bkd_valid).any():\n",
    "            viz_bkd[~self.img1_recon_bkd_valid] = im1[~self.img1_recon_bkd_valid]\n",
    "            viz_bkd[~self.img1_recon_bkd_valid] *= FADE_UNTOUCHED_PIXELS\n",
    "        else:\n",
    "            # viz_bkd = im1.copy() * FADE_UNTOUCHED_PIXELS\n",
    "            print('no invalids backwards!')\n",
    "        \n",
    "        html = \"\"\"\n",
    "            <table>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Forwards Warped <i>(dark pixels unwarped)</i></b></td></tr>\n",
    "            <tr><td><img src=\"{viz_fwd}\" width=\"100%\" /></td></tr>\n",
    "\n",
    "            <tr><td style=\"text-align:left\"><b>Backwards Warped <i>(dark pixels unwarped)</i></b></td></tr>\n",
    "            <tr><td><img src=\"{viz_bkd}\" width=\"100%\" /></td></tr>\n",
    "\n",
    "            </table>\n",
    "        \"\"\".format(\n",
    "                viz_fwd=img_to_data_uri(viz_fwd.astype(np.uint8)),\n",
    "                viz_bkd=img_to_data_uri(viz_bkd.astype(np.uint8)))\n",
    "        return html\n",
    "\n",
    "        \n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    DEMO_RECONS = []\n",
    "    for p in DEMO_FPS:\n",
    "        recon = FlowReconstructedImagePair.create_from(p)\n",
    "        show_html(recon.to_html() + \"</br></br></br>\")\n",
    "        DEMO_RECONS.append(recon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analysis Utils\n",
    "\n",
    "def mse(i1, i2, valid):\n",
    "    return np.mean((i1[valid] - i2[valid]) ** 2)\n",
    "\n",
    "def rmse(i1, i2, valid):\n",
    "    return math.sqrt(mse(i1, i2, valid))\n",
    "\n",
    "def psnr(i1, i2, valid):\n",
    "    return 20 * math.log10(255) - 10 * math.log10(max((mse(i1, i2, valid), 1e-12)))\n",
    "\n",
    "def ssim(i1, i2, valid):\n",
    "    # Some variance out there ...\n",
    "    # https://github.com/scikit-image/scikit-image/blob/master/skimage/metrics/_structural_similarity.py#L12-L232\n",
    "    # https://github.com/nianticlabs/monodepth2/blob/13200ab2f29f2f10dec3aa5db29c32a23e29d376/layers.py#L218\n",
    "    # https://cvnote.ddlee.cn/2019/09/12/psnr-ssim-python\n",
    "    # We will just use SKImage for now ...\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    mssim, S = ssim(i1, i2, win_size=11, multichannel=True, full=True)\n",
    "    return np.mean(S[valid])\n",
    "\n",
    "def to_edge_im(img):\n",
    "    return np.stack([\n",
    "        cv2.Laplacian(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), cv2.CV_32F, ksize=1),\n",
    "        cv2.Sobel(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), cv2.CV_32F, 1, 0, ksize=3),\n",
    "        cv2.Sobel(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), cv2.CV_32F, 0, 1, ksize=3),\n",
    "    ], axis=-1)\n",
    "\n",
    "def edges_mse(i1, i2, valid):\n",
    "    return mse(to_edge_im(i1), to_edge_im(i2), valid)\n",
    "\n",
    "\n",
    "def oflow_coverage(valid):\n",
    "    return valid.sum() / (valid.shape[0] * valid.shape[1])\n",
    "\n",
    "def oflow_magnitude_hist(flow, valid, bins=50):\n",
    "    flow_l2s = np.sqrt( flow[valid][:, 0] ** 2 + flow[valid][:, 1] ** 2 )\n",
    "    bin_counts, bin_edges = np.histogram(flow_l2s, bins=bins)\n",
    "    return bin_edges, bin_counts\n",
    "\n",
    "\n",
    "# Analysis Data Model\n",
    "\n",
    "class OFlowReconErrors(object):\n",
    "    \"\"\"Various measures of reconstruction error for a `FlowReconstructedImagePair` instance.\n",
    "    Encapsulated as two dictionaries of stats for easy interop with Spark SQL.\"\"\"\n",
    "\n",
    "    RECONSTRUCTION_ERR_METRICS = {\n",
    "        'SSIM': ssim,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'PSNR': psnr,\n",
    "        'Edges_MSE': edges_mse,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, recon_pair: FlowReconstructedImagePair):\n",
    "        im2 = recon_pair.opair.get_img2()\n",
    "        img2_recon_fwd = recon_pair.img2_recon_fwd\n",
    "        img2_recon_fwd_valid = recon_pair.img2_recon_fwd_valid\n",
    "        self.forward_stats = dict(\n",
    "            (name, func(im2, img2_recon_fwd, img2_recon_fwd_valid))\n",
    "            for name, func in self.RECONSTRUCTION_ERR_METRICS.items())\n",
    "        \n",
    "        im1 = recon_pair.opair.get_img1()\n",
    "        img1_recon_fwd = recon_pair.img1_recon_bkd\n",
    "        img1_recon_fwd_valid = recon_pair.img1_recon_bkd_valid\n",
    "        self.backward_stats = dict(\n",
    "            (name, func(im1, img1_recon_fwd, img1_recon_fwd_valid))\n",
    "            for name, func in self.RECONSTRUCTION_ERR_METRICS.items())\n",
    "\n",
    "    def to_html(self):\n",
    "        stat_names = self.RECONSTRUCTION_ERR_METRICS.keys()\n",
    "\n",
    "        rows = [\n",
    "            \"\"\"\n",
    "            <tr>\n",
    "              <td style=\"text-align:left\"><b>{name}</b></td>\n",
    "              <td style=\"text-align:left\">{fwd:.2f}</td>\n",
    "              <td style=\"text-align:left\">{bkd:.2f}</td>\n",
    "            </tr>\n",
    "            \"\"\".format(name=name, fwd=self.forward_stats[name], bkd=self.backward_stats[name])\n",
    "            for name in stat_names\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        html = \"\"\"\n",
    "            <table>\n",
    "              <tr>\n",
    "                  <th></th> <th><b>Forwards Warp</b></th> <th><b>Backwards Warp</b></th>\n",
    "              </tr>\n",
    "\n",
    "              {table_rows}\n",
    "\n",
    "            </table>\n",
    "        \"\"\".format(table_rows=\"\".join(rows))\n",
    "        \n",
    "        return html\n",
    "            \n",
    "# @attr.s(slots=True, eq=False, weakref_slot=False)\n",
    "class OFlowStats(object):\n",
    "    \"\"\"Stats on the optical flow of a `OpticalFlowPair` instance\"\"\"\n",
    "\n",
    "    slots = (\n",
    "        'opair',\n",
    "        'coverage',\n",
    "        'magnitude_hist',\n",
    "    )\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        for k in self.slots:\n",
    "            setattr(self, k, kwargs.get(k))\n",
    "    \n",
    "#     opair = attr.ib(default=OpticalFlowPair())\n",
    "#     \"\"\"The original `OpticalFlowPair` with the source of the data for this reconstruction result.\"\"\"\n",
    "    \n",
    "#     coverage = attr.ib(default=0)\n",
    "#     \"\"\"Fraction of the image with valid flow\"\"\"\n",
    "    \n",
    "#     magnitude_hist = attr.ib(default=[np.array([]), np.array([])])\n",
    "#     \"\"\"Histogram [bin edges, bin counts] of flow magnitudes\"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def create_from(cls, oflow_pair: OpticalFlowPair):\n",
    "        flow = oflow_pair.get_flow()\n",
    "        valid = ~zero_flow(flow)\n",
    "        return OFlowStats(\n",
    "                 opair=oflow_pair,\n",
    "                 coverage=oflow_coverage(valid),\n",
    "                 magnitude_hist=oflow_magnitude_hist(flow, valid))\n",
    "                 \n",
    "    def to_html(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig = plt.figure()\n",
    "        bin_edges, bin_counts = self.magnitude_hist\n",
    "        plt.bar(bin_edges[:-1], bin_counts)\n",
    "        plt.title(\"Histogram of Flow Magnitudes\")\n",
    "        plt.xlabel('Flow Magnitude (pixels)')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        hist_img = matplotlib_fig_to_img(fig)\n",
    "        \n",
    "        html = \"\"\"\n",
    "            <table>           \n",
    "            <tr><td style=\"text-align:left\"><b>Flow Coverage:</b> {coverage:.2f}% </td></tr>\n",
    "            <tr><td><img src=\"{flow_hist}\" width=\"100%\" /></td></tr>\n",
    "            </table>\n",
    "        \"\"\".format(\n",
    "                coverage=100. * self.coverage,\n",
    "                flow_hist=img_to_data_uri(matplotlib_fig_to_img(hist_img)))\n",
    "        return html\n",
    "\n",
    "\n",
    "# Misc\n",
    "\n",
    "def matplotlib_fig_to_img(fig):\n",
    "    import io\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    im = Image.open(buf)\n",
    "    im.show()\n",
    "    buf.seek(0)\n",
    "\n",
    "    import imageio\n",
    "    hist_img = imageio.imread(buf)\n",
    "    buf.close()\n",
    "    return hist_img\n",
    "\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    %matplotlib agg\n",
    "    for recon in DEMO_RECONS:\n",
    "        p = recon.opair\n",
    "        errors = OFlowReconErrors(recon)\n",
    "        err_html = errors.to_html()  \n",
    "            \n",
    "        fstats = OFlowStats.create_from(p)\n",
    "        stats_html = fstats.to_html()\n",
    "            \n",
    "        title = \"<b>{dataset} {id1} -> {id2}</b>\".format(dataset=p.dataset, id1=p.id1, id2=p.id2)\n",
    "        \n",
    "        show_html(title + stats_html + err_html + \"</br></br></br>\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene Flow Analysis (where depth and intrinsics are available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " * for psegs, we have uvd and K\n",
    " * for kitti tracking, we'll be able to have uvd and K\n",
    " * for deepdeform, the intrinsics are in each seq.  also a mask for maybe the images of interest?\n",
    " * for kitti sf, we can get K (P) from kitti-like file.  !!! kitti has obj_map colors image!!  \n",
    "     need to figure out depth meters from disparity ...  derrrp to get the raw velodynes we have to cross-ref\n",
    "     with odometry dataset. let's talk to yiyi about that...\n",
    " * !!! do a test where you use nearest neighbor correspondence on raw clouds for OFlow. then can see how bad\n",
    "     the pairing is sometimes\n",
    " \n",
    " * metrics: end-pt-error for NN forward; same for backward; then also do a chamfer distance metric\n",
    " * (do all this again but first do an ICP on the raw depths-- the rigid background should probably align, right?\n",
    "     use the ICP's RT to pose raw and \n",
    " * a common class for all these is background / foreground.  want to break down chamfer dist etc bucket by at least\n",
    "      background / foreground\n",
    " * debug image: surface pairs of points with end pt error larger than E and plot on the image\n",
    " \n",
    " * another good test: (1) train self-sup SF on raw clouds.  then test on large displacement pair \n",
    "     (walk a prediction forward many time steps). then can see how well that holds up vs our \"GT\"\n",
    " \n",
    "# From code above, which we won't run every time since\n",
    "# it's complicated and just gets static information.\n",
    "f, cx, cy, w, h = 1144.27150333,  960. ,  540., 1920, 1080\n",
    "K = np.array([\n",
    "      [f, 0, cx],\n",
    "      [0, f, cy],\n",
    "      [0, 0, 1],\n",
    "])\n",
    "\n",
    "px_y = np.tile(np.arange(h)[:, np.newaxis], [1, w])\n",
    "px_x = np.tile(np.arange(w)[np.newaxis, :], [h, 1])\n",
    "PYX = np.concatenate([px_y[:,:,np.newaxis], px_x[:, :,np.newaxis]], axis=-1)\n",
    "RAYS_FOR_CAM = np.zeros((h, w, 3))\n",
    "RAYS_FOR_CAM[:, :, 0] = (PYX[:, :, 0] - cy) / f\n",
    "RAYS_FOR_CAM[:, :, 1] = (PYX[:, :, 1] - cx) / f\n",
    "RAYS_FOR_CAM[:, :, 2] = 1\n",
    "\n",
    "yxz = RAYS_FOR_CAM * (demo[:, :, 3][:, :,np.newaxis])\n",
    "yxz = yxz.reshape([-1, 3])\n",
    "yxzrgb = np.concatenate([yxz, demo[:, :, :3].reshape([-1, 3])], axis=-1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def nn_distance(xyz_src, xyz_target):\n",
    "    import numpy as np\n",
    "    import open3d as o3d\n",
    "    pcds = o3d.geometry.PointCloud()\n",
    "    pcds.points = o3d.utility.Vector3dVector(xyz_src)\n",
    "    pcdt = o3d.geometry.PointCloud()\n",
    "    pcdt.points = o3d.utility.Vector3dVector(xyz_target)\n",
    "    dists = pcds.compute_point_cloud_distance(pcdt)\n",
    "    dists = np.asarray(dists)\n",
    "    return dists\n",
    "\n",
    "\n",
    "class SFlowStats(object):\n",
    "    \"\"\"Stats on the scene flow of a `OpticalFlowPair` instance (that has scene flow data)\"\"\"\n",
    "\n",
    "    slots = (\n",
    "        'fwd_nn_end_point_error',\n",
    "        'bkd_nn_end_point_error',\n",
    "        'chamfer_distance',\n",
    "        'fwd_epe_50th',\n",
    "        'fwd_epe_75th',\n",
    "        'fwd_epe_95th',\n",
    "#         'icp_fwd_nn_end_point_error',\n",
    "#         'icp_bkd_nn_end_point_error',\n",
    "#         'icp_chamfer_distance',\n",
    "        \n",
    "        'opair',\n",
    "        'coverage',\n",
    "        'magnitude_hist',\n",
    "    )\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        for k in self.slots:\n",
    "            setattr(self, k, kwargs.get(k))\n",
    "    \n",
    "#     opair = attr.ib(default=OpticalFlowPair())\n",
    "#     \"\"\"The original `OpticalFlowPair` with the source of the data for this reconstruction result.\"\"\"\n",
    "    \n",
    "#     coverage = attr.ib(default=0)\n",
    "#     \"\"\"Fraction of the image with valid flow\"\"\"\n",
    "    \n",
    "#     magnitude_hist = attr.ib(default=[np.array([]), np.array([])])\n",
    "#     \"\"\"Histogram [bin edges, bin counts] of flow magnitudes\"\"\"\n",
    "    \n",
    "#     @classmethod\n",
    "#     def create_from(cls, oflow_pair: OpticalFlowPair):\n",
    "#         flow = oflow_pair.get_flow()\n",
    "#         valid = ~zero_flow(flow)\n",
    "#         return OFlowStats(\n",
    "#                  opair=oflow_pair,\n",
    "#                  coverage=oflow_coverage(valid),\n",
    "#                  magnitude_hist=oflow_magnitude_hist(flow, valid))\n",
    "                 \n",
    "#     def to_html(self):\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         fig = plt.figure()\n",
    "#         bin_edges, bin_counts = self.magnitude_hist\n",
    "#         plt.bar(bin_edges[:-1], bin_counts)\n",
    "#         plt.title(\"Histogram of Flow Magnitudes\")\n",
    "#         plt.xlabel('Flow Magnitude (pixels)')\n",
    "#         plt.ylabel('Count')\n",
    "\n",
    "#         hist_img = matplotlib_fig_to_img(fig)\n",
    "        \n",
    "#         html = \"\"\"\n",
    "#             <table>           \n",
    "#             <tr><td style=\"text-align:left\"><b>Flow Coverage:</b> {coverage:.2f}% </td></tr>\n",
    "#             <tr><td><img src=\"{flow_hist}\" width=\"100%\" /></td></tr>\n",
    "#             </table>\n",
    "#         \"\"\".format(\n",
    "#                 coverage=100. * self.coverage,\n",
    "#                 flow_hist=img_to_data_uri(matplotlib_fig_to_img(hist_img)))\n",
    "#         return html\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on Full Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/opt/psegs')\n",
    "\n",
    "# from oarphpy.spark import NBSpark\n",
    "# NBSpark.SRC_ROOT = os.path.join(ALIB_SRC_DIR, 'cheap_optical_flow_eval_analysis')\n",
    "# NBSpark.CONF_KV.update({\n",
    "#     'spark.driver.maxResultSize': '2g',\n",
    "#     'spark.driver.memory': '16g',\n",
    "#   })\n",
    "# spark = NBSpark.getOrCreate()\n",
    "\n",
    "\n",
    "from oarphpy.spark import RowAdapter\n",
    "\n",
    "from pyspark import Row\n",
    "\n",
    "\n",
    "def flow_pair_to_full_row(fp):\n",
    "    from threadpoolctl import threadpool_limits\n",
    "    with threadpool_limits(limits=1, user_api='blas'):\n",
    "        recon = FlowReconstructedImagePair.create_from(fp)\n",
    "        fstats = OFlowStats.create_from(fp)\n",
    "        errors = OFlowReconErrors(recon)\n",
    "\n",
    "        rowdata = dict(\n",
    "                fp_datset=fp.dataset,\n",
    "                fp_uri=str(fp.uri),\n",
    "                flow_coverage=fstats.coverage,\n",
    "                diff_time_sec=fp.diff_time_sec,\n",
    "                translation_meters=fp.translation_meters,\n",
    "        )\n",
    "        rowdata.update(\n",
    "            ('Forwards_' + k, float(v))\n",
    "            for k, v in errors.forward_stats.items())\n",
    "        rowdata.update(\n",
    "            ('Backwards_' + k, float(v))\n",
    "            for k, v in errors.backward_stats.items())\n",
    "        return RowAdapter.to_row(rowdata)\n",
    "\n",
    "\n",
    "analysis_uris_demo = MiddFactory.list_fp_uris(spark) + PSEGS_SYNTHFLOW_DEMO_URIS + KITTI_SF15_DEMO_URIS + DD_DEMO_URIS\n",
    "\n",
    "\n",
    "class UnionFactory(FlowPairUnionFactory):\n",
    "    FACTORIES = ALL_FP_FACTORY_CLSS\n",
    "\n",
    "analysis_uris_full = UnionFactory.list_fp_uris(spark)\n",
    "# analysis_uris_full = analysis_uris_full[4900:]\n",
    "print('analysis_uris_full', len(analysis_uris_full))\n",
    "\n",
    "from oarphpy import util as oputil\n",
    "thru = oputil.ThruputObserver(name='run_analysis', n_total=len(analysis_uris_full))\n",
    "for uri_chunk in oputil.ichunked(analysis_uris_full, 100):\n",
    "    thru.start_block()\n",
    "    fp_rdd = UnionFactory.get_fp_rdd_for_uris(spark, uri_chunk)\n",
    "    result_rdd = fp_rdd.map(flow_pair_to_full_row)\n",
    "    df = spark.createDataFrame(result_rdd)\n",
    "    df.write.save(\n",
    "            mode='append',\n",
    "            path='/outer_root/media/rocket4q/oflow_pq_eval_test.parquet',\n",
    "            format='parquet',\n",
    "            compression='lz4')\n",
    "    thru.stop_block(n=len(uri_chunk))\n",
    "    thru.maybe_log_progress(every_n=1)\n",
    "\n",
    "\n",
    "# if True:#RUN_FULL_ANALYSIS:\n",
    "# #     spark = Spark.getOrCreate()\n",
    "    \n",
    "# #     for p in ALL_FPS:\n",
    "# #         import cloudpickle\n",
    "# #         try:\n",
    "# #             cloudpickle.dumps(p)\n",
    "# #         except Exception:\n",
    "# #             assert False, p\n",
    "# #     print('all good')\n",
    "    \n",
    "#     import pickle\n",
    "#     fp_rdd = spark.sparkContext.parallelize(ALL_FPS, numSlices=200)\n",
    "# #     print(fp_rdd.count())\n",
    "#     df = spark.createDataFrame(fp_rdd.map(flow_pair_to_full_row)).persist()\n",
    "\n",
    "#     print(df.count())\n",
    "#     df.show(10)\n",
    "#     df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pyspark-3.0.1-py3.8.egg/pyspark/sql/session.py:401: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+------------------+-------------------+--------------------+-----------+--------------------+\n",
      "|Backwards_Edges_MSE|     Backwards_MSE|    Backwards_PSNR|   Backwards_RMSE|     Backwards_SSIM|Forwards_Edges_MSE|      Forwards_MSE|     Forwards_PSNR|     Forwards_RMSE|      Forwards_SSIM|       flow_coverage| fp_dataset|              fp_uri|\n",
      "+-------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+------------------+-------------------+--------------------+-----------+--------------------+\n",
      "|  744.0916748046875| 57.34668296563634| 30.54572058271115|7.572759270281629| 0.5458482847253787|  998.046142578125| 55.85433777856281| 30.66023453780854|  7.47357596994657| 0.5581415463961072| 0.22368489583333334|deep_deform|psegs://dataset=d...|\n",
      "|    842.26904296875|63.144757188528715|30.127430632784414| 7.94636754678065|0.48511891470603075|1118.8370361328125|61.636287347680906|30.232438896952353| 7.850878125896549|0.47128979929899434| 0.22875651041666667|deep_deform|psegs://dataset=d...|\n",
      "| 1998.0679931640625| 78.73121872782315| 29.16933386668721|8.873061406742497| 0.3231775629814451|1954.3299560546875| 78.95831056880957| 29.15682513487142| 8.885848894101766|0.28861682466823346|      0.120791015625|deep_deform|psegs://dataset=d...|\n",
      "|  2672.272705078125| 81.31109833237493|29.029305334097423|9.017266677456918| 0.3259503059506655|1705.5645751953125| 82.84565656565657|28.948106167186534| 9.101958941110237| 0.3731756839238324| 0.09623372395833334|deep_deform|psegs://dataset=d...|\n",
      "|  2457.840087890625| 72.51869736336542|29.526303665059853|8.515791059165638| 0.3050414613909844| 1705.949462890625|  73.2788829525217|  29.4810152044902| 8.560308578113391|0.32643188059062733| 0.10448893229166667|deep_deform|psegs://dataset=d...|\n",
      "| 1100.0374755859375| 78.26172497818959|29.195309448492104|8.846565716603793| 0.3898784367426715| 1195.979248046875| 76.51568598143258|29.293298847096455| 8.747324504180268| 0.3670060030667816| 0.10820638020833333|deep_deform|psegs://dataset=d...|\n",
      "|   2242.25146484375| 89.38949809377824| 28.61793862070096|9.454601953217187| 0.2527629025698106|1949.9300537109375| 90.78591888175986|28.550618672606934| 9.528164507488306|0.25743406808197944|      0.082822265625|deep_deform|psegs://dataset=d...|\n",
      "|  358.2347106933594|39.531417055579126| 32.16137978389646|6.287401454939801| 0.6542302909755142| 265.1328430175781| 39.62400430570506| 32.15121998701308|6.2947600673659565| 0.6929602756926074| 0.15345052083333333|deep_deform|psegs://dataset=d...|\n",
      "|  625.0802612304688| 64.21114470927705|30.054699485983388| 8.01318567794838| 0.4876974674830181| 571.2742919921875|  64.0517526872667| 30.06549342759196| 8.003233889326658| 0.4775712232780704|         0.219609375|deep_deform|psegs://dataset=d...|\n",
      "|  3651.825439453125| 66.64405327291476|29.893189579954615|  8.1635809099264|0.42555037637833854|  1634.67626953125| 65.40354588991669| 29.97479066386478| 8.087245877918928|0.44361654484099555| 0.07471028645833333|deep_deform|psegs://dataset=d...|\n",
      "|   5508.53466796875|  81.8454998077063|29.000855557077358|9.046850269994874|0.32750328327189254| 1850.443115234375| 78.84953111679454|29.162812457520594| 8.879725846938888| 0.3327725628504689| 0.10439127604166666|deep_deform|psegs://dataset=d...|\n",
      "|   4119.54638671875| 70.78092296464271|29.631641394435277|8.413139899267259| 0.3378384969451392|    1869.873046875|  70.4279000698812|29.653356215265386|  8.39213322522237| 0.3881848431644795| 0.08325846354166666|deep_deform|psegs://dataset=d...|\n",
      "|        8923.046875| 73.85284450063212|29.447131337028434|8.593767770927494| 0.3410453199862943|  4494.50634765625| 75.29549902152642|29.363113449399606| 8.677297910151895| 0.3859862741582903|0.012874348958333334|deep_deform|psegs://dataset=d...|\n",
      "|    6910.7001953125| 87.82390794949814|28.694676026253738|9.371441081791964| 0.4084825770654837|  4515.19287109375| 87.06107495782116|28.732563357059853| 9.330652440093413| 0.4759995269357926|      0.031025390625|deep_deform|psegs://dataset=d...|\n",
      "|      10077.1328125| 78.98757399917942|   29.155215856287|8.887495372667116| 0.2934088736969065|   6089.6376953125| 80.78231673195299|29.057640568535046| 8.987898348999781|0.38160361941253457|0.018512369791666667|deep_deform|psegs://dataset=d...|\n",
      "|    4977.2666015625| 67.72279854528111|29.823454644779645|8.229386279989603|0.49910330827492944| 2811.965576171875| 66.49077127155581|29.903189902110295| 8.154187345870575| 0.5626879942561307| 0.07279947916666667|deep_deform|psegs://dataset=d...|\n",
      "|   4882.63330078125| 68.17577097147668|29.794503029705638|8.256862053557434| 0.4748969089921049| 2952.626220703125| 66.67877861398557|29.890927248118896| 8.165707477860419| 0.5404893002507919| 0.07216471354166666|deep_deform|psegs://dataset=d...|\n",
      "|  2415.248291015625|36.793887508054866|32.473046846404145|6.065796527089815| 0.7180408057622849|1540.7264404296875| 38.33584226080532| 32.29475351467864| 6.191594484525398| 0.7293513700231207|      0.082509765625|deep_deform|psegs://dataset=d...|\n",
      "|   2152.64111328125| 57.00251182279423| 30.57186367533243|7.550000782966465| 0.6040401075764357| 1050.544189453125| 53.48957987532736| 30.84811174050545| 7.313657079418433| 0.6292799804957274| 0.19525716145833333|deep_deform|psegs://dataset=d...|\n",
      "|  1902.520751953125| 74.92421376020697| 29.38458166626206|8.655877411343518| 0.5488770193177908|1257.5657958984375| 80.90038438498402| 29.05129775771325|  8.99446409659764|  0.551739892037525|      0.201318359375|deep_deform|psegs://dataset=d...|\n",
      "+-------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+------------------+-------------------+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19214"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = spark.read.parquet('/outer_root/media/rocket4q/oflow_pq_eval_test.parquet')\n",
    "\n",
    "\n",
    "def add_dataset(row):\n",
    "    from psegs import datum\n",
    "    row = row.asDict()\n",
    "    uri = datum.URI.from_str(row['fp_uri'])\n",
    "    row['fp_dataset'] = uri.dataset\n",
    "    return row\n",
    "\n",
    "results_df = spark.createDataFrame(results_df.rdd.map(add_dataset))\n",
    "results_df = results_df.persist()\n",
    "\n",
    "results_df.show()\n",
    "results_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of psegs.datasets.kitti_360 failed: Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/extensions/autoreload.py\", line 292, in update_class\n",
      "    if (old_obj == new_obj) is True:\n",
      "  File \"<attrs generated eq attr._make.Attribute>\", line 4, in __eq__\n",
      "    return  (\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering 11 histograms\n",
      "Working on Backwards_Edges_MSE\n",
      "547\n",
      "Working on Backwards_MSE\n",
      "938\n",
      "Working on Backwards_PSNR\n",
      "892\n",
      "Working on Backwards_RMSE\n",
      "932\n",
      "Working on Backwards_SSIM\n",
      "945\n",
      "Working on Forwards_Edges_MSE\n",
      "548\n",
      "Working on Forwards_MSE\n",
      "891\n",
      "Working on Forwards_PSNR\n",
      "892\n",
      "Working on Forwards_RMSE\n",
      "888\n",
      "Working on Forwards_SSIM\n",
      "867\n",
      "Working on flow_coverage\n",
      "792\n",
      "Rendering 2031 histogram bucket pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 02:08:31,794\tps   3501100 : Building DF for psegs://dataset=kitti-360&split=train&segment_id=2013_05_28_drive_0004_sync\n",
      "2021-04-15 02:08:31,795\tps   3501100 : Loading /opt/psegs/dataroot/stamped_datum/stamped_datums ...\n",
      "2021-04-15 02:08:31,901\tps   3501100 : Creating datums for KITTI-360 ...\n",
      "2021-04-15 02:08:31,901\tps   3501100 : Filtering to only 1 segments\n",
      "2021-04-15 02:08:35,435\tps   3501100 : ... seq 2013_05_28_drive_0004_sync has 99660 URIs spanning 1211 sec, creating 389 slices ...\n",
      "2021-04-15 02:08:35,966\tps   3501100 : ... partitioned datums into 1 RDDs.\n",
      "2021-04-15 02:08:36,004\tps   3501100 : Going to write in 1 chunks ...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 34.0 failed 1 times, most recent failure: Lost task 9.0 in stage 34.0 (TID 13400, 192.168.0.213, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/psegs/psegs/datasets/kitti_360.py\", line 449, in create_stamped_datum\n    if uri.topic.startswith('camera'):\nAttributeError: 'tuple' object has no attribute 'topic'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1388)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/psegs/psegs/datasets/kitti_360.py\", line 449, in create_stamped_datum\n    if uri.topic.startswith('camera'):\nAttributeError: 'tuple' object has no attribute 'topic'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1388)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9ff7e3554328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0manalysis_uris_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnionFactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_fp_uris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mfp_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnionFactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fp_rdd_for_uris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_fp_uris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrender_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mthreadpoolctl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreadpool_limits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/tmp4376nzva_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis/ofp.py\u001b[0m in \u001b[0;36mget_fp_rdd_for_uris\u001b[0;34m(cls, spark, uris)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mrdds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mF\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFACTORIES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fp_rdd_for_uris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mrdds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/tmp4376nzva_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis/ofp.py\u001b[0m in \u001b[0;36mget_fp_rdd_for_uris\u001b[0;34m(cls, spark, uris)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muris\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_fp_rdd_for_uris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/tmp4376nzva_cheap_optical_flow_eval_analysis/cheap_optical_flow_eval_analysis/psegs_synthflow.py\u001b[0m in \u001b[0;36m_get_fp_rdd_for_uris\u001b[0;34m(cls, spark, uris)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_frec_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mruris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pssf.ruri'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muri\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muris\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         rec_sample_rdd = T.get_records_with_samples_rdd(\n\u001b[0m\u001b[1;32m     99\u001b[0m                           \u001b[0mrecord_uris\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mruris\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                           \u001b[0minclude_cameras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/psegs/psegs/exp/fused_lidar_flow.py\u001b[0m in \u001b[0;36mget_records_with_samples_rdd\u001b[0;34m(self, record_uris, include_cameras, include_cuboids, include_point_clouds)\u001b[0m\n\u001b[1;32m   3868\u001b[0m       \u001b[0mkey_uri_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_uri_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'uri'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3869\u001b[0m       \u001b[0msd_ut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sd_ut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3870\u001b[0;31m       \u001b[0mkey_sample_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msd_ut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_keyed_sample_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_uri_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3872\u001b[0m       \u001b[0mjoined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_sample_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_sample_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/psegs/psegs/table/sd_db.py\u001b[0m in \u001b[0;36mget_keyed_sample_df\u001b[0;34m(self, df, key_col, uri_col, datum_col, spark)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     df[uri_col + '.segment_id']).distinct()\n\u001b[1;32m    319\u001b[0m     \u001b[0mseg_uris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuri_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_seg_uri_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     datum_df = self._get_union_df_for_segments(\n\u001b[0m\u001b[1;32m    321\u001b[0m                         seg_uris, ignore_unknown_tables=True, spark=spark)\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/psegs/psegs/table/sd_db.py\u001b[0m in \u001b[0;36m_get_union_df_for_segments\u001b[0;34m(self, suris, ignore_unknown_tables, spark)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mthru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_log_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuri\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msuri\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuris\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/psegs/psegs/table/sd_db.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mthru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_log_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuri\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msuri\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuris\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/psegs/psegs/table/sd_db.py\u001b[0m in \u001b[0;36mget_df\u001b[0;34m(suri)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_df_for_segment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mNoKnownTable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_unknown_tables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/psegs/psegs/table/sd_db.py\u001b[0m in \u001b[0;36m_get_df_for_segment\u001b[0;34m(self, suri, spark)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmatching_seg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m       \u001b[0mdatum_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_datum_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0;31m# Use the datum_df to get a fully-qualified segment_uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/psegs/psegs/table/sd_db.py\u001b[0m in \u001b[0;36m_build_datum_df\u001b[0;34m(self, uri, spark)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building DF for %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_dfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_segments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msuri\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m       \u001b[0mdatum_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_segment_datum_df_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/psegs/psegs/table/sd_table.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(cls, spark, only_segments)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sd_rdd_to_sd_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msd_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0mdf_thunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStampedDatumDFThunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd_rdd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msd_rdd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msd_rdds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m       Spark.save_df_thunks(\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mdf_thunks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/psegs/psegs/spark.py\u001b[0m in \u001b[0;36msave_df_thunks\u001b[0;34m(df_thunks, compute_df_sizes, **save_opts)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m#   return y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# num_bytes = df.rdd.map(getsize).sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moputil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size_of_deep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m       \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msave_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark-3.0.1-py3.8.egg/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \"\"\"\n\u001b[0;32m-> 1132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark-3.0.1-py3.8.egg/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark-3.0.1-py3.8.egg/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j-0.10.9-py3.8.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark-3.0.1-py3.8.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j-0.10.9-py3.8.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 34.0 failed 1 times, most recent failure: Lost task 9.0 in stage 34.0 (TID 13400, 192.168.0.213, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/psegs/psegs/datasets/kitti_360.py\", line 449, in create_stamped_datum\n    if uri.topic.startswith('camera'):\nAttributeError: 'tuple' object has no attribute 'topic'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1388)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/psegs/psegs/datasets/kitti_360.py\", line 449, in create_stamped_datum\n    if uri.topic.startswith('camera'):\nAttributeError: 'tuple' object has no attribute 'topic'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1388)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def fp_uri_to_fname(fp_uri):\n",
    "    fp_uri = str(fp_uri)\n",
    "    import urllib.parse\n",
    "    fname = urllib.parse.quote(fp_uri)\n",
    "    from slugify import slugify\n",
    "    fname = slugify(fname)\n",
    "    return fname\n",
    "\n",
    "def extract_fp_uris_from_html(html):\n",
    "    import re\n",
    "    matches = list(set(re.findall(r'alt=\\\\\"(.*?)\\\\\"', html)))\n",
    "    import html\n",
    "    return set(html.unescape(s) for s in matches)\n",
    "\n",
    "FLOW_EVAL_REPORT_BASEDIR = '/tmp/flow_eval/'\n",
    "from oarphpy import util as oputil\n",
    "oputil.mkdir(FLOW_EVAL_REPORT_BASEDIR)\n",
    "\n",
    "from oarphpy import plotting as pl\n",
    "class Plotter(pl.HistogramWithExamplesPlotter):\n",
    "    NUM_BINS = 50\n",
    "    ROWS_TO_DISPLAY_PER_BUCKET = 10\n",
    "    SUB_PIVOT_COL = 'fp_dataset'\n",
    "\n",
    "    def display_bucket(self, sub_pivot, bucket_id, irows):\n",
    "        from oarphpy.spark import RowAdapter\n",
    "        from psegs import datum\n",
    "        \n",
    "        # Sample from irows using reservior sampling\n",
    "        import random\n",
    "        rows = []\n",
    "        for i, row in enumerate(irows):\n",
    "            r = random.randint(0, i)\n",
    "            if r < cls.ROWS_TO_DISPLAY_PER_BUCKET:\n",
    "                if i < cls.ROWS_TO_DISPLAY_PER_BUCKET:\n",
    "                    rows.insert(r, row)\n",
    "                else:\n",
    "                    rows[r] = row\n",
    "        \n",
    "        # Now render each row to HTML\n",
    "        row_htmls = []\n",
    "        for row in rows:\n",
    "            rowdata = RowAdapter.from_row(row)\n",
    "            \n",
    "            fp_uri_str = rowdata['fp_uri']\n",
    "            fp_uri = datum.URI.from_str(fp_uri_str)\n",
    "            fp_page_uri = fp_uri_to_fname(fp_uri_str) + '.html'\n",
    "            dataset = fp_uri.dataset\n",
    "            id1 = \"TODO\"\n",
    "            id2 = \"TODO\"\n",
    "            \n",
    "            row_html = f\"\"\"\n",
    "                <a href=\"{fp_page_uri}\" alt=\"{fp_uri_str}\">\n",
    "                    {fp_uri.dataset} {fp_uri.split} {fp_uri.segment_id} {id1} -> {id2}\n",
    "                </a><br />\"\"\"\n",
    "            row_htmls.append(row_html)\n",
    "        \n",
    "        HTML = \"\"\"\n",
    "        <b>Pivot: {spv} Bucket: {bucket_id} </b> <br/>\n",
    "        \n",
    "        {row_bodies}\n",
    "        \"\"\".format(\n",
    "              spv=sub_pivot,\n",
    "              bucket_id=bucket_id,\n",
    "              row_bodies=\"<br/><br/><br/>\".join(row_htmls))\n",
    "        \n",
    "        return bucket_id, HTML\n",
    "\n",
    "plotter = Plotter()\n",
    "\n",
    "chosen_fp_uris = set()\n",
    "histogram_htmls = []\n",
    "cols = [col for col in results_df.columns if col not in ('fp_uri', 'fp_dataset')]\n",
    "print(\"Rendering %s histograms\" % len(cols))\n",
    "for col in cols:\n",
    "    print(\"Working on %s\" % col)\n",
    "#     fig = plotter.run(results_df, col)\n",
    "    dest = os.path.join(FLOW_EVAL_REPORT_BASEDIR, '%s.html' % col)\n",
    "#     pl.save_bokeh_fig(fig, dest)\n",
    "    \n",
    "    with open(dest, 'r') as f:\n",
    "        cur_chosen_fp_uris = extract_fp_uris_from_html(f.read())\n",
    "        print(len(cur_chosen_fp_uris))\n",
    "    chosen_fp_uris |= cur_chosen_fp_uris\n",
    "# assert False, len(chosen_fp_uris)\n",
    "    \n",
    "print(\"Rendering %s histogram bucket pages\" % len(chosen_fp_uris))\n",
    "class UnionFactory(FlowPairUnionFactory):\n",
    "    FACTORIES = ALL_FP_FACTORY_CLSS\n",
    "\n",
    "analysis_uris_full = UnionFactory.list_fp_uris(spark)\n",
    "\n",
    "fp_rdd = UnionFactory.get_fp_rdd_for_uris(spark, list(chosen_fp_uris))\n",
    "def render_and_save(fp):\n",
    "    from threadpoolctl import threadpool_limits\n",
    "    with threadpool_limits(limits=1, user_api='blas'):\n",
    "        import os\n",
    "        recon = FlowReconstructedImagePair.create_from(fp)\n",
    "        fstats = OFlowStats.create_from(fp)\n",
    "        errors = OFlowReconErrors(recon)\n",
    "        page_html = \"<br/>\".join((fp.to_html(), recon.to_html(), fstats.to_html(), errors.to_html()))\n",
    "\n",
    "        dest = os.path.join(FLOW_EVAL_REPORT_BASEDIR, fp_uri_to_fname(fp.uri) + '.html')\n",
    "        with open(dest, 'w') as f:\n",
    "            f.write(page_html)\n",
    "fp_rdd.foreach(render_and_save)\n",
    "    \n",
    "    \n",
    "# from bokeh.io import output_notebook\n",
    "# output_notebook()\n",
    "# from bokeh.plotting import show\n",
    "# show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
