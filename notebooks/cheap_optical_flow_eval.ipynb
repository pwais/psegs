{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheap Optical Flow: Is It Good?\n",
    "\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "## Credits\n",
    "\n",
    "Some portions of this notebook adapted from:\n",
    " * [Middlebury Flow code by Johannes Oswald](https://github.com/Johswald/flow-code-python/blob/master/readFlowFile.py)\n",
    " * [DeepDeform Demo Code](https://github.com/AljazBozic/DeepDeform)\n",
    " * [OpticalFlowToolkit by RUOTENG LI](https://github.com/liruoteng/OpticalFlowToolkit)\n",
    " * [OpenCV Samples](https://github.com/opencv/opencv/blob/master/samples/python/opt_flow.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "SHOW_DEMO_OUTPUT = True\n",
    "DEMO_FPS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Model & Utility Code\n",
    "\n",
    "!pip3 install pypng\n",
    "\n",
    "import attr\n",
    "import cv2\n",
    "import imageio\n",
    "import IPython.display\n",
    "import math\n",
    "import os\n",
    "import PIL.Image\n",
    "import six\n",
    "\n",
    "from oarphpy import plotting as op_plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@attr.s(slots=True, eq=False, weakref_slot=False)\n",
    "class OpticalFlowPair(object):\n",
    "    \"\"\"A flyweight for a pair of images with an optical flow field.\n",
    "    Supports lazy-loading of large data attributes.\"\"\"\n",
    "    \n",
    "    dataset = attr.ib(type=str, default='')\n",
    "    \"\"\"To which dataset does this pair belong?\"\"\"\n",
    "    \n",
    "    id1 = attr.ib(type=str, default='')\n",
    "    \"\"\"Identifier or URI for the first image\"\"\"\n",
    "    \n",
    "    id2 = attr.ib(type=str, default='')\n",
    "    \"\"\"Identifier or URI for the second image\"\"\"\n",
    "    \n",
    "    img1 = attr.ib(default=None)\n",
    "    \"\"\"URI or numpy array for the first image (source image)\"\"\"\n",
    "\n",
    "    img2 = attr.ib(default=None)\n",
    "    \"\"\"URI or numpy array for the second image (target image)\"\"\"\n",
    "    \n",
    "    flow = attr.ib(default=None)\n",
    "    \"\"\"A callable or numpy array representing optical flow from img1 -> img2\"\"\"\n",
    "    \n",
    "    def get_img1(self):\n",
    "        if isinstance(self.img1, six.string_types):\n",
    "            self.img1 = imageio.imread(self.img1)\n",
    "        return self.img1\n",
    "    \n",
    "    def get_img2(self):\n",
    "        if isinstance(self.img2, six.string_types):\n",
    "            self.img2 = imageio.imread(self.img2)\n",
    "        return self.img2\n",
    "    \n",
    "    def get_flow(self):\n",
    "        if not isinstance(self.flow, (np.ndarray, np.generic)):\n",
    "            self.flow = self.flow()\n",
    "        return self.flow\n",
    "    \n",
    "    def to_html(self):\n",
    "        im1 = self.get_img1()\n",
    "        im2 = self.get_img2()\n",
    "        flow = self.get_flow()\n",
    "        fviz = draw_flow(im1, flow)\n",
    "        html = \"\"\"\n",
    "            <table>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Dataset:</b> {dataset}</td></tr>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Source Image:</b> {id1}</td></tr>\n",
    "            <tr><td><img src=\"{im1}\" width=\"100%\" /></td></tr>\n",
    "\n",
    "            <tr><td style=\"text-align:left\"><b>Target Image:</b> {id2}</td></tr>\n",
    "            <tr><td><img src=\"{im2}\" width=\"100%\" /></td></tr>\n",
    "\n",
    "            <tr><td style=\"text-align:left\"><b>Flow</b></td></tr>\n",
    "            <tr><td><img src=\"{fviz}\" width=\"100%\" /></td></tr>\n",
    "            </table>\n",
    "        \"\"\".format(\n",
    "                dataset=self.dataset,\n",
    "                id1=self.id1, id2=self.id2,\n",
    "                im1=img_to_data_uri(im1), im2=img_to_data_uri(im2),\n",
    "                fviz=img_to_data_uri(fviz))\n",
    "        return html\n",
    "\n",
    "\n",
    "## General Utilities\n",
    "    \n",
    "def imshow(x):\n",
    "    IPython.display.display(PIL.Image.fromarray(x))\n",
    "\n",
    "def show_html(x):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(x))\n",
    "    \n",
    "def load_image(x):\n",
    "    if isinstance(x, (np.ndarray, np.generic)):\n",
    "        return x\n",
    "    if isinstance(x, six.string_types):\n",
    "        return imageio.imread(x)\n",
    "    else:\n",
    "        raise ValueError(\"Can't handle %s\" % (x,))\n",
    "\n",
    "img_to_data_uri = lambda x: op_plt.img_to_data_uri(x, format='png')\n",
    "# # TODO correct oarphpy img uri to be png or jpeg   data_url = 'data:image/png;base64,{}'.format(parse.quote(data)) \n",
    "# def img_to_data_uri(img, format='png', jpeg_quality=75):\n",
    "#     \"\"\"Given a numpy array `img`, return a `data:` URI suitable for use in \n",
    "#     an HTML image tag.\"\"\"\n",
    "\n",
    "#     from io import BytesIO\n",
    "#     out = BytesIO()\n",
    "\n",
    "#     import imageio\n",
    "#     kwargs = dict(format=format)\n",
    "#     if format == 'jpg':\n",
    "#         kwargs.update(quality=jpeg_quality)\n",
    "#     imageio.imwrite(out, img, **kwargs)\n",
    "\n",
    "#     from base64 import b64encode\n",
    "#     data = b64encode(out.getvalue()).decode('ascii')\n",
    "\n",
    "#     from six.moves.urllib import parse\n",
    "#     data_url = 'data:image/png;base64,{}'.format(parse.quote(data))\n",
    "\n",
    "#     return data_url\n",
    "\n",
    "\n",
    "def draw_flow(img, flow, step=8):\n",
    "    \"\"\"Based upon OpenCV sample: https://github.com/opencv/opencv/blob/master/samples/python/opt_flow.py\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n",
    "    fx, fy = flow[y,x].T\n",
    "    lines = np.vstack([x, y, x+fx, y+fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "    vis = img.copy()\n",
    "    cv2.polylines(vis, lines, 0, (0, 255, 0))\n",
    "    for (x1, y1), (_x2, _y2) in lines:\n",
    "        cv2.circle(vis, (x1, y1), 1, (0, 255, 0), -1)\n",
    "    return vis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Middlebury Optical Flow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Please unzip `other-color-allframes.zip` and `other-gt-flow.zip` to a directory and provide the target below:\n",
    "MIDD_DATA_ROOT = '/outer_root/host_mnt/Volumes/970-evo-raid0/middlebury-flow/'\n",
    "\n",
    "# For the Middlebury Flow dataset, we only consider the real scenes\n",
    "MIDD_SCENES = [\n",
    "    {\n",
    "        'input': 'other-data/Dimetrodon/frame10.png',\n",
    "        'expected_out': 'other-data/Dimetrodon/frame11.png',\n",
    "        'flow_gt': 'other-gt-flow/Dimetrodon/flow10.flo',\n",
    "    },\n",
    "        {\n",
    "        'input': 'other-data/Hydrangea/frame10.png',\n",
    "        'expected_out': 'other-data/Hydrangea/frame11.png',\n",
    "        'flow_gt': 'other-gt-flow/Hydrangea/flow10.flo',\n",
    "    },\n",
    "        {\n",
    "        'input': 'other-data/RubberWhale/frame10.png',\n",
    "        'expected_out': 'other-data/RubberWhale/frame11.png',\n",
    "        'flow_gt': 'other-gt-flow/RubberWhale/flow10.flo',\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def midd_read_flow(file):\n",
    "    # Based upon: https://github.com/Johswald/flow-code-python/blob/master/readFlowFile.py\n",
    "    # compute colored image to visualize optical flow file .flo\n",
    "    # Author: Johannes Oswald, Technical University Munich\n",
    "    # Contact: johannes.oswald@tum.de\n",
    "    # Date: 26/04/2017\n",
    "    # For more information, check http://vision.middlebury.edu/flow/ \n",
    "\n",
    "    assert type(file) is str, \"file is not str %r\" % str(file)\n",
    "    assert os.path.isfile(file) is True, \"file does not exist %r\" % str(file)\n",
    "    assert file[-4:] == '.flo', \"file ending is not .flo %r\" % file[-4:]\n",
    "    f = open(file, 'rb')\n",
    "    flo_number = np.fromfile(f, np.float32, count=1)[0]\n",
    "    TAG_FLOAT = 202021.25\n",
    "    assert flo_number == TAG_FLOAT, 'Flow number %r incorrect. Invalid .flo file' % flo_number\n",
    "    w = np.fromfile(f, np.int32, count=1)\n",
    "    h = np.fromfile(f, np.int32, count=1)\n",
    "\n",
    "    #if error try: data = np.fromfile(f, np.float32, count=2*w[0]*h[0])\n",
    "    data = np.fromfile(f, np.float32, count=int(2*w*h))\n",
    "    \n",
    "    # Reshape data into 3D array (columns, rows, bands)\n",
    "    flow = np.resize(data, (int(h), int(w), 2))\t\n",
    "    f.close()\n",
    "    \n",
    "    # We found that there are some invalid (?) (i.e. very large) flows, so we're going\n",
    "    # to ignore those for this experiment.\n",
    "    invalid = (flow >= 1666)\n",
    "    flow[invalid] = 0\n",
    "\n",
    "    return flow\n",
    "\n",
    "\n",
    "for i, scene in enumerate(MIDD_SCENES):\n",
    "    p = OpticalFlowPair(\n",
    "            dataset=\"Middlebury Optical Flow\",\n",
    "            id1=scene['input'],\n",
    "            img1='file://' + os.path.join(MIDD_DATA_ROOT, scene['input']),\n",
    "            id2=scene['expected_out'],\n",
    "            img2='file://' + os.path.join(MIDD_DATA_ROOT, scene['expected_out']),\n",
    "            flow=lambda: midd_read_flow(os.path.join(MIDD_DATA_ROOT, scene['flow_gt'])))\n",
    "    \n",
    "    if SHOW_DEMO_OUTPUT:\n",
    "        show_html(p.to_html() + \"<br/><br/><br/>\")\n",
    "        DEMO_FPS.append(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepDeform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Please extract deepdeform_v1.7z to a directory and provide the target below:\n",
    "DD_DATA_ROOT = '/outer_root/host_mnt/Volumes/970-evo-raid0/deepdeform_v1/'\n",
    "\n",
    "DD_DEMO_SCENES = [\n",
    "    {\n",
    "        \"input\": \"train/seq000/color/000000.jpg\",\n",
    "        \"expected_out\": \"train/seq000/color/000200.jpg\",\n",
    "        \"flow_gt\": \"train/seq000/optical_flow/blackdog_000000_000200.oflow\",\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"input\": \"train/seq000/color/000000.jpg\",\n",
    "        \"expected_out\": \"train/seq000/color/001200.jpg\",\n",
    "        \"flow_gt\": \"train/seq000/optical_flow/blackdog_000000_001200.oflow\",\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"input\": \"train/seq001/color/003400.jpg\",\n",
    "        \"expected_out\": \"train/seq001/color/003600.jpg\",\n",
    "        \"flow_gt\": \"train/seq001/optical_flow/lady_003400_003600.oflow\",\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"input\": \"train/seq337/color/000050.jpg\",\n",
    "        \"expected_out\": \"train/seq337/color/000350.jpg\",\n",
    "        \"flow_gt\": \"train/seq337/optical_flow/adult_000050_000350.oflow\",\n",
    "    },\n",
    "]\n",
    "\n",
    "def dd_load_flow(path):\n",
    "    # Based upon https://github.com/AljazBozic/DeepDeform/blob/master/utils.py#L1\n",
    "    import shutil\n",
    "    import struct\n",
    "    \n",
    "    # Flow is stored row-wise in order [channels, height, width].\n",
    "    assert os.path.isfile(path)\n",
    "\n",
    "    flow_gt = None\n",
    "    with open(path, 'rb') as fin:\n",
    "        width = struct.unpack('I', fin.read(4))[0]\n",
    "        height = struct.unpack('I', fin.read(4))[0]\n",
    "        channels = struct.unpack('I', fin.read(4))[0]\n",
    "        n_elems = height * width * channels\n",
    "\n",
    "        flow = struct.unpack('f' * n_elems, fin.read(n_elems * 4))\n",
    "        flow_gt = np.asarray(flow, dtype=np.float32).reshape([channels, height, width])\n",
    "    \n",
    "    # Match format used in this analysis\n",
    "    flow_gt = np.moveaxis(flow_gt, 0, -1) # (h, w, 2)\n",
    "    invalid_flow = flow_gt == -np.Inf\n",
    "    flow_gt[invalid_flow] = 0.0\n",
    "    return flow_gt\n",
    "\n",
    "def dd_create_fp(info):\n",
    "     return OpticalFlowPair(\n",
    "                dataset=\"DeepDeform Semi-Synthetic Optical Flow\",\n",
    "                id1=scene['input'],\n",
    "                img1='file://' + os.path.join(DD_DATA_ROOT, scene['input']),\n",
    "                id2=scene['expected_out'],\n",
    "                img2='file://' + os.path.join(DD_DATA_ROOT, scene['expected_out']),\n",
    "                flow=lambda: dd_load_flow(os.path.join(DD_DATA_ROOT, scene['flow_gt'])))\n",
    "\n",
    "import json\n",
    "DD_ALIGNMENTS = json.load(open(os.path.join(DD_DATA_ROOT, 'train_alignments.json')))\n",
    "ALL_DD_SCENES = [\n",
    "    {\n",
    "        \"input\": ascene['source_color'],\n",
    "        \"expected_out\": ascene['target_color'],\n",
    "        \"flow_gt\": ascene['optical_flow'],\n",
    "    }\n",
    "    for ascene in DD_ALIGNMENTS\n",
    "]\n",
    "\n",
    "print(\"Found %s DeepDeform scenes\" % len(ALL_DD_SCENES))\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    for scene in DD_DEMO_SCENES:\n",
    "        p = dd_create_fp(scene)\n",
    "        show_html(p.to_html())\n",
    "        DEMO_FPS.append(p)\n",
    "    \n",
    "# for i, scene in enumerate(ALL_DD_SCENES):\n",
    "    \n",
    "\n",
    "    \n",
    "#     img_in = imageio.imread(os.path.join(DD_DATA_ROOT, scene['input']))\n",
    "#     expected = imageio.imread(os.path.join(DD_DATA_ROOT, scene['expected_out']))\n",
    "#     flow_gt = dd_load_flow(os.path.join(DD_DATA_ROOT, scene['flow_gt']))\n",
    "    \n",
    "#     show_scene(img_in, expected, flow_gt, dataset='deepdeform', exname=str(i))\n",
    "#     continue\n",
    "    \n",
    "#     vis = draw_flow(img_in, flow_gt)\n",
    "#     imshow(vis)\n",
    "\n",
    "#     warped = warp_flow(img_in, flow_gt[:, :, :2])\n",
    "#     imshow(warped)\n",
    "    \n",
    "#     exclude = (flow_gt == np.array([0, 0])).all(axis=-1)\n",
    "#     warped[exclude] = np.array([0, 0, 0])\n",
    "#     imshow(warped)\n",
    "    \n",
    "#     imshow(expected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kitti Scene Flow Benchmark (2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Please unzip `data_scene_flow.zip` and `data_scene_flow_calib.zip` to a directory and provide that target below:\n",
    "KITTI_SF15_DATA_ROOT = '/outer_root/host_mnt/Volumes/970-evo-raid0/kitti_sceneflow_scratch/'\n",
    "\n",
    "# You have to ls flow_occ \n",
    "KITTI_SF15_DEMO_SCENES = [\n",
    "    {\n",
    "        'input': 'training/image_2/000000_10.png',\n",
    "        'expected_out': 'training/image_2/000000_11.png',\n",
    "        'flow_gt': 'training/flow_occ/000000_10.png',\n",
    "    },\n",
    "    {\n",
    "        'input': 'training/image_2/000007_10.png',\n",
    "        'expected_out': 'training/image_2/000007_11.png',\n",
    "        'flow_gt': 'training/flow_occ/000007_10.png',\n",
    "    },\n",
    "    {\n",
    "        'input': 'training/image_2/000023_10.png',\n",
    "        'expected_out': 'training/image_2/000023_11.png',\n",
    "        'flow_gt': 'training/flow_occ/000023_10.png',\n",
    "    },\n",
    "    {\n",
    "        'input': 'training/image_2/000051_10.png',\n",
    "        'expected_out': 'training/image_2/000051_11.png',\n",
    "        'flow_gt': 'training/flow_occ/000051_10.png',\n",
    "    },\n",
    "    {\n",
    "        'input': 'training/image_2/000003_10.png',\n",
    "        'expected_out': 'training/image_2/000003_11.png',\n",
    "        'flow_gt': 'training/flow_occ/000003_10.png',\n",
    "    },\n",
    "]\n",
    "\n",
    "from oarphpy import util as oputil\n",
    "KITTI_SF15_ALL_FLOW_OCC = [\n",
    "    os.path.basename(p)\n",
    "    for p in oputil.all_files_recursive(\n",
    "        os.path.join(KITTI_SF15_DATA_ROOT, 'training/flow_occ'), pattern='*.png')\n",
    "]\n",
    "    \n",
    "KITTI_SF15_ALL_SCENES = [\n",
    "    {\n",
    "        \"input\": 'training/image_2/%s' % fname,\n",
    "        \"expected_out\": 'training/image_2/%s' % fname.replace('_10', '_11'),\n",
    "        \"flow_gt\": 'training/flow_occ/%s' % fname,\n",
    "    }\n",
    "    for fname in KITTI_SF15_ALL_FLOW_OCC\n",
    "]\n",
    "print(\"Found %s KITTI SceneFlow 2015 scenes\" % len(KITTI_SF15_ALL_SCENES))\n",
    "\n",
    "\n",
    "def kitti_sf15_load_flow_from_png(path):\n",
    "    # Based upon https://github.com/liruoteng/OpticalFlowToolkit/blob/master/lib/flowlib.py#L559\n",
    "    import png\n",
    "    flow_object = png.Reader(filename=path)\n",
    "    flow_direct = flow_object.asDirect()\n",
    "    flow_data = list(flow_direct[2])\n",
    "    w, h = flow_direct[3]['size']\n",
    "    flow = np.zeros((h, w, 3), dtype=np.float64)\n",
    "    for i in range(len(flow_data)):\n",
    "        flow[i, :, 0] = flow_data[i][0::3]\n",
    "        flow[i, :, 1] = flow_data[i][1::3]\n",
    "        flow[i, :, 2] = flow_data[i][2::3]\n",
    "\n",
    "    invalid_idx = (flow[:, :, 2] == 0)\n",
    "    flow[:, :, 0:2] = (flow[:, :, 0:2] - 2 ** 15) / 64.0\n",
    "    flow[invalid_idx, 0] = 0\n",
    "    flow[invalid_idx, 1] = 0\n",
    "    return flow[:, :, :2]\n",
    "\n",
    "def kitti_sf15_create_fp(info):\n",
    "     return OpticalFlowPair(\n",
    "                dataset=\"KITTI Scene Flow 2015\",\n",
    "                id1=scene['input'],\n",
    "                img1='file://' + os.path.join(KITTI_SF15_DATA_ROOT, scene['input']),\n",
    "                id2=scene['expected_out'],\n",
    "                img2='file://' + os.path.join(KITTI_SF15_DATA_ROOT, scene['expected_out']),\n",
    "                flow=lambda: kitti_sf15_load_flow_from_png(os.path.join(KITTI_SF15_DATA_ROOT, scene['flow_gt'])))\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    for scene in KITTI_SF15_DEMO_SCENES:\n",
    "        p = kitti_sf15_create_fp(scene)\n",
    "        show_html(p.to_html())\n",
    "        DEMO_FPS.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction via Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Reconstruction via Optical Flow\n",
    "\n",
    "def zero_flow(flow):\n",
    "    return (flow[:, :, :2] == np.array([0, 0])).all(axis=-1)\n",
    "\n",
    "def warp_flow_backwards(img, flow):\n",
    "    \"\"\"Given an image, apply the inverse of `flow`\"\"\"\n",
    "    h, w = flow.shape[:2]\n",
    "    flow = -flow\n",
    "    flow[:,:,0] += np.arange(w)\n",
    "    flow[:,:,1] += np.arange(h)[:,np.newaxis]\n",
    "    res = cv2.remap(img, flow.astype(np.float32), None, cv2.INTER_LINEAR)\n",
    "    return res\n",
    "    \n",
    "def warp_flow_forwards(img, flow):\n",
    "    \"\"\"Given an image, apply the given optical flow `flow`.  Returns not only the warped\n",
    "    image, but a `mask` indicating warped pixels (i.e. there was non-zero flow *into* these pixels ).\n",
    "    With some help from https://stackoverflow.com/questions/41703210/inverting-a-real-valued-index-grid/46009462#46009462\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    pts = flow.copy()\n",
    "    pts[:, :, 0] += np.arange(w)\n",
    "    pts[:, :, 1] += np.arange(h)[:, np.newaxis]\n",
    "    exclude = zero_flow(flow)\n",
    "    if exclude.all():\n",
    "        # No flow anywhere!\n",
    "        return img.copy(), np.zeros((h, w)).astype(np.bool)\n",
    "    else:\n",
    "        inpts = pts[~exclude]\n",
    "    \n",
    "    from scipy.interpolate import griddata\n",
    "    inpts = np.reshape(inpts, [-1, 2])\n",
    "    grid_y, grid_x = np.mgrid[:h, :w]\n",
    "    chan_out = []\n",
    "    for ch in range(img.shape[-1]):\n",
    "        spts = img[:, :, ch][~exclude].reshape([-1, 1])\n",
    "        mapped = griddata(inpts, spts, (grid_x, grid_y), method='linear')\n",
    "        chan_out.append(mapped.astype(img.dtype))\n",
    "    out = np.stack(chan_out, axis=-1)\n",
    "    out = out.reshape([h, w, len(chan_out)])\n",
    "\n",
    "    mask = np.reshape(inpts, [-1, 2])\n",
    "    mask = np.rint(mask).astype(np.int)\n",
    "    mask = mask[np.where((mask[:, 0] >= 0) & (mask[:, 0] < w) & (mask[:, 1] >= 0) & (mask[:, 1] < h))]\n",
    "    valid_mask = np.zeros((h, w))\n",
    "    valid_mask[mask[:, 1], mask[:, 0]] = 1\n",
    "    \n",
    "    return out, valid_mask.astype(np.bool)\n",
    "\n",
    "@attr.s(slots=True, eq=False, weakref_slot=False)\n",
    "class FlowReconstructedImagePair(object):\n",
    "    \"\"\"A pair of reconstructed images using an input pair of images and optical\n",
    "    flow field (i.e. an `OpticalFlowPair` instance).\"\"\"\n",
    "\n",
    "    opair = attr.ib(default=OpticalFlowPair())\n",
    "    \"\"\"The original `OpticalFlowPair` with the source of the data for this reconstruction result.\"\"\"\n",
    "    \n",
    "    img2_recon_fwd = attr.ib(default=np.array([]))\n",
    "    \"\"\"A Numpy image containing the result of FORWARDS-WARPING OpticalFlowPair::img1\n",
    "    via OpticalFlowPair::flow to reconstruct OpticalFlowPair::img2\"\"\"\n",
    "\n",
    "    img2_recon_fwd_valid = attr.ib(default=np.array([]))\n",
    "    \"\"\"A Numpy boolean mask indicating which pixels of `img2_recon_fwd` were modified via non-zero flow\"\"\"\n",
    "    \n",
    "    img1_recon_bkd = attr.ib(default=np.array([]))\n",
    "    \"\"\"A Numpy image containing the result of BACKWARDS-WARPING OpticalFlowPair::img2\n",
    "    via OpticalFlowPair::flow to reconstruct OpticalFlowPair::img1\"\"\"\n",
    "\n",
    "    img1_recon_bkd_valid = attr.ib(default=np.array([]))\n",
    "    \"\"\"A Numpy boolean mask indicating which pixels of `img1_recon_bkd` were modified via non-zero flow\"\"\"\n",
    "        \n",
    "    @classmethod\n",
    "    def create_from(cls, oflow_pair: OpticalFlowPair):\n",
    "        flow = oflow_pair.get_flow()\n",
    "        \n",
    "        # Forward Warp\n",
    "        fwarped, fvalid = warp_flow_forwards(oflow_pair.get_img1(), flow)\n",
    "\n",
    "        # Backwards Warp\n",
    "        exclude = zero_flow(flow)\n",
    "        bwarped = warp_flow_backwards(oflow_pair.get_img2(), -flow[:, :, :2])\n",
    "        bvalid = ~exclude\n",
    "        \n",
    "        return FlowReconstructedImagePair(\n",
    "                opair=oflow_pair,\n",
    "                img2_recon_fwd=fwarped,\n",
    "                img2_recon_fwd_valid=fvalid,\n",
    "                img1_recon_bkd=bwarped,\n",
    "                img1_recon_bkd_valid=bvalid)\n",
    "        \n",
    "        vwarped = warped.copy()\n",
    "        if (~valid_mask).any():\n",
    "            vwarped[~valid_mask] = im2[~valid_mask]#np.array([255, 0, 0])\n",
    "        else:\n",
    "            vwarped = im2.copy()\n",
    "            print('!!!! no invalids')\n",
    "        imshow(vwarped)\n",
    "    \n",
    "    def to_html(self):\n",
    "        # We use pixels from the destination image in order to make the reconstruction \n",
    "        # easier to interpret; we'll fade them in intensity so that they are more\n",
    "        # conspicuous.        \n",
    "        FADE_UNTOUCHED_PIXELS = 0.3\n",
    "        \n",
    "        viz_fwd = self.img2_recon_fwd.copy().astype(np.float32)\n",
    "        im2 = self.opair.get_img2()\n",
    "        if (~self.img2_recon_fwd_valid).any():\n",
    "            viz_fwd[~self.img2_recon_fwd_valid] = im2[~self.img2_recon_fwd_valid]\n",
    "            viz_fwd[~self.img2_recon_fwd_valid] *= FADE_UNTOUCHED_PIXELS\n",
    "        else:\n",
    "            viz_fwd = im2.copy() * FADE_UNTOUCHED_PIXELS\n",
    "        \n",
    "        viz_bkd = self.img1_recon_bkd.copy().astype(np.float32)\n",
    "        im1 = self.opair.get_img1()\n",
    "        if (~self.img1_recon_bkd_valid).any():\n",
    "            viz_bkd[~self.img1_recon_bkd_valid] = im1[~self.img1_recon_bkd_valid]\n",
    "            viz_bkd[~self.img1_recon_bkd_valid] *= FADE_UNTOUCHED_PIXELS\n",
    "        else:\n",
    "            viz_bkd = im1.copy() * FADE_UNTOUCHED_PIXELS\n",
    "        \n",
    "        html = \"\"\"\n",
    "            <table>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Forwards Warped:</b></td></tr>\n",
    "            <tr><td><img src=\"{viz_fwd}\" width=\"100%\" /></td></tr>\n",
    "\n",
    "            <tr><td style=\"text-align:left\"><b>Backwards Warped:</b></td></tr>\n",
    "            <tr><td><img src=\"{viz_bkd}\" width=\"100%\" /></td></tr>\n",
    "\n",
    "            </table>\n",
    "        \"\"\".format(\n",
    "                viz_fwd=img_to_data_uri(viz_fwd.astype(np.uint8)),\n",
    "                viz_bkd=img_to_data_uri(viz_bkd.astype(np.uint8)))\n",
    "        return html\n",
    "\n",
    "        \n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    DEMO_RECONS = []\n",
    "    for p in DEMO_FPS:\n",
    "        recon = FlowReconstructedImagePair.create_from(p)\n",
    "        show_html(recon.to_html() + \"</br></br></br>\")\n",
    "        DEMO_RECONS.append(recon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Utils\n",
    "\n",
    "def mse(i1, i2, valid):\n",
    "    return np.mean((i1[valid] - i2[valid]) ** 2)\n",
    "\n",
    "def rmse(i1, i2, valid):\n",
    "    return math.sqrt(mse(i1, i2, valid))\n",
    "\n",
    "def psnr(i1, i2, valid):\n",
    "    return 20 * math.log10(255) - 10 * math.log10(max((mse(i1, i2, valid), 1e-12)))\n",
    "\n",
    "def ssim(i1, i2, valid):\n",
    "    # Some variance out there ...\n",
    "    # https://github.com/scikit-image/scikit-image/blob/master/skimage/metrics/_structural_similarity.py#L12-L232\n",
    "    # https://github.com/nianticlabs/monodepth2/blob/13200ab2f29f2f10dec3aa5db29c32a23e29d376/layers.py#L218\n",
    "    # https://cvnote.ddlee.cn/2019/09/12/psnr-ssim-python\n",
    "    # We will just use SKImage for now ...\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    mssim, S = ssim(i1, i2, win_size=11, multichannel=True, full=True)\n",
    "    return np.mean(S[valid])\n",
    "\n",
    "def to_edge_im(img):\n",
    "    return np.stack([\n",
    "        cv2.Laplacian(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), cv2.CV_32F, ksize=1),\n",
    "        cv2.Sobel(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), cv2.CV_32F, 1, 0, ksize=3),\n",
    "        cv2.Sobel(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), cv2.CV_32F, 0, 1, ksize=3),\n",
    "    ], axis=-1)\n",
    "\n",
    "def edges_mse(i1, i2, valid):\n",
    "    return mse(to_edge_im(i1), to_edge_im(i2), valid)\n",
    "\n",
    "\n",
    "def oflow_coverage(valid):\n",
    "    return valid.sum() / (valid.shape[0] * valid.shape[1])\n",
    "\n",
    "def oflow_magnitude_hist(flow, valid, bins=50):\n",
    "    flow_l2s = np.sqrt( flow[valid][:, 0] ** 2 + flow[valid][:, 1] ** 2 )\n",
    "    bin_edges = np.histogram_bin_edges(flow_l2s, bins=bins)\n",
    "    bin_counts = np.histogram(flow_l2s, bins=bin_edges)\n",
    "    return bin_edges, bin_counts\n",
    "\n",
    "\n",
    "# Analysis Data Model\n",
    "\n",
    "class OFlowReconErrors(object):\n",
    "    \"\"\"Various measures of reconstruction error for a `FlowReconstructedImagePair` instance.\n",
    "    Encapsulated as two dictionaries of stats for easy interop with Spark SQL.\"\"\"\n",
    "\n",
    "    RECONSTRUCTION_ERR_METRICS = {\n",
    "        'Mean Squared Error (MSE)': mse,\n",
    "        'Root-MSE': rmse,\n",
    "        'Peak Signal-to-Noise Ratio (PSNR)': psnr,\n",
    "        'Structured Similarity (SSIM)': ssim,\n",
    "        'Edges MSE (non-standard metric)': edges_mse,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, recon_pair: FlowReconstructedImagePair):\n",
    "        im2 = recon_pair.opair.get_img2()\n",
    "        img2_recon_fwd = recon_pair.img2_recon_fwd\n",
    "        img2_recon_fwd_valid = recon_pair.img2_recon_fwd_valid\n",
    "        self.forward_stats = dict(\n",
    "            (name, func(im2, img2_recon_fwd, img2_recon_fwd_valid))\n",
    "            for name, func in self.RECONSTRUCTION_ERR_METRICS.items())\n",
    "        \n",
    "        im1 = recon_pair.opair.get_img1()\n",
    "        img1_recon_fwd = recon_pair.img1_recon_bkd\n",
    "        img1_recon_fwd_valid = recon_pair.img1_recon_bkd_valid\n",
    "        self.backward_stats = dict(\n",
    "            (name, func(im1, img1_recon_fwd, img1_recon_fwd_valid))\n",
    "\n",
    "    def to_html(self):\n",
    "        stat_names = self.RECONSTRUCTION_ERR_METRICS.keys()\n",
    "\n",
    "        rows = [\n",
    "            \"\"\"\n",
    "            <tr>   <td><b>{name}</b></td>  <td>{fwd}</td>  <td>{bkd}</td>  </tr>\n",
    "            \"\"\".format(name=name, fwd=self.forward_stats[name], bkd=self.backward_stats[name])\n",
    "            for name in sorted(stat_names)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        html = \"\"\"\n",
    "            <table>\n",
    "              <tr>\n",
    "                  <th></th> <th><b>Forwards Warp</b></th> <th><b>Backwards Warp</b></th>\n",
    "              </tr>\n",
    "\n",
    "              {table_rows}\n",
    "\n",
    "            </table>\n",
    "        \"\"\".format(table_rows=\"\".join(rows))\n",
    "        \n",
    "        return html\n",
    "            \n",
    "@attr.s(slots=True, eq=False, weakref_slot=False)\n",
    "class OFlowStats(object):\n",
    "    \"\"\"Stats on the optical flow of a `OpticalFlowPair` instance\"\"\"\n",
    "\n",
    "    opair = attr.ib(default=OpticalFlowPair())\n",
    "    \"\"\"The original `OpticalFlowPair` with the source of the data for this reconstruction result.\"\"\"\n",
    "    \n",
    "    coverage = attr.ib(default=0)\n",
    "    \"\"\"Fraction of the image with valid flow\"\"\"\n",
    "    \n",
    "    magnitude_hist = attr.ib(default=[np.array([]), np.array([])])\n",
    "    \"\"\"Histogram [bin edges, bin counts] of flow magnitudes\"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def create_from(cls, oflow_pair: OpticalFlowPair):\n",
    "        flow = oflow_pair.get_flow()\n",
    "        valid = ~zero_flow(flow)\n",
    "        return OFlowStats(\n",
    "                 opair=oflow_pair,\n",
    "                 coverage=oflow_coverage(valid),\n",
    "                 magnitude_hist=oflow_magnitude_hist(flow, valid))\n",
    "                 \n",
    "    def to_html(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        flow_l2s = np.sqrt( flow[bvalid][:, 0] ** 2 + flow[bvalid][:, 1] ** 2 )\n",
    "        print(flow_l2s.shape)\n",
    "        fig = plt.figure()\n",
    "        bin_edges, bin_counts = self.magnitude_hist\n",
    "        plt.bar(bin_edges, bin_counts)\n",
    "        plt.title(\"Histogram of Flow Magnitudes\")\n",
    "        plt.xlabel('Flow Magnitude (pixels)')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        hist_img = matplotlib_fig_to_img(fig)\n",
    "        \n",
    "        html = \"\"\"\n",
    "            <table>           \n",
    "            <tr><td style=\"text-align:left\"><b>Flow Coverage:</b> {coverage} </td></tr>\n",
    "            <tr><td><img src=\"{flow_hist}\" width=\"100%\" /></td></tr>\n",
    "            </table>\n",
    "        \"\"\".format(\n",
    "                coverage=self.coverage,\n",
    "                flow_hist=img_to_data_uri(matplotlib_fig_to_img))\n",
    "        return html\n",
    "\n",
    "\n",
    "# Misc\n",
    "\n",
    "def matplotlib_fig_to_img(fig):\n",
    "    import io\n",
    "    from PIL import Image\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    im = Image.open(buf)\n",
    "    im.show()\n",
    "    buf.seek(0)\n",
    "\n",
    "    import imageio\n",
    "    hist_img = imageio.imread(buf)\n",
    "    buf.close()\n",
    "    return imageio.imread(hist_img)\n",
    "\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    for recon in DEMO_RECONS:\n",
    "        p = recon.oflow_pair\n",
    "        errors = OFlowReconError(recon)\n",
    "        err_html = errors.to_html()  \n",
    "            \n",
    "        fstats = OFlowStats.create_from(p)\n",
    "        stats_html = fstats.to_html()\n",
    "            \n",
    "        title = \"<b>{dataset} {id1} -> {id2}</b>\".format(dataset=p.dataset, id1=p.id1, id2=p.id2)\n",
    "        \n",
    "        show_html(title + stats_html + err_html + \"</br></br></br>\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/opt/psegs')\n",
    "\n",
    "from psegs.datasets import nuscenes as psnusc\n",
    "from psegs.spark import Spark\n",
    "spark = Spark.getOrCreate()\n",
    "\n",
    "from oarphpy import plotting as pl\n",
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([\n",
    "  Row(x=x, mod_11=int(x % 11), square=x*x)\n",
    "  for x in range(101)\n",
    "])\n",
    "\n",
    "\n",
    "### Check plotting with custom example plotter\n",
    "class PlotterWithMicroFacet(pl.HistogramWithExamplesPlotter):\n",
    "  NUM_BINS = 25\n",
    "\n",
    "  def display_bucket(self, sub_pivot, bucket_id, irows):\n",
    "    rows_str = \"<br />\".join(\n",
    "        \"x: {x} square: {square} mod_11: {mod_11}\".format(**row.asDict())\n",
    "        for row in sorted(irows, key=lambda r: r.x))\n",
    "    TEMPLATE = \"\"\"\n",
    "      <b>Pivot: {spv} Bucket: {bucket_id} </b> <br/>\n",
    "      {rows}\n",
    "      <br/> <br/>\n",
    "    \"\"\"\n",
    "    disp = TEMPLATE.format(\n",
    "              spv=sub_pivot,\n",
    "              bucket_id=bucket_id,\n",
    "              rows=rows_str)\n",
    "    return bucket_id, disp\n",
    "\n",
    "plotter = PlotterWithMicroFacet()\n",
    "fig = plotter.run(df, 'square')\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "from bokeh.plotting import show\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
