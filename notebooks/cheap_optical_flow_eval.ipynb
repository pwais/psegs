{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheap Optical Flow: Is It Good?\n",
    "\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "## Credits\n",
    "\n",
    "Some portions of this notebook adapted from:\n",
    " * [Middlebury Flow code by Johannes Oswald](https://github.com/Johswald/flow-code-python/blob/master/readFlowFile.py)\n",
    " * [DeepDeform Demo Code](https://github.com/AljazBozic/DeepDeform)\n",
    " * [OpticalFlowToolkit by RUOTENG LI](https://github.com/liruoteng/OpticalFlowToolkit)\n",
    " * [OpenCV Samples](https://github.com/opencv/opencv/blob/master/samples/python/opt_flow.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "SHOW_DEMO_OUTPUT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Model & Utility Code\n",
    "\n",
    "!pip3 install pypng\n",
    "\n",
    "import attr\n",
    "import cv2\n",
    "import imageio\n",
    "import IPython.display\n",
    "import os\n",
    "import PIL.Image\n",
    "import six\n",
    "\n",
    "from oarphpy import plotting as op_plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@attr.s(slots=True, eq=False, weakref_slot=False)\n",
    "class OpticalFlowPair(object):\n",
    "    \"\"\"A flyweight for a pair of images with an optical flow field\"\"\"\n",
    "    \n",
    "    dataset = attr.ib(type=str, default='')\n",
    "    \"\"\"To which dataset does this pair belong?\"\"\"\n",
    "    \n",
    "    id1 = attr.ib(type=str, default='')\n",
    "    \"\"\"Identifier or URI for the first image\"\"\"\n",
    "    \n",
    "    id2 = attr.ib(type=str, default='')\n",
    "    \"\"\"Identifier or URI for the second image\"\"\"\n",
    "    \n",
    "    img1 = attr.ib(default=None)\n",
    "    \"\"\"URI or numpy array for the first image (source image)\"\"\"\n",
    "\n",
    "    img2 = attr.ib(default=None)\n",
    "    \"\"\"URI or numpy array for the second image (target image)\"\"\"\n",
    "    \n",
    "    flow = attr.ib(default=None)\n",
    "    \"\"\"A callable or numpy array representing optical flow from img1 -> img2\"\"\"\n",
    "    \n",
    "    def to_html(self):\n",
    "        im1 = load_image(self.img1)\n",
    "        im2 = load_image(self.img2)\n",
    "        flow = self.flow if isinstance(self.flow, (np.ndarray, np.generic)) else self.flow()\n",
    "        fviz = draw_flow(im1, flow)\n",
    "        html = \"\"\"\n",
    "            <table>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Dataset:</b> {dataset}</td></tr>\n",
    "            \n",
    "            <tr><td style=\"text-align:left\"><b>Source Image:</b> {id1}</td></tr>\n",
    "            <tr><td><img src=\"{im1}\" width=\"100%\" /></td></tr>\n",
    "\n",
    "            <tr><td style=\"text-align:left\"><b>Target Image:</b> {id2}</td></tr>\n",
    "            <tr><td><img src=\"{im2}\" width=\"100%\" /></td></tr>\n",
    "\n",
    "            <tr><td style=\"text-align:left\"><b>Flow</b></td></tr>\n",
    "            <tr><td><img src=\"{fviz}\" width=\"100%\" /></td></tr>\n",
    "            </table>\n",
    "        \"\"\".format(\n",
    "                dataset=self.dataset,\n",
    "                id1=self.id1, id2=self.id2,\n",
    "                im1=img_to_data_uri(im1), im2=img_to_data_uri(im2),\n",
    "                fviz=img_to_data_uri(fviz))\n",
    "        return html\n",
    "\n",
    "\n",
    "## General Utilities\n",
    "    \n",
    "def imshow(x):\n",
    "    IPython.display.display(PIL.Image.fromarray(x))\n",
    "\n",
    "def show_html(x):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(x))\n",
    "    \n",
    "def load_image(x):\n",
    "    if isinstance(x, (np.ndarray, np.generic)):\n",
    "        return x\n",
    "    if isinstance(x, six.string_types):\n",
    "        return imageio.imread(x)\n",
    "    else:\n",
    "        raise ValueError(\"Can't handle %s\" % (x,))\n",
    "\n",
    "img_to_data_uri = lambda x: op_plt.img_to_data_uri(x, format='png')\n",
    "# # TODO correct oarphpy img uri to be png or jpeg   data_url = 'data:image/png;base64,{}'.format(parse.quote(data)) \n",
    "# def img_to_data_uri(img, format='png', jpeg_quality=75):\n",
    "#     \"\"\"Given a numpy array `img`, return a `data:` URI suitable for use in \n",
    "#     an HTML image tag.\"\"\"\n",
    "\n",
    "#     from io import BytesIO\n",
    "#     out = BytesIO()\n",
    "\n",
    "#     import imageio\n",
    "#     kwargs = dict(format=format)\n",
    "#     if format == 'jpg':\n",
    "#         kwargs.update(quality=jpeg_quality)\n",
    "#     imageio.imwrite(out, img, **kwargs)\n",
    "\n",
    "#     from base64 import b64encode\n",
    "#     data = b64encode(out.getvalue()).decode('ascii')\n",
    "\n",
    "#     from six.moves.urllib import parse\n",
    "#     data_url = 'data:image/png;base64,{}'.format(parse.quote(data))\n",
    "\n",
    "#     return data_url\n",
    "\n",
    "\n",
    "def draw_flow(img, flow, step=8):\n",
    "    \"\"\"Based upon OpenCV sample: https://github.com/opencv/opencv/blob/master/samples/python/opt_flow.py\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n",
    "    fx, fy = flow[y,x].T\n",
    "    lines = np.vstack([x, y, x+fx, y+fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "    vis = img.copy()\n",
    "    cv2.polylines(vis, lines, 0, (0, 255, 0))\n",
    "    for (x1, y1), (_x2, _y2) in lines:\n",
    "        cv2.circle(vis, (x1, y1), 1, (0, 255, 0), -1)\n",
    "    return vis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Middlebury Optical Flow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Please unzip `other-color-allframes.zip` and `other-gt-flow.zip` to a directory and provide the target below:\n",
    "MIDD_DATA_ROOT = '/outer_root/host_mnt/Volumes/970-evo-raid0/middlebury-flow/'\n",
    "\n",
    "# For the Middlebury Flow dataset, we only consider the real scenes\n",
    "MIDD_SCENES = [\n",
    "    {\n",
    "        'input': 'other-data/Dimetrodon/frame10.png',\n",
    "        'expected_out': 'other-data/Dimetrodon/frame11.png',\n",
    "        'flow_gt': 'other-gt-flow/Dimetrodon/flow10.flo',\n",
    "    },\n",
    "        {\n",
    "        'input': 'other-data/Hydrangea/frame10.png',\n",
    "        'expected_out': 'other-data/Hydrangea/frame11.png',\n",
    "        'flow_gt': 'other-gt-flow/Hydrangea/flow10.flo',\n",
    "    },\n",
    "        {\n",
    "        'input': 'other-data/RubberWhale/frame10.png',\n",
    "        'expected_out': 'other-data/RubberWhale/frame11.png',\n",
    "        'flow_gt': 'other-gt-flow/RubberWhale/flow10.flo',\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def midd_read_flow(file):\n",
    "    # Based upon: https://github.com/Johswald/flow-code-python/blob/master/readFlowFile.py\n",
    "    # compute colored image to visualize optical flow file .flo\n",
    "    # Author: Johannes Oswald, Technical University Munich\n",
    "    # Contact: johannes.oswald@tum.de\n",
    "    # Date: 26/04/2017\n",
    "    # For more information, check http://vision.middlebury.edu/flow/ \n",
    "\n",
    "    assert type(file) is str, \"file is not str %r\" % str(file)\n",
    "    assert os.path.isfile(file) is True, \"file does not exist %r\" % str(file)\n",
    "    assert file[-4:] == '.flo', \"file ending is not .flo %r\" % file[-4:]\n",
    "    f = open(file, 'rb')\n",
    "    flo_number = np.fromfile(f, np.float32, count=1)[0]\n",
    "    TAG_FLOAT = 202021.25\n",
    "    assert flo_number == TAG_FLOAT, 'Flow number %r incorrect. Invalid .flo file' % flo_number\n",
    "    w = np.fromfile(f, np.int32, count=1)\n",
    "    h = np.fromfile(f, np.int32, count=1)\n",
    "\n",
    "    #if error try: data = np.fromfile(f, np.float32, count=2*w[0]*h[0])\n",
    "    data = np.fromfile(f, np.float32, count=int(2*w*h))\n",
    "    \n",
    "    # Reshape data into 3D array (columns, rows, bands)\n",
    "    flow = np.resize(data, (int(h), int(w), 2))\t\n",
    "    f.close()\n",
    "    \n",
    "    # We found that there are some invalid (?) (i.e. very large) flows, so we're going\n",
    "    # to ignore those for this experiment.\n",
    "    invalid = (flow >= 1666)\n",
    "    flow[invalid] = 0\n",
    "\n",
    "    return flow\n",
    "\n",
    "\n",
    "for i, scene in enumerate(MIDD_SCENES):\n",
    "    p = OpticalFlowPair(\n",
    "            dataset=\"Middlebury Optical Flow\",\n",
    "            id1=scene['input'],\n",
    "            img1='file://' + os.path.join(MIDD_DATA_ROOT, scene['input']),\n",
    "            id2=scene['expected_out'],\n",
    "            img2='file://' + os.path.join(MIDD_DATA_ROOT, scene['expected_out']),\n",
    "            flow=lambda: midd_read_flow(os.path.join(MIDD_DATA_ROOT, scene['flow_gt'])))\n",
    "    \n",
    "    if SHOW_DEMO_OUTPUT:\n",
    "        show_html(p.to_html() + \"<br/><br/><br/>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepDeform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Please extract deepdeform_v1.7z to a directory and provide the target below:\n",
    "DD_DATA_ROOT = '/outer_root/host_mnt/Volumes/970-evo-raid0/deepdeform_v1/'\n",
    "\n",
    "DD_DEMO_SCENES = [\n",
    "    {\n",
    "        \"input\": \"train/seq000/color/000000.jpg\",\n",
    "        \"expected_out\": \"train/seq000/color/000200.jpg\",\n",
    "        \"flow_gt\": \"train/seq000/optical_flow/blackdog_000000_000200.oflow\",\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"input\": \"train/seq000/color/000000.jpg\",\n",
    "        \"expected_out\": \"train/seq000/color/001200.jpg\",\n",
    "        \"flow_gt\": \"train/seq000/optical_flow/blackdog_000000_001200.oflow\",\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"input\": \"train/seq001/color/003400.jpg\",\n",
    "        \"expected_out\": \"train/seq001/color/003600.jpg\",\n",
    "        \"flow_gt\": \"train/seq001/optical_flow/lady_003400_003600.oflow\",\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"input\": \"train/seq337/color/000050.jpg\",\n",
    "        \"expected_out\": \"train/seq337/color/000350.jpg\",\n",
    "        \"flow_gt\": \"train/seq337/optical_flow/adult_000050_000350.oflow\",\n",
    "    },\n",
    "]\n",
    "\n",
    "def dd_load_flow(path):\n",
    "    # Based upon https://github.com/AljazBozic/DeepDeform/blob/master/utils.py#L1\n",
    "    import shutil\n",
    "    import struct\n",
    "    \n",
    "    # Flow is stored row-wise in order [channels, height, width].\n",
    "    assert os.path.isfile(path)\n",
    "\n",
    "    flow_gt = None\n",
    "    with open(path, 'rb') as fin:\n",
    "        width = struct.unpack('I', fin.read(4))[0]\n",
    "        height = struct.unpack('I', fin.read(4))[0]\n",
    "        channels = struct.unpack('I', fin.read(4))[0]\n",
    "        n_elems = height * width * channels\n",
    "\n",
    "        flow = struct.unpack('f' * n_elems, fin.read(n_elems * 4))\n",
    "        flow_gt = np.asarray(flow, dtype=np.float32).reshape([channels, height, width])\n",
    "    \n",
    "    # Match format used in this analysis\n",
    "    flow_gt = np.moveaxis(flow_gt, 0, -1) # (h, w, 2)\n",
    "    invalid_flow = flow_gt == -np.Inf\n",
    "    flow_gt[invalid_flow] = 0.0\n",
    "    return flow_gt\n",
    "\n",
    "def dd_create_fp(info):\n",
    "     return OpticalFlowPair(\n",
    "                dataset=\"DeepDeform Semi-Synthetic Optical Flow\",\n",
    "                id1=scene['input'],\n",
    "                img1='file://' + os.path.join(DD_DATA_ROOT, scene['input']),\n",
    "                id2=scene['expected_out'],\n",
    "                img2='file://' + os.path.join(DD_DATA_ROOT, scene['expected_out']),\n",
    "                flow=lambda: dd_load_flow(os.path.join(DD_DATA_ROOT, scene['flow_gt'])))\n",
    "\n",
    "import json\n",
    "DD_ALIGNMENTS = json.load(open(os.path.join(DD_DATA_ROOT, 'train_alignments.json')))\n",
    "ALL_DD_SCENES = [\n",
    "    {\n",
    "        \"input\": ascene['source_color'],\n",
    "        \"expected_out\": ascene['target_color'],\n",
    "        \"flow_gt\": ascene['optical_flow'],\n",
    "    }\n",
    "    for ascene in DD_ALIGNMENTS\n",
    "]\n",
    "\n",
    "print(\"Found %s DeepDeform scenes\" % len(ALL_DD_SCENES))\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    for scene in DD_DEMO_SCENES:\n",
    "        p = dd_create_fp(scene)\n",
    "        show_html(p.to_html())\n",
    "    \n",
    "# for i, scene in enumerate(ALL_DD_SCENES):\n",
    "    \n",
    "\n",
    "    \n",
    "#     img_in = imageio.imread(os.path.join(DD_DATA_ROOT, scene['input']))\n",
    "#     expected = imageio.imread(os.path.join(DD_DATA_ROOT, scene['expected_out']))\n",
    "#     flow_gt = dd_load_flow(os.path.join(DD_DATA_ROOT, scene['flow_gt']))\n",
    "    \n",
    "#     show_scene(img_in, expected, flow_gt, dataset='deepdeform', exname=str(i))\n",
    "#     continue\n",
    "    \n",
    "#     vis = draw_flow(img_in, flow_gt)\n",
    "#     imshow(vis)\n",
    "\n",
    "#     warped = warp_flow(img_in, flow_gt[:, :, :2])\n",
    "#     imshow(warped)\n",
    "    \n",
    "#     exclude = (flow_gt == np.array([0, 0])).all(axis=-1)\n",
    "#     warped[exclude] = np.array([0, 0, 0])\n",
    "#     imshow(warped)\n",
    "    \n",
    "#     imshow(expected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kitti Scene Flow Benchmark (2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Please unzip `data_scene_flow.zip` and `data_scene_flow_calib.zip` to a directory and provide that target below:\n",
    "KITTI_SF15_DATA_ROOT = '/outer_root/host_mnt/Volumes/970-evo-raid0/kitti_sceneflow_scratch/'\n",
    "\n",
    "# You have to ls flow_occ \n",
    "KITTI_SF15_DEMO_SCENES = [\n",
    "    {\n",
    "        'input': 'training/image_2/000000_10.png',\n",
    "        'expected_out': 'training/image_2/000000_11.png',\n",
    "        'flow_gt': 'training/flow_occ/000000_10.png',\n",
    "    },\n",
    "    {\n",
    "        'input': 'training/image_2/000007_10.png',\n",
    "        'expected_out': 'training/image_2/000007_11.png',\n",
    "        'flow_gt': 'training/flow_occ/000007_10.png',\n",
    "    },\n",
    "    {\n",
    "        'input': 'training/image_2/000023_10.png',\n",
    "        'expected_out': 'training/image_2/000023_11.png',\n",
    "        'flow_gt': 'training/flow_occ/000023_10.png',\n",
    "    },\n",
    "    {\n",
    "        'input': 'training/image_2/000051_10.png',\n",
    "        'expected_out': 'training/image_2/000051_11.png',\n",
    "        'flow_gt': 'training/flow_occ/000051_10.png',\n",
    "    },\n",
    "    {\n",
    "        'input': 'training/image_2/000003_10.png',\n",
    "        'expected_out': 'training/image_2/000003_11.png',\n",
    "        'flow_gt': 'training/flow_occ/000003_10.png',\n",
    "    },\n",
    "]\n",
    "\n",
    "from oarphpy import util as oputil\n",
    "KITTI_SF15_ALL_FLOW_OCC = [\n",
    "    os.path.basename(p)\n",
    "    for p in oputil.all_files_recursive(\n",
    "        os.path.join(KITTI_SF15_DATA_ROOT, 'training/flow_occ'), pattern='*.png')\n",
    "]\n",
    "    \n",
    "KITTI_SF15_ALL_SCENES = [\n",
    "    {\n",
    "        \"input\": 'training/image_2/%s' % fname,\n",
    "        \"expected_out\": 'training/image_2/%s' % fname.replace('_10', '_11'),\n",
    "        \"flow_gt\": 'training/flow_occ/%s' % fname,\n",
    "    }\n",
    "    for fname in KITTI_SF15_ALL_FLOW_OCC\n",
    "]\n",
    "print(\"Found %s KITTI SceneFlow 2015 scenes\" % len(KITTI_SF15_ALL_SCENES))\n",
    "\n",
    "\n",
    "def kitti_sf15_load_flow_from_png(path):\n",
    "    # Based upon https://github.com/liruoteng/OpticalFlowToolkit/blob/master/lib/flowlib.py#L559\n",
    "    import png\n",
    "    flow_object = png.Reader(filename=path)\n",
    "    flow_direct = flow_object.asDirect()\n",
    "    flow_data = list(flow_direct[2])\n",
    "    w, h = flow_direct[3]['size']\n",
    "    flow = np.zeros((h, w, 3), dtype=np.float64)\n",
    "    for i in range(len(flow_data)):\n",
    "        flow[i, :, 0] = flow_data[i][0::3]\n",
    "        flow[i, :, 1] = flow_data[i][1::3]\n",
    "        flow[i, :, 2] = flow_data[i][2::3]\n",
    "\n",
    "    invalid_idx = (flow[:, :, 2] == 0)\n",
    "    flow[:, :, 0:2] = (flow[:, :, 0:2] - 2 ** 15) / 64.0\n",
    "    flow[invalid_idx, 0] = 0\n",
    "    flow[invalid_idx, 1] = 0\n",
    "    return flow[:, :, :2]\n",
    "\n",
    "def kitti_sf15_create_fp(info):\n",
    "     return OpticalFlowPair(\n",
    "                dataset=\"KITTI Scene Flow 2015\",\n",
    "                id1=scene['input'],\n",
    "                img1='file://' + os.path.join(KITTI_SF15_DATA_ROOT, scene['input']),\n",
    "                id2=scene['expected_out'],\n",
    "                img2='file://' + os.path.join(KITTI_SF15_DATA_ROOT, scene['expected_out']),\n",
    "                flow=lambda: kitti_sf15_load_flow_from_png(os.path.join(KITTI_SF15_DATA_ROOT, scene['flow_gt'])))\n",
    "\n",
    "if SHOW_DEMO_OUTPUT:\n",
    "    for scene in KITTI_SF15_DEMO_SCENES:\n",
    "        p = kitti_sf15_create_fp(scene)\n",
    "        show_html(p.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
