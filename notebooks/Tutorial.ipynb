{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * trailer: \n",
    "     * show a histogram with examples of distance / orientation with samples over ALL datasets\n",
    "     * show perf!  show time to fetch frames using Spark + Parquet\n",
    "     * show a video of one camera with debug overlays.  maybe one with delauny lidar too (!)\n",
    "     * show a frame HTML with 3d interface\n",
    "     * show new things: argo associated bikes, delauny lidar, occlusion tree\n",
    " * supported datasets, how to get a blurb and **stats** on each of them.  prolly render histo reports for each.\n",
    " * data structures:\n",
    "    * StampedDatum\n",
    "    * Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/opt/psegs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.datasets import nuscenes\n",
    "\n",
    "nuscenes.NuscStampedDatumTableLabelsAllFrames.build()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from psegs.datasets import kitti\n",
    "\n",
    "# kitti.DSUtil.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from psegs.datasets import kitti_360\n",
    "\n",
    "kitti_360.KITTI360SDTable.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/psegs')\n",
    "from oarphpy.spark import NBSpark\n",
    "NBSpark.SRC_ROOT = '/opt/psegs'\n",
    "NBSpark.SRC_ROOT_MODULES = ['psegs']\n",
    "NBSpark.MAYBE_REBUILD_EGG_EVERY_CELL_RUN = False\n",
    "NBSpark.CONF_KV = {\n",
    "    'spark.driver.memory': '8g',\n",
    "    'spark.pyspark.python': 'python3',\n",
    "    'spark.python.worker.reuse': False,\n",
    "    'spark.sql.files.maxPartitionBytes': int(8 * 1e6),\n",
    "    'spark.port.maxRetries': '256',\n",
    "  }\n",
    "from psegs.datasets import kitti\n",
    "\n",
    "spark = NBSpark.getOrCreate()\n",
    "df = kitti.KITTISDTable.as_df(spark)\n",
    "# df = spark.read.parquet('/tmp/avs_test/test_kitti_sd_table_tracking/sd_table/')\n",
    "df.show(5)\n",
    "# print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg = df.filter('uri.segment_id = \"kitti-tracking-train-0009\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_uri = seg.select('uri').persist()\n",
    "df.createOrReplaceTempView('data')\n",
    "# seg_uri.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      uri.segment_id AS seg, \n",
    "      uri.topic AS topic,\n",
    "      count(*) AS N,\n",
    "      MAX(uri.timestamp),\n",
    "      MIN(uri.timestamp),\n",
    "      (MAX(uri.timestamp) - MIN(uri.timestamp)) * 1e-9 AS len\n",
    "    FROM data\n",
    "    GROUP BY topic, seg\n",
    "    ORDER BY len ASC\n",
    "\"\"\").show(1000, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "init_notebook_mode(connected=False)         # initiate notebook for offline plot\n",
    "\n",
    "trace0 = Scatter(\n",
    "  x=[1, 2, 3, 4],\n",
    "  y=[10, 15, 13, 17]\n",
    ")\n",
    "trace1 = Scatter(\n",
    "  x=[1, 2, 3, 4],\n",
    "  y=[16, 5, 11, 9]\n",
    ")\n",
    "\n",
    "iplot([trace0, trace1])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/opt/psegs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psegs\n",
    "from psegs.datasets import ios_lidar\n",
    "from psegs import datum\n",
    "\n",
    "from oarphpy.spark import NBSpark\n",
    "# NBSpark.SRC_ROOT = '/opt/psegs'\n",
    "NBSpark.SRC_ROOT_MODULES = ['psegs']\n",
    "NBSpark.MAYBE_REBUILD_EGG_EVERY_CELL_RUN = True\n",
    "NBSpark.CONF_KV = {\n",
    "    'spark.driver.memory': '8g',\n",
    "    'spark.pyspark.python': 'python3',\n",
    "    'spark.python.worker.reuse': False,\n",
    "    'spark.sql.files.maxPartitionBytes': int(8 * 1e6),\n",
    "    'spark.port.maxRetries': '256',\n",
    "  }\n",
    "\n",
    "\n",
    "spark = NBSpark.getOrCreate()\n",
    "\n",
    "\n",
    "suri = datum.URI.from_str(\n",
    "      'psegs://dataset=psegs-ios-lidar-ext&split=threeDScannerApp_data&segment_id=charuco-test-fixture-lowres')\n",
    "sd_df = ios_lidar.IOSLidarSDTable.as_df(spark, force_compute=True, only_segments=[suri])\n",
    "\n",
    "sd_df.createTempView('sd_df')\n",
    "sd_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      topic,\n",
    "      n,\n",
    "      (n / (1e-9 * duration_ns)) AS Hz,\n",
    "      1e-9 * duration_ns AS duration_sec,\n",
    "      width,\n",
    "      height,\n",
    "      channel_names,\n",
    "      uncompressed_MBytes,\n",
    "      uncompressed_MBytes / (1e-9 * duration_ns) AS uncompressed_MBps,\n",
    "      FROM_UNIXTIME(start * 1e-9) AS start,\n",
    "      FROM_UNIXTIME(end * 1e-9) AS end\n",
    "\n",
    "    FROM \n",
    "        (\n",
    "            SELECT\n",
    "                FIRST(uri.topic) AS topic,\n",
    "                MIN(uri.timestamp) AS start,\n",
    "                MAX(uri.timestamp) AS end,\n",
    "                MAX(uri.timestamp) - MIN(uri.timestamp) AS duration_ns,\n",
    "                FIRST(camera_image.width) AS width,\n",
    "                FIRST(camera_image.height) AS height,\n",
    "                FIRST(camera_image.channel_names) AS channel_names,\n",
    "                1e-6 * FIRST(camera_image.width * camera_image.height * 3) AS uncompressed_MBytes,\n",
    "                COUNT(*) AS n\n",
    "            FROM sd_df\n",
    "            WHERE camera_image IS NOT NULL\n",
    "            GROUP BY uri.topic\n",
    "        )\n",
    "    ORDER BY topic\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      topic,\n",
    "      n,\n",
    "      (n / (1e-9 * duration_ns)) AS Hz,\n",
    "      1e-9 * duration_ns AS duration_sec,\n",
    "      cloud_colnames,\n",
    "      FROM_UNIXTIME(start * 1e-9) AS start,\n",
    "      FROM_UNIXTIME(end * 1e-9) AS end\n",
    "\n",
    "    FROM \n",
    "        (\n",
    "            SELECT\n",
    "                FIRST(uri.topic) AS topic,\n",
    "                MIN(uri.timestamp) AS start,\n",
    "                MAX(uri.timestamp) AS end,\n",
    "                MAX(uri.timestamp) - MIN(uri.timestamp) AS duration_ns,\n",
    "                FIRST(point_cloud.cloud_colnames) AS cloud_colnames,\n",
    "                COUNT(*) AS n\n",
    "            FROM sd_df\n",
    "            WHERE point_cloud IS NOT NULL\n",
    "            GROUP BY uri.topic\n",
    "        )\n",
    "    ORDER BY topic\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      topic,\n",
    "      n,\n",
    "      (n / (1e-9 * duration_ns)) AS Hz,\n",
    "      1e-9 * duration_ns AS duration_sec,\n",
    "      xform,\n",
    "      FROM_UNIXTIME(start * 1e-9) AS start,\n",
    "      FROM_UNIXTIME(end * 1e-9) AS end\n",
    "\n",
    "    FROM \n",
    "        (\n",
    "            SELECT\n",
    "                FIRST(uri.topic) AS topic,\n",
    "                MIN(uri.timestamp) AS start,\n",
    "                MAX(uri.timestamp) AS end,\n",
    "                MAX(uri.timestamp) - MIN(uri.timestamp) AS duration_ns,\n",
    "                FIRST(CONCAT(transform.src_frame, '->', transform.dest_frame)) AS xform,\n",
    "                COUNT(*) AS n\n",
    "            FROM sd_df\n",
    "            WHERE transform IS NOT NULL\n",
    "            GROUP BY uri.topic\n",
    "        )\n",
    "    ORDER BY topic\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      topic,\n",
    "      n,\n",
    "      (n / (1e-9 * duration_ns)) AS Hz,\n",
    "      1e-9 * duration_ns AS duration_sec,\n",
    "      FROM_UNIXTIME(start * 1e-9) AS start,\n",
    "      FROM_UNIXTIME(end * 1e-9) AS end\n",
    "\n",
    "    FROM \n",
    "        (\n",
    "            SELECT\n",
    "                FIRST(uri.topic) AS topic,\n",
    "                MIN(uri.timestamp) AS start,\n",
    "                MAX(uri.timestamp) AS end,\n",
    "                MAX(uri.timestamp) - MIN(uri.timestamp) AS duration_ns,\n",
    "                COUNT(*) AS n\n",
    "            FROM sd_df\n",
    "            WHERE SIZE(cuboids) > 0\n",
    "            GROUP BY uri.topic\n",
    "        )\n",
    "    ORDER BY topic\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/opt/psegs')\n",
    "\n",
    "import psegs\n",
    "from psegs.datasets import ios_lidar\n",
    "from psegs import datum\n",
    "\n",
    "from oarphpy.spark import NBSpark\n",
    "# NBSpark.SRC_ROOT = '/opt/psegs'\n",
    "NBSpark.SRC_ROOT_MODULES = ['psegs']\n",
    "NBSpark.MAYBE_REBUILD_EGG_EVERY_CELL_RUN = True\n",
    "NBSpark.CONF_KV = {\n",
    "    'spark.driver.memory': '8g',\n",
    "    'spark.pyspark.python': 'python3',\n",
    "    'spark.python.worker.reuse': False,\n",
    "    'spark.sql.files.maxPartitionBytes': int(8 * 1e6),\n",
    "    'spark.port.maxRetries': '256',\n",
    "  }\n",
    "\n",
    "\n",
    "spark = NBSpark.getOrCreate()\n",
    "\n",
    "\n",
    "suri = datum.URI.from_str(\n",
    "      'psegs://dataset=psegs-ios-lidar-ext&split=threeDScannerApp_data&segment_id=charuco-test-fixture-lowres')\n",
    "sd_df = ios_lidar.IOSLidarSDTable.as_df(spark, force_compute=True, only_segments=[suri])\n",
    "\n",
    "sd_df.createOrReplaceTempView('sd_df')\n",
    "# sd_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_depth_90th(row):\n",
    "    ci = ios_lidar.IOSLidarSDTable.from_row(row.camera_image)\n",
    "    depth = ci.get_depth()\n",
    "    if depth is None:\n",
    "        return 0\n",
    "    else:\n",
    "        import numpy as np\n",
    "        return np.percentile(depth, 0.9)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "sample_sd_df = spark.sql(\"\"\"\n",
    "                        SELECT \n",
    "                          *\n",
    "                        FROM sd_df\n",
    "                        WHERE uri.topic == 'camera|front'\n",
    "                        ORDER BY RAND(1337)\n",
    "                        LIMIT 50\n",
    "                    \"\"\")\n",
    "\n",
    "depth_top_90th = sample_sd_df.rdd.map(get_depth_90th).max()\n",
    "\n",
    "\n",
    "if depth_top_90th <= 0.1:\n",
    "    period_meters = 0.005\n",
    "elif depth_top_90th <= 1.0:\n",
    "    period_meters = 0.05\n",
    "elif depth_top_90th <= 10.0:\n",
    "    period_meters = 0.5\n",
    "else:\n",
    "    period_meters = 10.\n",
    "\n",
    "print('period_meters', period_meters)\n",
    "    \n",
    "def to_t_debug_image(row):\n",
    "    import cv2\n",
    "    ci = ios_lidar.IOSLidarSDTable.from_row(row.camera_image)\n",
    "    image = ci.get_debug_image(period_meters=period_meters)\n",
    "#     if len(image.shape) != 3:\n",
    "#         import numpy\n",
    "#         image = np.tile([image, image, image], axis=2)\n",
    "#     assert False, image.shape\n",
    "    aspect = float(ci.width) / float(ci.height)\n",
    "    target_height = 400\n",
    "    target_width = int(aspect * target_height)\n",
    "    \n",
    "    # Pad the width a little to make ffmpeg most efficient\n",
    "    if target_width % 16 != 0:\n",
    "        target_width += 16 - (target_width % 16)\n",
    "    \n",
    "    image = cv2.resize(image, (target_width, target_height))\n",
    "\n",
    "    return row.uri.timestamp, image\n",
    "\n",
    "\n",
    "\n",
    "iter_t_image = sample_sd_df.rdd.map(to_t_debug_image).collect()\n",
    "images = [image for t, image in sorted(iter_t_image, key=lambda ti: ti[0])]\n",
    "\n",
    "im_min = min(i.min() for i in images)\n",
    "print(im_min)\n",
    "im_max = min(i.max() for i in images)\n",
    "print(im_max)\n",
    "images = [(255 * (i.astype('float') - im_min) / (im_max - im_min)).astype('uint8') for i in images]\n",
    "\n",
    "from psegs.util.plotting import images_to_html_video\n",
    "\n",
    "html = images_to_html_video(images, fps=4)\n",
    "print('html size', 1e-6 * len(html))\n",
    "\n",
    "def show_html(s):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(s), metadata=dict(isolated=True))\n",
    "\n",
    "show_html(html)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "init_notebook_mode(connected=False)         # initiate notebook for offline plot\n",
    "\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_sd_df = spark.sql(\"\"\"\n",
    "                        SELECT \n",
    "                          *\n",
    "                        FROM sd_df\n",
    "                        WHERE uri.topic == 'camera|front|depth'\n",
    "                        ORDER BY RAND(1337)\n",
    "                        LIMIT 50\n",
    "                    \"\"\")\n",
    "\n",
    "def get_cloud_world(row):\n",
    "    ci = ios_lidar.IOSLidarSDTable.from_row(row.camera_image)\n",
    "    pc = ci.depth_image_to_point_cloud()\n",
    "    cloud = pc.get_cloud()\n",
    "    cloud = cloud[:, :3]\n",
    "    T_world_from_ego = ci.ego_pose['ego', 'world']\n",
    "    cloud_world = T_world_from_ego.apply(cloud).T\n",
    "    return cloud_world\n",
    "\n",
    "cloud_worlds = sample_sd_df.rdd.map(get_cloud_world).collect()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "cloud_df = None\n",
    "for i, cloud in enumerate(cloud_worlds):\n",
    "    color = int(255 * (float(i) / len(cloud_worlds)))\n",
    "    \n",
    "    to_sample = 1000\n",
    "    if cloud.shape[0] > to_sample:\n",
    "        idx = np.random.choice(np.arange(cloud.shape[0]), to_sample)\n",
    "        cloud = cloud[idx, :]\n",
    "    \n",
    "    cur_df = pd.DataFrame(cloud, columns=['x', 'y', 'z'])\n",
    "    cur_df['color'] = 'rgb(%s, %s, %s)' % (color, color, color)\n",
    "    if cloud_df is None:\n",
    "        cloud_df = cur_df\n",
    "    else:\n",
    "        cloud_df = pd.concat([cloud_df, cur_df])\n",
    "    \n",
    "plots = []\n",
    "\n",
    "\n",
    "# cloud_df = pd.DataFrame(cloud_world, columns=['x', 'y', 'z'])\n",
    "# from psegs.util.plotting import rgb_for_distance\n",
    "# cloud_df['color'] = [\n",
    "#   (128, 128, 128)#rgb_for_distance(np.linalg.norm(pt), period_meters=period_meters)\n",
    "#   for pt in cloud_df[['x', 'y', 'z']].values\n",
    "# ]\n",
    "scatter = go.Scatter3d(\n",
    "                x=cloud_df['x'], y=cloud_df['y'], z=cloud_df['z'],\n",
    "                mode='markers',\n",
    "                marker=dict(size=2, color=cloud_df['color'], opacity=0.5),)\n",
    "\n",
    "plots.append(scatter)\n",
    "\n",
    "\n",
    "fig = go.Figure(data=plots)\n",
    "\n",
    "fig.update_layout(\n",
    "  width=1000, height=700,\n",
    "  scene_aspectmode='data')\n",
    "  # scene_camera=dict(\n",
    "  #   up=dict(x=0, y=0, z=1),\n",
    "  #   eye=dict(x=0, y=0, z=0),\n",
    "  #   center=dict(x=1, y=0, z=0),\n",
    "  # ))\n",
    "    \n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sd_df = spark.sql(\"\"\"\n",
    "                        SELECT \n",
    "                          *\n",
    "                        FROM sd_df\n",
    "                        WHERE uri.topic == 'camera|front'\n",
    "                        ORDER BY RAND(1337)\n",
    "                        LIMIT 50\n",
    "                    \"\"\")\n",
    "\n",
    "def get_t_ci(row):\n",
    "    ci = ios_lidar.IOSLidarSDTable.from_row(row.camera_image)\n",
    "    return row.uri.timestamp, ci\n",
    "\n",
    "t_cis = sample_sd_df.rdd.map(get_t_ci).collect()\n",
    "cis = [ci for t, ci in sorted(t_cis, key=lambda tc: tc[0])]\n",
    "\n",
    "plots += [ci.to_plotly_world_frame_3d() for ci in cis]\n",
    "\n",
    "\n",
    "fig = go.Figure(data=plots)\n",
    "\n",
    "fig.update_layout(\n",
    "  width=1000, height=700,\n",
    "  scene_aspectmode='data')\n",
    "  # scene_camera=dict(\n",
    "  #   up=dict(x=0, y=0, z=1),\n",
    "  #   eye=dict(x=0, y=0, z=0),\n",
    "  #   center=dict(x=1, y=0, z=0),\n",
    "  # ))\n",
    "    \n",
    "iplot(fig)\n",
    "\n",
    "\n",
    "\n",
    "# def get_xzy_90ths(row):\n",
    "#     ci = ios_lidar.IOSLidarSDTable.from_row(row.camera_image)\n",
    "#     pc = ci.depth_image_to_point_cloud()\n",
    "#     cloud = pc.get_cloud()\n",
    "    \n",
    "#     import numpy as np\n",
    "#     return np.percentile(depth, 0.9)\n",
    "    \n",
    "\n",
    "# def to_t_pc_rv_debug_image(row):\n",
    "#     ci = ios_lidar.IOSLidarSDTable.from_row(row.camera_image)\n",
    "#     pc = ci.depth_image_to_point_cloud()\n",
    "#     debug = pc.get_front_rv_debug_image()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
