{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemanticKITTI to Stamped Datum Table\n",
    "\n",
    "CAN DELETE THIS NOTEBOOK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "# Please follow the instructions posted on the SemanticKITTI website to obtain the data:\n",
    "# http://www.semantic-kitti.org/dataset.html#download\n",
    "# Additionally, if you wish to study optical flow, you'll want to expand the KITTI zip\n",
    "# file `data_odometry_color.zip`.\n",
    "# Extract the data as described to a directory and paste that directory path here:\n",
    "SEMANTICKITTI_ROOT = '/outer_root/host_mnt/Volumes/970-evo-raid0/semantickitti_odom_tmp/'\n",
    "\n",
    "OUTPUT_ROOT = '/tmp/semantickitti_fused_root/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Sequence 00 with 4541 scans\n",
      "Found Sequence 01 with 1101 scans\n",
      "Found Sequence 02 with 4661 scans\n",
      "Found Sequence 03 with 801 scans\n",
      "Found Sequence 04 with 271 scans\n",
      "Found Sequence 05 with 2761 scans\n",
      "Found Sequence 06 with 1101 scans\n",
      "Found Sequence 07 with 1101 scans\n",
      "Found Sequence 09 with 1591 scans\n",
      "Found Sequence 10 with 1201 scans\n",
      "Found 19130 total scans\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import open3d as o3d\n",
    "from oarphpy import util as oputil\n",
    "\n",
    "# Deduced from:\n",
    "# https://github.com/PRBonn/semantic-kitti-api/blob/c2d7712964a9541ed31900c925bf5971be2107c2/auxiliary/SSCDataset.py#L20\n",
    "SK_SPLIT_SEQUENCES = {\n",
    "    \"train\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"09\", \"10\"],\n",
    "    \"valid\": [\"08\"],\n",
    "    \"test\": [\"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\"]\n",
    "}\n",
    "\n",
    "SK_MOVING_LABELS = [\n",
    "    252, # \"moving-car\"\n",
    "    253, # \"moving-bicyclist\"\n",
    "    254, # \"moving-person\"\n",
    "    255, # \"moving-motorcyclist\"\n",
    "    256, # \"moving-on-rails\"\n",
    "    257, # \"moving-bus\"\n",
    "    258, # \"moving-truck\"\n",
    "    259, # \"moving-other-vehicle\"\n",
    "]\n",
    "\n",
    "def get_scene_basepath(seq):\n",
    "    return os.path.join(SEMANTICKITTI_ROOT, 'dataset/sequences', seq)\n",
    "\n",
    "SK_SEQ_TO_NSCANS = {}\n",
    "for seq in SK_SPLIT_SEQUENCES['train']:\n",
    "    scene_base = get_scene_basepath(seq)\n",
    "    last_vel = max(os.listdir(os.path.join(scene_base + '/velodyne/')))\n",
    "    n_scans = int(last_vel.replace('.bin', '')) + 1\n",
    "    print('Found Sequence %s with %s scans' % (seq, n_scans))\n",
    "    SK_SEQ_TO_NSCANS[seq] = n_scans\n",
    "print(\"Found %s total scans\" % sum(SK_SEQ_TO_NSCANS.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import six\n",
    "# from contextlib import contextmanager\n",
    "# class ThruputObserver(object):\n",
    "#   \"\"\"A utility for measuring the runtime and throughput of a subroutine.\n",
    "#   Similar in spirit to `tqdm`, except `ThruputObserver`:\n",
    "#    * Tracks not just time but a size metric (e.g. memory) in bytes\n",
    "#    * Reports percentiles\n",
    "#    * Simply logs strings and is not terminal-interactive\n",
    "  \n",
    "#   While `tqdm` is useful for notebooks, `ThruputObserver` seeks to be more\n",
    "#   useful for longer-running batch jobs.\n",
    "#   \"\"\"\n",
    "  \n",
    "#   def __init__(\n",
    "#       self,\n",
    "#       name='',\n",
    "#       log_on_del=False,\n",
    "#       only_stats=None,\n",
    "#       log_freq=100,\n",
    "#       n_total=None,\n",
    "#       n_total_chunks=None):\n",
    "#     self.n = 0\n",
    "#     self.num_bytes = 0\n",
    "#     self.ts = []\n",
    "#     self.name = name\n",
    "#     self.log_on_del = log_on_del\n",
    "#     self.only_stats = only_stats or []\n",
    "#     self.n_total = max(n_total, 1) if n_total is not None else None\n",
    "#     self.n_total_chunks = (\n",
    "#       max(n_total_chunks, 1) if n_total_chunks is not None else None)\n",
    "#     self._start = None\n",
    "#     self.__log_freq = log_freq\n",
    "#     self.__last_log = 0\n",
    "  \n",
    "#   @contextmanager\n",
    "#   def observe(self, n=0, num_bytes=0):\n",
    "#     \"\"\"\n",
    "#     NB: contextmanagers appear to be expensive due to object creation.\n",
    "#     Use ThurputObserver#{start,stop}_block() for <10ms ops. \n",
    "#     FMI https://stackoverflow.com/questions/34872535/why-contextmanager-is-slow\n",
    "#     \"\"\"\n",
    "\n",
    "#     self.start_block()\n",
    "#     yield\n",
    "#     self.stop_block(n=n, num_bytes=num_bytes)\n",
    "  \n",
    "#   def start_block(self):\n",
    "#     self._start = time.time()\n",
    "  \n",
    "#   def update_tallies(self, n=0, num_bytes=0, new_block=False):\n",
    "#     self.n += n\n",
    "#     self.num_bytes += num_bytes\n",
    "#     if new_block:\n",
    "#       self.stop_block()\n",
    "#       self.start_block()\n",
    "  \n",
    "#   def stop_block(self, n=0, num_bytes=0):\n",
    "#     end = time.time()\n",
    "#     self.n += n\n",
    "#     self.num_bytes += num_bytes\n",
    "#     if self._start is not None:\n",
    "#       self.ts.append(end - self._start)\n",
    "#     self._start = None\n",
    "  \n",
    "#   def maybe_log_progress(self, every_n=-1):\n",
    "#     if every_n >= 0:\n",
    "#       self.__log_freq = every_n\n",
    "#     if self.n >= self.__last_log + self.__log_freq:\n",
    "#       from oarphpy.util import log\n",
    "#       print(\"Progress for \\n\" + str(self)) # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#       self.__last_log = self.n\n",
    "#         # Track last log because `n` may increase inconsistently\n",
    "#       if every_n == -1 and (self.n >= (1.7 * self.__log_freq)):\n",
    "#         self.__log_freq = int(1.7 * self.__log_freq)\n",
    "#           # Exponentially decay logging frequency. Don't decay quite as\n",
    "#           # fast as Vowpal Wabbit did, though.\n",
    "\n",
    "#   @staticmethod\n",
    "#   def union(thruputs):\n",
    "#     u = ThruputObserver()\n",
    "#     for t in thruputs:\n",
    "#       u += t\n",
    "#     return u\n",
    "\n",
    "#   @property\n",
    "#   def total_time(self):\n",
    "#     return sum(self.ts)\n",
    "\n",
    "#   def get_stats(self):\n",
    "#     import numpy as np\n",
    "#     from humanfriendly import format_size\n",
    "#     from humanfriendly import format_timespan\n",
    "\n",
    "#     total_time = self.total_time\n",
    "\n",
    "#     stats = [\n",
    "#       ('Thruput', ''),\n",
    "#       ('N thru', (self.n\n",
    "#                     if self.n_total is None\n",
    "#                     else '%s (of %s)' % (self.n, self.n_total))),\n",
    "#       ('N chunks', (len(self.ts)\n",
    "#                     if self.n_total_chunks is None\n",
    "#                     else '%s (of %s)' % (len(self.ts), self.n_total_chunks))),\n",
    "#       ('Total time', format_timespan(total_time) if total_time else '-'),\n",
    "#       ('Total thru', format_size(self.num_bytes)),\n",
    "#       ('Rate', \n",
    "#         format_size(self.num_bytes / total_time) + ' / sec'\n",
    "#         if total_time else '-'),\n",
    "#       ('Hz', float(self.n) / total_time if total_time else '-'),\n",
    "#     ]\n",
    "#     percent_complete = None\n",
    "#     if self.n_total is not None:\n",
    "#       percent_complete = 100. * float(self.n) / self.n_total\n",
    "#     elif self.n_total_chunks is not None:\n",
    "#       percent_complete = 100. * float(len(self.ts)) / self.n_total_chunks\n",
    "#     if percent_complete is not None:\n",
    "#       eta_sec = (\n",
    "#         (100. - percent_complete) * \n",
    "#         (total_time / (percent_complete + 1e-10)))\n",
    "#       stats.extend([\n",
    "#         ('Progress', ''),\n",
    "#         ('Percent Complete', percent_complete),\n",
    "#         ('Est. Time To Completion', format_timespan(eta_sec)),\n",
    "#       ])\n",
    "#     if len(self.ts) >= 2:\n",
    "#       format_t = lambda t: format_timespan(t, detailed=True)\n",
    "#       stats.extend([\n",
    "#         ('Latency (per chunk)', ''),\n",
    "#         ('Avg', format_t(np.mean(self.ts))),\n",
    "#         ('p50', format_t(np.percentile(self.ts, 50))),\n",
    "#         ('p95', format_t(np.percentile(self.ts, 95))),\n",
    "#         ('p99', format_t(np.percentile(self.ts, 99))),\n",
    "#       ])\n",
    "#     if self.only_stats:\n",
    "#       stats = tuple(\n",
    "#         (name, value)\n",
    "#         for name, value in stats\n",
    "#         if name in self.only_stats\n",
    "#       )\n",
    "#     return stats\n",
    "\n",
    "#   def __iadd__(self, other):\n",
    "#     self.n += other.n\n",
    "#     self.num_bytes += other.num_bytes\n",
    "#     self.ts.extend(other.ts)\n",
    "#     return self\n",
    "\n",
    "#   def __str__(self):\n",
    "#     import tabulate\n",
    "#     stats = self.get_stats()\n",
    "#     summary = tabulate.tabulate(stats)\n",
    "#     if self.name:\n",
    "#       prefix = '%s [Pid:%s Id:%s]' % (self.name, os.getpid(), id(self))\n",
    "#       summary = prefix + '\\n' + summary\n",
    "#     return summary\n",
    "  \n",
    "#   def __del__(self):\n",
    "#     if self.log_on_del:\n",
    "#       self.stop_block()\n",
    "\n",
    "#       from oarphpy.util import create_log\n",
    "#       log = create_log()\n",
    "#       print('\\n' + str(self) + '\\n') #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  \n",
    "#   @staticmethod\n",
    "#   def monitoring_tensor(name, tensor, **observer_init_kwargs):\n",
    "#     \"\"\"Monitor the size of the given tensorflow `Tensor` and record a\n",
    "#     text TF Summary with the contents of this ThruputObserver.\"\"\"\n",
    "\n",
    "#     class Observer(object):\n",
    "#       def __init__(self, dtype_size_bytes):\n",
    "#         self.observer = ThruputObserver(name=name, **observer_init_kwargs)\n",
    "#         self.dtype_size_bytes = dtype_size_bytes\n",
    "#       def __call__(self, t_shape):\n",
    "#         import numpy as np\n",
    "#         n = t_shape[0]\n",
    "#         num_bytes = np.prod(t_shape) * self.dtype_size_bytes\n",
    "#         self.observer.stop_block(n=n, num_bytes=num_bytes)\n",
    "#         self.observer.maybe_log_progress()\n",
    "        \n",
    "#         # Tensorboard is very picky about wanting Markdown :P\n",
    "#         import tabulatehelper as th\n",
    "#         stats = self.observer.get_stats()\n",
    "#         out = th.md_table(stats, headers=[name])\n",
    "\n",
    "#         self.observer.start_block()\n",
    "#         return out\n",
    "    \n",
    "#     import tensorflow as tf\n",
    "#     obs_str_tensor = tf.compat.v1.py_func(\n",
    "#               Observer(tensor.dtype.size), [tf.shape(tensor)], tf.string)\n",
    "#     tf.summary.text(name + '/ThruputObserver', obs_str_tensor)\n",
    "#     return obs_str_tensor\n",
    "  \n",
    "#   @staticmethod\n",
    "#   def wrap_func(func, **observer_init_kwargs):\n",
    "#     \"\"\"Decorate `func` and observe a block on each call\"\"\"\n",
    "#     class MonitoredFunc(object):\n",
    "#       def __init__(self, func, observer_init_kwargs):\n",
    "#         self.func = func\n",
    "#         self.observer = ThruputObserver(**observer_init_kwargs)\n",
    "#       def __call__(self, *args, **kwargs):\n",
    "#         from oarphpy.util.misc import get_size_of_deep\n",
    "#         self.observer.start_block()\n",
    "#         ret = self.func(*args, **kwargs)\n",
    "#         self.observer.stop_block(n=1, num_bytes=get_size_of_deep(ret))\n",
    "#         self.observer.maybe_log_progress()\n",
    "#         return ret\n",
    "#     return MonitoredFunc(func, observer_init_kwargs)\n",
    "\n",
    "#   @staticmethod\n",
    "#   def monitor_generator(gen, **observer_init_kwargs): #~~~~~~~~~~~~~~~~~~~~\n",
    "#     observer_init_kwargs['log_on_del'] = True\n",
    "#     t = ThruputObserver(**observer_init_kwargs)\n",
    "#     while True:\n",
    "#         t.start_block()\n",
    "#         x = six.next(gen)\n",
    "#         t.stop_block(n=1, num_bytes=oputil.get_size_of_deep(x)) # ~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "#         yield x\n",
    "        \n",
    "#         t.maybe_log_progress()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support Code\n",
    "\n",
    "def get_calibration(seq):\n",
    "    scene_base = get_scene_basepath(seq)\n",
    "    return parse_calibration(os.path.join(scene_base, 'calib.txt'))\n",
    "\n",
    "def get_poses(seq):\n",
    "    scene_base = get_scene_basepath(seq)\n",
    "    return parse_poses(os.path.join(scene_base, \"poses.txt\"))\n",
    "    \n",
    "def parse_calibration(path):\n",
    "    \"\"\"Parse a calibration file and return a map to 4x4 Numpy matrices.\n",
    "    Important keys returned:\n",
    "    * Tr - the lidar to camera static transform\n",
    "    * P2 - the left camera projective matrix P\n",
    "    Based upon https://github.com/PRBonn/semantic-kitti-api/blob/9b5feda3b19ea560a298493b9a5ebebe0cbe2cc2/generate_sequential.py#L14\n",
    "    \"\"\"\n",
    "    calib = {}\n",
    "\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            key, mat_str = line.strip().split(\":\")\n",
    "            values = [float(v) for v in mat_str.strip().split()]\n",
    "            mat = np.zeros((4, 4))\n",
    "            mat[0, 0:4] = values[0:4]\n",
    "            mat[1, 0:4] = values[4:8]\n",
    "            mat[2, 0:4] = values[8:12]\n",
    "            mat[3, 3] = 1.0\n",
    "            calib[key] = mat\n",
    "    return calib\n",
    "\n",
    "def parse_poses(path):\n",
    "    \"\"\"Read a SemanticKITTI (per-scan) poses file and return a list of 4x4 homogenous\n",
    "    RT matrices that express world-to-left-camera transforms.  The index of this list is\n",
    "    implicitly the scan ID.\n",
    "    \n",
    "    Based upon: https://github.com/PRBonn/semantic-kitti-api/blob/9b5feda3b19ea560a298493b9a5ebebe0cbe2cc2/generate_sequential.py#L42\n",
    "    \"\"\"\n",
    "    poses = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            values = [float(v) for v in line.strip().split()]\n",
    "            mat = np.zeros((4, 4))\n",
    "            mat[0, 0:4] = values[0:4]\n",
    "            mat[1, 0:4] = values[4:8]\n",
    "            mat[2, 0:4] = values[8:12]\n",
    "            mat[3, 3] = 1.0\n",
    "            poses.append(mat)\n",
    "    return poses\n",
    "    \n",
    "\n",
    "    \n",
    "# #     Tr = calib[\"Tr\"]\n",
    "# #     Tr_inv = np.linalg.inv(Tr)\n",
    "    \n",
    "    \n",
    "    \n",
    "#   \"\"\" read poses file with per-scan poses from given filename\n",
    "#       Returns\n",
    "#       -------\n",
    "#       list\n",
    "#           list of poses as 4x4 numpy arrays.\n",
    "#   \"\"\"\n",
    "#   file = open(filename)\n",
    "\n",
    "#   poses = []\n",
    "\n",
    "#   Tr = calibration[\"Tr\"]\n",
    "# #   print('Tr', Tr)\n",
    "# #   Tr = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "#   Tr_inv = np.linalg.inv(Tr)\n",
    "\n",
    "#   for line in file:\n",
    "#     values = [float(v) for v in line.strip().split()]\n",
    "\n",
    "#     pose = np.zeros((4, 4))\n",
    "#     pose[0, 0:4] = values[0:4]\n",
    "#     pose[1, 0:4] = values[4:8]\n",
    "#     pose[2, 0:4] = values[8:12]\n",
    "#     pose[3, 3] = 1.0\n",
    "\n",
    "#     poses.append(np.matmul(Tr_inv, np.matmul(pose, Tr)))\n",
    "# #     poses.append(np.matmul(pose, Tr))\n",
    "#   file.close()\n",
    "#   return poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-11 09:51:40,514\toarph 17241 : Using source root /opt/psegs/psegs \n",
      "INFO - 2021-02-11 09:51:40,514 - spark - Using source root /opt/psegs/psegs \n",
      "2021-02-11 09:51:40,516\toarph 17241 : Using source root /opt/psegs \n",
      "INFO - 2021-02-11 09:51:40,516 - spark - Using source root /opt/psegs \n",
      "2021-02-11 09:51:40,555\toarph 17241 : Generating egg to /tmp/tmpngjmscf__oarphpy_eggbuild ...\n",
      "INFO - 2021-02-11 09:51:40,555 - spark - Generating egg to /tmp/tmpngjmscf__oarphpy_eggbuild ...\n",
      "INFO - 2021-02-11 09:51:40,571 - driver - Generating grammar tables from /usr/lib/python3.8/lib2to3/Grammar.txt\n",
      "INFO - 2021-02-11 09:51:40,626 - driver - Generating grammar tables from /usr/lib/python3.8/lib2to3/PatternGrammar.txt\n",
      "2021-02-11 09:51:40,706\toarph 17241 : ... done.  Egg at /tmp/tmpngjmscf__oarphpy_eggbuild/psegs-0.0.0-py3.8.egg\n",
      "INFO - 2021-02-11 09:51:40,706 - spark - ... done.  Egg at /tmp/tmpngjmscf__oarphpy_eggbuild/psegs-0.0.0-py3.8.egg\n",
      "INFO - 2021-02-11 09:51:43,209 - kernelextension - Client Connected ('127.0.0.1', 56946)\n"
     ]
    }
   ],
   "source": [
    "from oarphpy.spark import NBSpark\n",
    "NBSpark.SRC_ROOT = '/opt/psegs/psegs'\n",
    "NBSpark.SRC_ROOT_MODULES = ['psegs']\n",
    "NBSpark.CONF_KV.update({\n",
    "    'spark.driver.maxResultSize': '10g',\n",
    "    'spark.driver.memory': '16g',\n",
    "  })\n",
    "# NBSpark.CONF_KV.pop('spark.extraListeners')\n",
    "spark = NBSpark.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuse World Clouds and Dump Them\n",
    "\n",
    "Nota Bene! Excellent large point cloud viewer: \n",
    "```\n",
    "docker --context default run -it --name=potree_viewer --rm --net=host -v `pwd`:/shared  jonazpiazu/potree\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SingleSequenceWorldCloudFuser(object):\n",
    "    \n",
    "#     def __init__(self, seq):\n",
    "#         self.seq = seq\n",
    "#         self.scene_base = get_scene_basepath(seq)\n",
    "        \n",
    "#         print(\"Loading calibration for sequence %s\" % seq)\n",
    "#         self.calib = get_calibration(seq)\n",
    "              \n",
    "#         print(\"Loading poses for sequence %s\" % seq)\n",
    "#         self.all_poses = get_poses(seq)\n",
    "\n",
    "#     @classmethod\n",
    "#     def get_moving_mask_for_scan(cls, scene_base, scan_id):\n",
    "#         scan_name = str(scan_id).rjust(6, '0')\n",
    "#         labels_path = os.path.join(scene_base, 'labels', scan_name + '.label')\n",
    "#         labels = np.fromfile(labels_path, dtype=np.uint32)\n",
    "#         labels = labels.reshape((-1))\n",
    "#         sem_label = labels & 0xFFFF  # semantic label in lower half\n",
    "#         inst_label = labels >> 16    # instance id in upper half\n",
    "#          # NB: 22 / 252 is chase car in scene 08 !!!\n",
    "        \n",
    "#         moving_mask = np.logical_or.reduce(tuple((sem_label == c) for c in SK_MOVING_LABELS))\n",
    "#         return moving_mask\n",
    "        \n",
    "#     def read_scan_get_clean_world_cloud(self, scan_id):\n",
    "#         import numpy as np\n",
    "\n",
    "#         scan_name = str(scan_id).rjust(6, '0')\n",
    "#         scan_path = os.path.join(self.scene_base, 'velodyne', scan_name + '.bin')\n",
    "#         lidar = np.frombuffer(open(scan_path, 'rb').read(), dtype=np.float32).reshape((-1, 4))\n",
    "#         cloud = np.ones(lidar.shape)  # need homogenous for change below\n",
    "#         cloud[:, 0:3] = lidar[:, 0:3]\n",
    "\n",
    "#         # Move cloud into the world frame\n",
    "#         Tr = self.calib[\"Tr\"]\n",
    "#         Tr_inv = np.linalg.inv(Tr)\n",
    "#         cam2_pose = self.all_poses[scan_id]\n",
    "#         pose = np.matmul(Tr_inv, np.matmul(cam2_pose, Tr))  \n",
    "#         cloud = np.matmul(pose, cloud.T).T\n",
    "\n",
    "#         # Clean out points for anything moving\n",
    "# #         moving_mask = np.logical_or.reduce(tuple((sem_label == c) for c in SK_MOVING_LABELS))\n",
    "# #         if not moving_mask.any():\n",
    "# #             frames_no_movers.append(s)\n",
    "#         moving_mask = self.get_moving_mask_for_scan(self.scene_base, scan_id)\n",
    "#         static_cloud = cloud[~moving_mask][:, :3]\n",
    "        \n",
    "#         # TODO need to scrube the ego car !!  \n",
    "#         # moving_cloud = cloud[moving_mask][:, :3]\n",
    "#         return static_cloud\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for seq, n_scans in sorted(SK_SEQ_TO_NSCANS.items()):\n",
    "#     print(\"Fusing sequence %s ...\" % seq)\n",
    "#     fuser = SingleSequenceWorldCloudFuser(seq)\n",
    "    \n",
    "#     slices = n_scans // 100\n",
    "#     task_rdd = spark.sparkContext.parallelize(range(n_scans), numSlices=slices)\n",
    "#     cloud_rdd = task_rdd.map(lambda s: fuser.read_scan_get_clean_world_cloud(s))\n",
    "    \n",
    "#     import pyspark\n",
    "#     cloud_rdd = cloud_rdd.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "    \n",
    "    \n",
    "#     iter_clouds = cloud_rdd.toLocalIterator()#prefetchPartitions=True)):  TODO FIXME USING SPARK 2.4 !!!\n",
    "#     iter_clouds_t = ThruputObserver.monitor_generator(iter_clouds, n_total=n_scans, log_freq=100)\n",
    "#     fused_world_cloud = np.vstack(iter_clouds_t)\n",
    "    \n",
    "#     print(\"Fused world cloud: {s} ({sz:.2f} GBytes)\".format(\n",
    "#         s=fused_world_cloud.shape, sz=fused_world_cloud.nbytes * 1e-9))\n",
    "    \n",
    "#     fused_world_root = os.path.join(OUTPUT_ROOT, 'fused_world_clouds')\n",
    "#     oputil.mkdir(fused_world_root)\n",
    "\n",
    "#     import pickle\n",
    "#     path = os.path.join(fused_world_root, \"%s.pkl\" % seq)\n",
    "#     pickle.dump(fused_world_cloud, open(path, 'wb'), protocol=4)\n",
    "#     print('Saved fused world cloud pkl to %s' % path)\n",
    "    \n",
    "#     pcd = o3d.geometry.PointCloud()\n",
    "#     pcd.points = o3d.utility.Vector3dVector(fused_world_cloud)\n",
    "#     path = os.path.join(fused_world_root, \"%s.ply\" % seq)\n",
    "#     o3d.io.write_point_cloud(path, pcd)\n",
    "#     print('Saved fused world cloud to %s' % path)\n",
    "# # #     n_moving_pts = sum(c.shape[0] for c in all_moving_clouds)\n",
    "# # #     print('moving_cloud pts', n_moving_pts, float(n_moving_pts) / fused_world_cloud.shape[0])\n",
    "# # #     print('frames_no_movers', frames_no_movers[:20])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for frames with zero moving things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # for seq, n_scans in sorted(SK_SEQ_TO_NSCANS.items()):\n",
    "# #     print(\"Searching sequence %s ...\" % seq)\n",
    "    \n",
    "# #     slices = n_scans // 100\n",
    "# #     task_rdd = spark.sparkContext.parallelize(range(n_scans), numSlices=slices)\n",
    "    \n",
    "# #     scan_has_no_movers = lambda scan_id: (not seq_scan_has_movers(seq, scan_id))\n",
    "# #     scans_no_movers = task_rdd.filter(scan_has_no_movers).collect()\n",
    "    \n",
    "# #     print(\"Sequence %s has %s frames with no moving points ...\" % (seq, len(scans_no_movers)))\n",
    "\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/opt/psegs')\n",
    "\n",
    "# import copy\n",
    "# from psegs import datum\n",
    "# from psegs import util\n",
    "# from psegs.table.sd_table import StampedDatumTableBase\n",
    "# class SemanticKITTIFusedSDTable(StampedDatumTableBase):\n",
    "    \n",
    "#     ONLY_FRAMES_WITH_NO_MOVERS = True\n",
    "    \n",
    "#     import sys\n",
    "#     sys.path.append('/opt/psegs')\n",
    "    \n",
    "#     @classmethod\n",
    "#     def _get_all_segment_uris(cls):\n",
    "#         return [\n",
    "#             datum.URI(\n",
    "#                 dataset='semantikitti-psegs-fused',\n",
    "#                 split='train',\n",
    "#                 segment_id=str(seq))\n",
    "#             for seq in SK_SEQ_TO_NSCANS.keys()\n",
    "#         ]\n",
    "\n",
    "#     @classmethod\n",
    "#     def _create_datum_rdds(cls, spark, existing_uri_df=None, only_segments=None):\n",
    "#         \"\"\"Subclasses should create and return a list of `RDD[StampedDatum]`s\n",
    "\n",
    "#         only_segments must be segment uris\n",
    "#         TODO docs ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
    "        \n",
    "        \n",
    "#         assert existing_uri_df is None, \"Resume feature not supported\"\n",
    "#         seg_uris = cls.get_all_segment_uris()\n",
    "#         if only_segments:\n",
    "#             util.log.info(\"Filtering to only %s segments\" % len(only_segments))\n",
    "#             seg_uris = [\n",
    "#                 uri for uri in seg_uris\n",
    "#                 if any(\n",
    "#                   suri.soft_matches_segment(uri) for suri in only_segments)\n",
    "#             ]\n",
    "        \n",
    "#         datum_rdds = []\n",
    "#         for seg_uri in seg_uris:\n",
    "#             seq = seg_uri.segment_id\n",
    "#             if cls.ONLY_FRAMES_WITH_NO_MOVERS:\n",
    "#                 util.log.info(\"Finding scans for sequence %s with no movering points ...\" % seq)\n",
    "#                 n_scans = SK_SEQ_TO_NSCANS[seq]\n",
    "#                 slices = n_scans // 100\n",
    "#                 task_rdd = spark.sparkContext.parallelize(range(n_scans), numSlices=slices)\n",
    "#                 scan_has_no_movers = lambda scan_id: (not seq_scan_has_movers(seq, scan_id))\n",
    "#                 scans_no_movers = task_rdd.filter(scan_has_no_movers).collect()\n",
    "#                 util.log.info(\"... sequence %s has %s scans with no movers.\" % (seq, len(scans_no_movers)))\n",
    "#                 scan_ids = scans_no_movers\n",
    "#             else:\n",
    "#                 scan_ids = list(range(SK_SEQ_TO_NSCANS[seq]))\n",
    "            \n",
    "            \n",
    "#             tasks = [(seg_uri, scan_id) for scan_id in scan_ids]\n",
    "            \n",
    "#             # Emit camera_image RDD\n",
    "#             ctask_rdd = spark.sparkContext.parallelize(tasks)\n",
    "#             datum_rdd = ctask_rdd.map(lambda t: cls.create_camera_frame(*t))\n",
    "#             datum_rdds.append(datum_rdd)\n",
    "            \n",
    "#             # Emit ego_pose RDD\n",
    "#             ptask_rdd = spark.sparkContext.parallelize(tasks)\n",
    "#             datum_rdd = ptask_rdd.map(lambda t: cls.create_ego_pose(*t))\n",
    "#             datum_rdds.append(datum_rdd)\n",
    "            \n",
    "#             # Emit world cloud once\n",
    "#             wc_rdd = spark.sparkContext.parallelize([seg_uri])\n",
    "#             datum_rdd = wc_rdd.map(lambda t: cls.create_world_cloud(t))\n",
    "#             datum_rdds.append(datum_rdd)\n",
    "    \n",
    "#         return datum_rdds\n",
    "        \n",
    "#         # Emit camera and pose RDDs\n",
    "        \n",
    "        \n",
    "#         # for each segment emit camera and ego pose RDDs\n",
    "#         # for each world cloud emit flyweight\n",
    "#         # if we had cuboids, we'd emit them and object fused clouds\n",
    "#         # for the fused stuff, perhaps lazy-create those? and/or require as a\n",
    "#         # FIXTURES thing.\n",
    "    \n",
    "#     @classmethod\n",
    "#     def _get_calib(cls, seq):\n",
    "#         if not hasattr(cls, '_calib'):\n",
    "#             cls._calib = {}\n",
    "#         if seq not in cls._calib:\n",
    "#             cls._calib[seq] = get_calibration(seq)\n",
    "#         return cls._calib[seq]\n",
    "    \n",
    "#     @classmethod\n",
    "#     def _get_poses(cls, seq):\n",
    "#         if not hasattr(cls, '_poses'):\n",
    "#             cls._poses = {}\n",
    "#         if seq not in cls._poses:\n",
    "#             cls._poses[seq] = get_poses(seq)\n",
    "#         return cls._poses[seq]\n",
    "    \n",
    "#     @classmethod\n",
    "#     def create_camera_frame(cls, base_uri, scan_id):\n",
    "#         seq = base_uri.segment_id\n",
    "#         calib = cls._get_calib(seq)\n",
    "        \n",
    "#         uri = copy.deepcopy(base_uri)\n",
    "#         uri.topic = 'camera|left_rect'\n",
    "#         uri.timestamp = int(scan_id) # HACK!\n",
    "\n",
    "#         scene_base = get_scene_basepath(seq)\n",
    "#         scan_name = str(scan_id).rjust(6, '0')\n",
    "#         img_path = os.path.join(scene_base, 'image_2/', scan_name + '.png')\n",
    "#         assert os.path.exists(img_path), (\n",
    "#             \"Did you remember to expand data_odometry_color.zip ? %s not found\" % img_path)\n",
    "#         with open(img_path, 'rb') as f:\n",
    "#             width, height = util.get_png_wh(f.read(100)) # HACK!!!!\n",
    "        \n",
    "#         image_png = util.LazyThunktor(lambda: open(img_path, 'rb').read())\n",
    "        \n",
    "#         # HACK!!!  This is actually P !!!\n",
    "#         K = calib['P2']\n",
    "        \n",
    "#         # hack! this is lidar to cam\n",
    "#         ego_to_sensor = datum.Transform.from_transformation_matrix(\n",
    "#                 calib['Tr'], src_frame='lidar', dest_frame=uri.topic)\n",
    "        \n",
    "#         sd_ego_pose = cls.create_ego_pose(base_uri, scan_id)\n",
    "#         ego_pose = sd_ego_pose.transform\n",
    "#         ci = datum.CameraImage(\n",
    "#               sensor_name=uri.topic,\n",
    "#               image_png=image_png,\n",
    "#               width=width,\n",
    "#               height=height,\n",
    "#               timestamp=uri.timestamp,\n",
    "#               ego_pose=ego_pose,\n",
    "#               K=K,\n",
    "#               ego_to_sensor=ego_to_sensor,\n",
    "#               extra={'semantic_kitti.scan_id': str(scan_id)})\n",
    "#         return datum.StampedDatum(uri=uri, camera_image=ci)\n",
    "    \n",
    "#     @classmethod\n",
    "#     def create_ego_pose(cls, base_uri, scan_id):\n",
    "#         seq = base_uri.segment_id\n",
    "#         poses = cls._get_poses(seq)\n",
    "        \n",
    "#         uri = copy.deepcopy(base_uri)\n",
    "#         uri.topic = 'ego_pose'\n",
    "#         uri.timestamp = int(scan_id) # HACK!\n",
    "        \n",
    "#         # Hack! believe ego frame is lidar here?\n",
    "#         ego_pose = datum.Transform.from_transformation_matrix(\n",
    "#                 poses[scan_id], src_frame='world', dest_frame='ego')\n",
    "\n",
    "#         return datum.StampedDatum(uri=uri, transform=ego_pose)      \n",
    "    \n",
    "#     @classmethod\n",
    "#     def create_world_cloud(cls, base_uri):\n",
    "#         seq = base_uri.segment_id\n",
    "\n",
    "#         uri = copy.deepcopy(base_uri)\n",
    "#         uri.topic = 'lidar|world_fused'\n",
    "#         uri.timestamp = 0 # HACK!\n",
    "        \n",
    "#         cloud_path = os.path.join(OUTPUT_ROOT, 'fused_world_clouds', seq + '.ply')\n",
    "#         def ply_to_np(path):\n",
    "#             import open3d\n",
    "#             pcd = open3d.io.read_point_cloud(str(path))\n",
    "#             return np.asarray(pcd.points)\n",
    "#         cloud = util.LazyThunktor(lambda: ply_to_np(cloud_path))\n",
    "#         pc = datum.PointCloud(\n",
    "#               sensor_name=uri.topic,\n",
    "#               timestamp=uri.timestamp,\n",
    "#               cloud=cloud,\n",
    "#               ego_to_sensor=datum.Transform(),\n",
    "#               ego_pose=datum.Transform(),\n",
    "#               extra={'semantic_kitti.world_cloud_path': cloud_path})\n",
    "#         return datum.StampedDatum(uri=uri, point_cloud=pc)\n",
    "\n",
    "# seg_uris = SemanticKITTIFusedSDTable.get_all_segment_uris()\n",
    "# sd_rdd = SemanticKITTIFusedSDTable._get_segment_datum_rdd_or_df(spark, seg_uris[0])\n",
    "# print(sd_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/psegs')\n",
    "\n",
    "import copy\n",
    "\n",
    "from psegs import datum\n",
    "from psegs import util\n",
    "from psegs.table.sd_table import StampedDatumTableBase\n",
    "\n",
    "\n",
    "def get_moving_mask_for_scan(scene_base, scan_id):\n",
    "    scan_name = str(scan_id).rjust(6, '0')\n",
    "    labels_path = os.path.join(scene_base, 'labels', scan_name + '.label')\n",
    "    labels = np.fromfile(labels_path, dtype=np.uint32)\n",
    "    labels = labels.reshape((-1))\n",
    "    sem_label = labels & 0xFFFF  # semantic label in lower half\n",
    "    inst_label = labels >> 16    # instance id in upper half\n",
    "     # NB: 22 / 252 is chase car in scene 08 !!!\n",
    "\n",
    "    moving_mask = np.logical_or.reduce(tuple((sem_label == c) for c in SK_MOVING_LABELS))\n",
    "    return moving_mask\n",
    "\n",
    "def seq_scan_has_movers(seq, scan_id):\n",
    "    scene_base = get_scene_basepath(seq)\n",
    "#     moving_mask = SingleSequenceWorldCloudFuser.get_moving_mask_for_scan(scene_base, scan_id)\n",
    "    moving_mask = get_moving_mask_for_scan(scene_base, scan_id)\n",
    "    return moving_mask.any()\n",
    "\n",
    "def read_scan_get_cloud(seq, scan_id, remove_movers=True, filter_ego=True):\n",
    "    scan_name = str(scan_id).rjust(6, '0')\n",
    "    scene_base = get_scene_basepath(seq)\n",
    "    scan_path = os.path.join(scene_base, 'velodyne', scan_name + '.bin')\n",
    "\n",
    "    # Read the raw lidar\n",
    "    lidar = np.frombuffer(open(scan_path, 'rb').read(), dtype=np.float32).reshape((-1, 4))\n",
    "    cloud = np.ones(lidar.shape)  # need homogenous for change below\n",
    "    cloud[:, 0:3] = lidar[:, 0:3]\n",
    "\n",
    "    if remove_movers:\n",
    "        # Clean out points for anything moving\n",
    "        moving_mask = get_moving_mask_for_scan(scene_base, scan_id)\n",
    "        cloud = cloud[~moving_mask]#[:, :3]\n",
    "    \n",
    "    if filter_ego:\n",
    "        pass # TODO ~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return cloud\n",
    "\n",
    "\n",
    "\n",
    "class SemanticKITTISDTable(StampedDatumTableBase):\n",
    "    \n",
    "    ONLY_FRAMES_WITH_NO_MOVERS = True\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_all_segment_uris(cls):\n",
    "        return [\n",
    "            datum.URI(\n",
    "                dataset='semantikitti-psegs-fused',\n",
    "                split='train',\n",
    "                segment_id=str(seq))\n",
    "            for seq in SK_SEQ_TO_NSCANS.keys()\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def _create_datum_rdds(cls, spark, existing_uri_df=None, only_segments=None):\n",
    "        assert existing_uri_df is None, \"Resume feature not supported\"\n",
    "        \n",
    "        seg_uris = cls.get_all_segment_uris()\n",
    "        if only_segments:\n",
    "            util.log.info(\"Filtering to only %s segments\" % len(only_segments))\n",
    "            seg_uris = [\n",
    "                uri for uri in seg_uris\n",
    "                if any(\n",
    "                  suri.soft_matches_segment(uri) for suri in only_segments)\n",
    "            ]\n",
    "        \n",
    "        datum_rdds = []\n",
    "        for seg_uri in seg_uris:\n",
    "            seq = seg_uri.segment_id\n",
    "            if cls.ONLY_FRAMES_WITH_NO_MOVERS:\n",
    "                util.log.info(\"Finding scans for sequence %s with no movering points ...\" % seq)\n",
    "                n_scans = SK_SEQ_TO_NSCANS[seq]\n",
    "                slices = n_scans // 100\n",
    "                task_rdd = spark.sparkContext.parallelize(range(n_scans), numSlices=slices)\n",
    "                scan_has_no_movers = lambda scan_id: (not seq_scan_has_movers(seq, scan_id))\n",
    "                scans_no_movers = task_rdd.filter(scan_has_no_movers).collect()\n",
    "                util.log.info(\"... sequence %s has %s scans with no movers.\" % (seq, len(scans_no_movers)))\n",
    "                scan_ids = scans_no_movers\n",
    "            else:\n",
    "                scan_ids = list(range(SK_SEQ_TO_NSCANS[seq]))\n",
    "            \n",
    "            \n",
    "            tasks = [(seg_uri, scan_id) for scan_id in scan_ids]\n",
    "            \n",
    "            # Emit camera_image RDD\n",
    "            ctask_rdd = spark.sparkContext.parallelize(tasks)\n",
    "            datum_rdd = ctask_rdd.map(lambda t: cls.create_camera_frame(*t))\n",
    "            datum_rdds.append(datum_rdd)\n",
    "            \n",
    "            # Emit ego_pose RDD\n",
    "            ptask_rdd = spark.sparkContext.parallelize(tasks)\n",
    "            datum_rdd = ptask_rdd.map(lambda t: cls.create_ego_pose(*t))\n",
    "            datum_rdds.append(datum_rdd)\n",
    "            \n",
    "            # Emit velodyne cloud RDD\n",
    "            pctask_rdd = spark.sparkContext.parallelize(tasks[:100])\n",
    "            datum_rdd = pctask_rdd.map(lambda t: cls.create_point_cloud_in_world(*t))\n",
    "            datum_rdds.append(datum_rdd)\n",
    "#             # Emit world cloud once\n",
    "#             wc_rdd = spark.sparkContext.parallelize([seg_uri])\n",
    "#             datum_rdd = wc_rdd.map(lambda t: cls.create_world_cloud(t))\n",
    "#             datum_rdds.append(datum_rdd)\n",
    "    \n",
    "        return datum_rdds\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_calib(cls, seq):\n",
    "        if not hasattr(cls, '_calib'):\n",
    "            cls._calib = {}\n",
    "        if seq not in cls._calib:\n",
    "            cls._calib[seq] = get_calibration(seq)\n",
    "        return cls._calib[seq]\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_poses(cls, seq):\n",
    "        if not hasattr(cls, '_poses'):\n",
    "            cls._poses = {}\n",
    "        if seq not in cls._poses:\n",
    "            cls._poses[seq] = get_poses(seq)\n",
    "        return cls._poses[seq]\n",
    "    \n",
    "    @classmethod\n",
    "    def create_camera_frame(cls, base_uri, scan_id):\n",
    "        seq = base_uri.segment_id\n",
    "        calib = cls._get_calib(seq)\n",
    "        \n",
    "        uri = copy.deepcopy(base_uri)\n",
    "        uri.topic = 'camera|left_rect'\n",
    "        uri.timestamp = int(scan_id) # HACK!\n",
    "\n",
    "        scene_base = get_scene_basepath(seq)\n",
    "        scan_name = str(scan_id).rjust(6, '0')\n",
    "        img_path = os.path.join(scene_base, 'image_2/', scan_name + '.png')\n",
    "        assert os.path.exists(img_path), (\n",
    "            \"Did you remember to expand data_odometry_color.zip ? %s not found\" % img_path)\n",
    "        with open(img_path, 'rb') as f:\n",
    "            width, height = util.get_png_wh(f.read(100)) # Util only needs the first few bytes\n",
    "        \n",
    "        import imageio\n",
    "        image_factory = lambda: imageio.imread(img_path)\n",
    "        \n",
    "        # HACK!!!  This is actually P !!!\n",
    "        K = calib['P2']\n",
    "        \n",
    "        # hack! this is lidar to cam\n",
    "        ego_to_sensor = datum.Transform.from_transformation_matrix(\n",
    "                calib['Tr'], src_frame='lidar', dest_frame=uri.topic)\n",
    "        \n",
    "        sd_ego_pose = cls.create_ego_pose(base_uri, scan_id)\n",
    "        ego_pose = sd_ego_pose.transform\n",
    "        ci = datum.CameraImage(\n",
    "              sensor_name=uri.topic,\n",
    "              image_factory=image_factory,\n",
    "              width=width,\n",
    "              height=height,\n",
    "              timestamp=uri.timestamp,\n",
    "              ego_pose=ego_pose,\n",
    "              K=K,\n",
    "              ego_to_sensor=ego_to_sensor,\n",
    "              extra={'semantic_kitti.scan_id': str(scan_id)})\n",
    "        return datum.StampedDatum(uri=uri, camera_image=ci)\n",
    "    \n",
    "    @classmethod\n",
    "    def create_ego_pose(cls, base_uri, scan_id):\n",
    "        seq = base_uri.segment_id\n",
    "        poses = cls._get_poses(seq)\n",
    "        \n",
    "        uri = copy.deepcopy(base_uri)\n",
    "        uri.topic = 'ego_pose'\n",
    "        uri.timestamp = int(scan_id) # HACK!\n",
    "        \n",
    "        # Hack! believe ego frame is lidar here?\n",
    "        ego_pose = datum.Transform.from_transformation_matrix(\n",
    "                poses[scan_id], src_frame='world', dest_frame='ego')\n",
    "\n",
    "        return datum.StampedDatum(uri=uri, transform=ego_pose)\n",
    "    \n",
    "    @classmethod\n",
    "    def create_point_cloud_in_world(cls, base_uri, scan_id):\n",
    "        \n",
    "        uri = copy.deepcopy(base_uri)\n",
    "        uri.topic = 'lidar|world' + ('_cleaned' if cls.ONLY_FRAMES_WITH_NO_MOVERS else '')\n",
    "        uri.timestamp = int(scan_id) # HACK!\n",
    "        \n",
    "        sd_ego_pose = cls.create_ego_pose(base_uri, scan_id)\n",
    "        ego_pose = sd_ego_pose.transform\n",
    "        \n",
    "        def _get_cloud(seq, sid):\n",
    "            cloud = read_scan_get_cloud(\n",
    "                        seq,\n",
    "                        sid,\n",
    "                        remove_movers=cls.ONLY_FRAMES_WITH_NO_MOVERS)\n",
    "            \n",
    "            # Move cloud into the world frame\n",
    "            calib = cls._get_calib(seq)\n",
    "            all_poses = cls._get_poses(seq)\n",
    "            Tr = calib[\"Tr\"]\n",
    "            Tr_inv = np.linalg.inv(Tr)\n",
    "            cam2_pose = all_poses[sid]\n",
    "            pose = np.matmul(Tr_inv, np.matmul(cam2_pose, Tr))\n",
    "            cloud = np.matmul(pose, cloud.T).T\n",
    "            \n",
    "            return cloud\n",
    "\n",
    "        pc = datum.PointCloud(\n",
    "          sensor_name=uri.topic,\n",
    "          timestamp=uri.timestamp,\n",
    "          cloud_factory=lambda: _get_cloud(base_uri.segment_id, scan_id),\n",
    "          ego_to_sensor=datum.Transform(), # Hack! cloud is in world frame\n",
    "          ego_pose=ego_pose,\n",
    "          extra={'semantic_kitti.scan_id': str(scan_id)})\n",
    "        return datum.StampedDatum(uri=uri, point_cloud=pc)\n",
    "        \n",
    "\n",
    "seg_uris = SemanticKITTISDTable.get_all_segment_uris()\n",
    "# sd_rdd = SemanticKITTISDTable._get_segment_datum_rdd_or_df(spark, seg_uris[0])\n",
    "# print(sd_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-11 09:51:44,067\tps   17241 : Filtering to only 1 segments\n",
      "INFO - 2021-02-11 09:51:44,067 - fused_lidar - Filtering to only 1 segments\n",
      "2021-02-11 09:51:44,068\tps   17241 : SemanticKITTIFusedWorldCloudTable building fused world clouds ...\n",
      "INFO - 2021-02-11 09:51:44,068 - fused_lidar - SemanticKITTIFusedWorldCloudTable building fused world clouds ...\n",
      "2021-02-11 09:51:44,069\tps   17241 : ... have 1 segments to fuse ...\n",
      "INFO - 2021-02-11 09:51:44,069 - fused_lidar - ... have 1 segments to fuse ...\n",
      "2021-02-11 09:51:44,070\tps   17241 : ... working on 00 ...\n",
      "INFO - 2021-02-11 09:51:44,070 - fused_lidar - ... working on 00 ...\n",
      "2021-02-11 09:51:44,071\tps   17241 : ... have fused cloud; skipping! /opt/psegs/dataroot/fused_world_clouds/naive_cuboid_scrubber/semantikitti-psegs-fused/train/00/fused_world.ply\n",
      "INFO - 2021-02-11 09:51:44,071 - fused_lidar - ... have fused cloud; skipping! /opt/psegs/dataroot/fused_world_clouds/naive_cuboid_scrubber/semantikitti-psegs-fused/train/00/fused_world.ply\n",
      "2021-02-11 09:51:44,072\tps   17241 : ... SemanticKITTIFusedWorldCloudTable done fusing clouds.\n",
      "INFO - 2021-02-11 09:51:44,072 - fused_lidar - ... SemanticKITTIFusedWorldCloudTable done fusing clouds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from psegs.exp.fused_lidar import FusedWorldCloudTableBase\n",
    "\n",
    "class SemanticKITTIFusedWorldCloudTable(FusedWorldCloudTableBase):\n",
    "    SRC_SD_TABLE = SemanticKITTISDTable\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "        seg_rdd = cls.SRC_SD_TABLE.get_segment_datum_rdd(spark, segment_uri)\n",
    "        \n",
    "        # SemanticKITTI has no cuboids, so the Fuser algo simply concats the cloud points\n",
    "        def iter_task_rows(iter_sds):\n",
    "            from pyspark import Row\n",
    "            from oarphpy.spark import RowAdapter\n",
    "            for sd in iter_sds:\n",
    "                if sd.point_cloud is not None:\n",
    "                    pc = sd.point_cloud\n",
    "                    task_id = \"%s.%s\" % (sd.uri.segment_id, pc.extra['semantic_kitti.scan_id'])\n",
    "                    yield Row(\n",
    "                        task_id=task_id,\n",
    "                        point_clouds=[pc],\n",
    "                        cuboids=[])\n",
    "        \n",
    "        task_rdd = seg_rdd.mapPartitions(iter_task_rows)\n",
    "        return task_rdd\n",
    "        \n",
    "seg_uris = SemanticKITTIFusedWorldCloudTable.get_all_segment_uris()\n",
    "sd_rdd = SemanticKITTIFusedWorldCloudTable._get_segment_datum_rdd_or_df(spark, seg_uris[0])\n",
    "print(sd_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
