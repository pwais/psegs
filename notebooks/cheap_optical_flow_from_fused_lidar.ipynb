{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Optical Flow from Fused Lidar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/psegs')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from psegs.exp.fused_lidar_flow import FusedLidarCloudTableBase\n",
    "from psegs.exp.fused_lidar_flow import TaskLidarCuboidCameraDFFactory\n",
    "from psegs.exp.fused_lidar_flow import OpticalFlowRenderBase\n",
    "\n",
    "import IPython.display\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "## General Notebook Utilities\n",
    "    \n",
    "def imshow(x):\n",
    "    IPython.display.display(PIL.Image.fromarray(x))\n",
    "\n",
    "def show_html(x):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemanticKITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.exp.semantic_kitti import SemanticKITTISDTable\n",
    "\n",
    "class SemanticKITTILCCDFFactory(TaskLidarCuboidCameraDFFactory):\n",
    "    \n",
    "    SRC_SD_TABLE = SemanticKITTISDTable\n",
    "    \n",
    "    @classmethod\n",
    "    def build_df_for_segment(cls, spark, segment_uri):\n",
    "        seg_rdd = cls.SRC_SD_TABLE.get_segment_datum_rdd(spark, segment_uri)\n",
    "        \n",
    "        def to_task_row(scan_id_iter_sds):\n",
    "            scan_id, iter_sds = scan_id_iter_sds\n",
    "            camera_images = []\n",
    "            point_clouds = []\n",
    "            for sd in iter_sds:\n",
    "                if sd.camera_image is not None:\n",
    "                    camera_images.append(sd)\n",
    "                elif sd.point_cloud is not None:\n",
    "                    point_clouds.append(sd)\n",
    "            \n",
    "            from pyspark import Row\n",
    "            r = Row(\n",
    "                    task_id=int(scan_id),\n",
    "                    pc_sds=point_clouds,\n",
    "                    cuboids_sds=[], # SemanticKITTI has no cuboids\n",
    "                    ci_sds=camera_images) \n",
    "            from oarphpy.spark import RowAdapter\n",
    "            return RowAdapter.to_row(r)\n",
    "            \n",
    "        grouped = seg_rdd.groupBy(lambda sd: sd.uri.extra['semantic_kitti.scan_id'])\n",
    "        row_rdd = grouped.map(to_task_row)\n",
    "\n",
    "        df = spark.createDataFrame(row_rdd, schema=cls.table_schema())\n",
    "        df = df.persist()\n",
    "        return df\n",
    "\n",
    "class SemanticKITTIFusedWorldCloudTable(FusedLidarCloudTableBase):\n",
    "    TASK_DF_FACTORY = SemanticKITTILCCDFFactory\n",
    "\n",
    "    # SemanticKITTI has no cuboids, so we skip this step.\n",
    "    HAS_OBJ_CLOUDS = False\n",
    "\n",
    "class SemanticKITTIOFlowRenderer(OpticalFlowRenderBase):\n",
    "    FUSED_LIDAR_SD_TABLE = SemanticKITTIFusedWorldCloudTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI-360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.datasets.kitti_360 import KITTI360SDTable\n",
    "class KITTI360OurFusedClouds(KITTI360SDTable):\n",
    "    INCLUDE_FISHEYES = False\n",
    "    INCLUDE_FUSED_CLOUDS = False  # Use our own fused clouds\n",
    "\n",
    "class KITTI360OurFusedWorldCloudTable(FusedLidarCloudTableBase):\n",
    "    SRC_SD_TABLE = KITTI360OurFusedClouds\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "        datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "        datum_df.registerTempTable('datums')\n",
    "        spark.catalog.dropTempView('culi_tasks_df')\n",
    "        print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        spark.sql(\"\"\"\n",
    "          CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "          SELECT \n",
    "              CONCAT(uri.segment_id, '.', uri.extra.`kitti-360.frame_id`) AS task_id,\n",
    "              FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "              COLLECT_LIST(point_cloud) AS point_clouds\n",
    "          FROM datums\n",
    "          WHERE \n",
    "              uri.topic LIKE '%cuboid%' OR uri.topic LIKE '%lidar%'\n",
    "          GROUP BY task_id\n",
    "        \"\"\")\n",
    "        \n",
    "        \n",
    "        # TODO! for lidar and camera image!\n",
    "        #         both_have_ego_pose = (\n",
    "        #             ci1.extra.get('kitti-360.has-valid-ego-pose') and\n",
    "        #             ci2.extra.get('kitti-360.has-valid-ego-pose'))\n",
    "        \n",
    "        tasks_df = spark.sql('SELECT * FROM culi_tasks_df')\n",
    "        print('... done.')\n",
    "        return tasks_df.rdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NuScenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip3 install nuscenes-devkit==1.1.2\n",
    "from psegs.datasets.nuscenes import NuscStampedDatumTableBase\n",
    "from psegs.datasets.nuscenes import NuscStampedDatumTableLabelsAllFrames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NuscKFOnlyLCCDFFactory(TaskLidarCuboidCameraDFFactory):\n",
    "    \n",
    "    SRC_SD_TABLE = NuscStampedDatumTableBase\n",
    "    \n",
    "    @classmethod\n",
    "    def build_df_for_segment(cls, spark, segment_uri):\n",
    "        datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "        datum_df.registerTempTable('datums')\n",
    "        spark.catalog.dropTempView('nusc_task_df')\n",
    "        print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        \n",
    "        # Nusc doesn't have numerical task_ids so we'll have to induce\n",
    "        # one via lidar timestamp.\n",
    "        # NB: for Nusc: can group by nuscenes-sample-token FOR KEYFRAMES-ONLY DATA\n",
    "        task_data_df = spark.sql(\"\"\"\n",
    "            SELECT \n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds,\n",
    "              MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS lidar_time,\n",
    "              FIRST(uri.extra.`nuscenes-sample-token`) AS sample_token\n",
    "            FROM datums\n",
    "            WHERE \n",
    "            uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "              uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "              uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "            ) AND (\n",
    "              uri.topic LIKE '%cuboid%' OR\n",
    "              uri.topic LIKE '%lidar%' OR\n",
    "              uri.topic LIKE '%camera%'\n",
    "            )\n",
    "            GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "            ORDER BY lidar_time\n",
    "        \"\"\")\n",
    "        sample_tokens_ordered = [r.sample_token for r in df.select('sample_token').collect()]\n",
    "        task_to_stoken = [\n",
    "            {'task_id': task_id, 'sample_token': sample_token}\n",
    "            for task_id, sample_token in enumerate(sample_tokens_ordered)\n",
    "        ]\n",
    "        task_id_rdd = spark.sparkContext.parallelize(task_to_stoken)\n",
    "        task_id_df = spark.createDataFrame(task_id_rdd)\n",
    "        tasks_df = df.join(task_id_df, on=['sample_token'], how='inner')\n",
    "        tasks_df = tasks_df.persist()\n",
    "        print('... done.')\n",
    "        return tasks_df\n",
    "\n",
    "\n",
    "class NuscWorldCloudTableBase(FusedLidarCloudTableBase):\n",
    "    SPLITS = ['train_detect', 'train_track']\n",
    "    \n",
    "    @classmethod\n",
    "    def _filter_ego_vehicle(cls, cloud_ego):\n",
    "        # Note: NuScenes authors have already corrected clouds for ego motion:\n",
    "        # https://github.com/nutonomy/nuscenes-devkit/issues/481#issuecomment-716250423\n",
    "        # But have not filtered out ego self-returns\n",
    "        cloud_ego = cloud_ego[np.where(  ~(\n",
    "                        (cloud_ego[:, 0] <= 1.5) & (cloud_ego[:, 0] >= -1.5) &  # Nusc lidar +x is +right\n",
    "                        (cloud_ego[:, 1] <= 2.5) & (cloud_ego[:, 0] >= -2.5) &  # Nusc lidar +y is +forward\n",
    "                        (cloud_ego[:, 1] <= 1.5) & (cloud_ego[:, 0] >= -1.5)    # Nusc lidar +z is +up\n",
    "        ))]\n",
    "        return cloud_ego\n",
    "    \n",
    "class NuscKFOnlyFusedWorldCloudTable(NuscWorldCloudTableBase):\n",
    "    TASK_DF_FACTORY = NuscKFOnlyLCCDFFactory\n",
    "    \n",
    "    \n",
    "    \n",
    "#     task_id=int(scan_id),\n",
    "#                     pc_sds=point_clouds,\n",
    "#                     cuboids_sds=[], # SemanticKITTI has no cuboids\n",
    "#                     ci_sds=camera_images\n",
    "#     @classmethod\n",
    "#     def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "#         datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "#         datum_df.registerTempTable('datums')\n",
    "#         spark.catalog.dropTempView('nusc_task_df')\n",
    "#         print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        \n",
    "#         # Nusc doesn't have numerical task_ids so we'll have to induce\n",
    "#         # one via lidar timestamp.\n",
    "#         if cls.SRC_SD_TABLE.LABELS_KEYFRAMES_ONLY:\n",
    "#             # For Nusc: group by nuscenes-sample-token WITH KEYFRAMES\n",
    "#             spark.sql(\"\"\"\n",
    "#               CACHE TABLE nusc_task_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#               SELECT \n",
    "#                   MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS task_id,\n",
    "#                   COLLECT_LIST(*) FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#                   COLLECT_LIST(*) FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#                   COLLECT_LIST(*) FILTER (WHERE uri.topic LIKE '%cam%') AS ci_sds\n",
    "#               FROM datums\n",
    "#               WHERE \n",
    "#                 uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "#                   uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#                   uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%' OR\n",
    "#                   uri.extra['nuscenes-label-channel'] LIKE '%CAM%'\n",
    "#                 ) AND (\n",
    "#                   uri.topic LIKE '%cuboid%' OR\n",
    "#                   uri.topic LIKE '%lidar%' OR\n",
    "#                   uri.topic LIKE '%cam%'\n",
    "#                 )\n",
    "#               GROUP BY task_id\n",
    "#             \"\"\")\n",
    "#         else:\n",
    "#             # For Nusc: group by nuscenes-sample-token WITH ALL FRAMES\n",
    "#             spark.sql(\"\"\"\n",
    "#               CACHE TABLE nusc_task_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#               SELECT \n",
    "#                   CONCAT(uri.segment_id, '.', uri.timestamp) AS task_id,\n",
    "#                   FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "#                   COLLECT_LIST(point_cloud) AS point_clouds\n",
    "#               FROM datums\n",
    "#               WHERE \n",
    "#                 (\n",
    "#                   uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#                   uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#                 ) AND (\n",
    "#                   uri.topic LIKE '%cuboid%' OR\n",
    "#                   uri.topic LIKE '%lidar%'\n",
    "#                 )\n",
    "#               GROUP BY task_id\n",
    "#               HAVING SIZE(cuboids) > 0 AND SIZE(point_clouds) > 0\n",
    "#             \"\"\")\n",
    "# #             spark.sql(\"\"\"\n",
    "# #               CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "# #               SELECT \n",
    "# #                   CONCAT(uri.segment_id, '.', uri.timestamp) AS task_id,\n",
    "# #                   FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "# #                   COLLECT_LIST(point_cloud) AS point_clouds\n",
    "# #               FROM datums\n",
    "# #               WHERE \n",
    "# #                 (\n",
    "# #                   uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "# #                   uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "# #                 ) AND (\n",
    "# #                   uri.topic LIKE '%cuboid%' OR\n",
    "# #                   uri.topic LIKE '%lidar%'\n",
    "# #                 )\n",
    "# #               GROUP BY task_id\n",
    "# #               HAVING SIZE(cuboids) > 0 AND SIZE(point_clouds) > 0\n",
    "# #             \"\"\")\n",
    "        \n",
    "#         tasks_df = spark.sql('SELECT * FROM nusc_task_df')\n",
    "#         print('... done.')\n",
    "#         return tasks_df.rdd\n",
    "\n",
    "# class NuscFusedWorldCloudKeyframesOnlyTable(NuscFusedWorldCloudTableBase):\n",
    "#     SRC_SD_TABLE = NuscStampedDatumTableBase\n",
    "\n",
    "# class NuscFusedWorldCloudAllFramesTable(NuscFusedWorldCloudTableBase):\n",
    "#     SRC_SD_TABLE = NuscStampedDatumTableLabelsAllFrames\n",
    "    \n",
    "class NuscKeyframesOFlowRenderer(OpticalFlowRenderBase):\n",
    "    FUSED_LIDAR_SD_TABLE = NuscKFOnlyFusedWorldCloudTable\n",
    "\n",
    "# class NuscAllFramesOFlowRenderer(OpticalFlowRenderBase):\n",
    "#     FUSED_LIDAR_SD_TABLE = NuscFusedWorldCloudAllFramesTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-21 22:55:27,893\toarph 3599341 : Using source root /opt/psegs/psegs \n",
      "2021-02-21 22:55:27,894\toarph 3599341 : Using source root /opt/psegs \n",
      "2021-02-21 22:55:27,978\toarph 3599341 : Generating egg to /tmp/tmp65b2h99d_oarphpy_eggbuild ...\n",
      "2021-02-21 22:55:28,061\toarph 3599341 : ... done.  Egg at /tmp/tmp65b2h99d_oarphpy_eggbuild/psegs-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from psegs.spark import NBSpark\n",
    "spark = NBSpark.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fused Lidar Assets\n",
    "\n",
    "```\n",
    "docker --context default run -it --name=potree_viewer --rm --net=host -v `pwd`:/shared  jonazpiazu/potree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# T = KITTI360OurFusedWorldCloudTable\n",
    "# rdds = T._create_datum_rdds(spark)\n",
    "# print([r.count() for r in rdds])\n",
    "\n",
    "# seg_uris = T.get_all_segment_uris()\n",
    "# samp = T.get_sample(seg_uris[0], spark=spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print([lc.sensor_name for lc in samp.lidar_clouds][:10])\n",
    "# c = samp.lidar_clouds[0]#[lc for lc in samp.lidar_clouds if lc.sensor_name == '11002'][0]\n",
    "# print(c.get_cloud().shape)\n",
    "# imshow(c.get_bev_debug_image(x_bounds_meters=None, y_bounds_meters=None))\n",
    "# imshow(c.get_front_rv_debug_image(y_bounds_meters=None, z_bounds_meters=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Candidate Optical Flow Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-21 23:24:52,058\tps   3599341 : Filtering to only 1 segments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tasks table for scene-1013 ...\n",
      "... done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-21 23:25:08,608\tps   3599341 : Filtering to only 1 segments\n",
      "2021-02-21 23:25:08,609\tps   3599341 : NuscKFOnlyFusedWorldCloudTable building fused clouds ...\n",
      "2021-02-21 23:25:08,610\tps   3599341 : ... have 1 segments to fuse ...\n",
      "2021-02-21 23:25:08,611\tps   3599341 : ... working on scene-1013 ...\n",
      "2021-02-21 23:25:08,612\tps   3599341 : ... skipping scene-1013; world and obj clouds done\n",
      "2021-02-21 23:25:08,613\tps   3599341 : World Cloud: /opt/psegs/dataroot/fused_world_clouds/naive_cuboid_scrubber/nuscenes-Lkfo+lseg/train_detect/scene-1013/fused_world.ply\n",
      "2021-02-21 23:25:08,613\tps   3599341 : Obj Clouds: /opt/psegs/dataroot/fused_obj_clouds/naive_cuboid_scrubber/nuscenes-Lkfo+lseg/train_detect/scene-1013\n",
      "2021-02-21 23:25:08,616\toarph 3599341 : Progress for \n",
      "FuseEachSegment [Pid:3599341 Id:139725201085104]\n",
      "-----------------------  ---------------\n",
      "Thruput\n",
      "N thru                   1 (of 1)\n",
      "N chunks                 1\n",
      "Total time               0 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       339\n",
      "Progress\n",
      "Percent Complete         100.000000\n",
      "Est. Time To Completion  0 seconds\n",
      "-----------------------  ---------------\n",
      "2021-02-21 23:25:08,616\tps   3599341 : ... NuscKFOnlyFusedWorldCloudTable done fusing clouds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tasks 40\n",
      "running map.. count\n"
     ]
    }
   ],
   "source": [
    "R = NuscKeyframesOFlowRenderer\n",
    "seg_uris = R.FUSED_LIDAR_SD_TABLE.get_all_segment_uris()\n",
    "R.build(spark=spark, only_segments=[seg_uris[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-------------------+-------+\n",
      "|        sample_token|              pc_sds|         cuboids_sds|              ci_sds|         lidar_time|task_id|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------------+-------+\n",
      "|9733299c72954c6fb...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193832147652000|      0|\n",
      "|8928669895174b1ea...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193832647546000|      1|\n",
      "|e5e8907e7dca4f6ea...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193833197705000|      2|\n",
      "|3a1fadfc6bfa45d49...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193833698143000|      3|\n",
      "|8801fd25ac25490db...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193834198008000|      4|\n",
      "|126177e74e854a329...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193834698455000|      5|\n",
      "|846fa9fbaee341a6a...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193835197796000|      6|\n",
      "|9c52b49d0bc140fa8...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193835697651000|      7|\n",
      "|b70a335095d54065b...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193836198087000|      8|\n",
      "|b1d9c91e80a84b96b...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193836697944000|      9|\n",
      "|6bb906191a504a6b8...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193837197287000|     10|\n",
      "|e355e5b3f2fb4f9ca...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193837697166000|     11|\n",
      "|a6dfc100c19343a38...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193838197620000|     12|\n",
      "|7b841e9060c94796b...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193838698571000|     13|\n",
      "|f4ab1e7a0e0d4e019...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193839199005000|     14|\n",
      "|cf6c052977c14977b...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193839649099000|     15|\n",
      "|387a4b2fb31545e6a...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193840148428000|     16|\n",
      "|b4f143b026ed4b46a...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193840648849000|     17|\n",
      "|5eaf50adc08b44c0a...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193841198501000|     18|\n",
      "|dde9c125f13641d6b...|[[psegs.datum.sta...|[[psegs.datum.sta...|[[psegs.datum.sta...|1542193841698375000|     19|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assert False, spark.sql(\"\"\"select uri.topic t from datums group by t\"\"\").show(truncate=False)\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "#           CACHE TABLE nusc_task_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#           SELECT \n",
    "#               MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS task_id,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) FILTER (WHERE uri.topic LIKE '%cam%') AS ci_sds\n",
    "#           FROM datums\n",
    "#           WHERE \n",
    "#             uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "#               uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#               uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#             ) AND (\n",
    "#               uri.topic LIKE '%cuboid%' OR\n",
    "#               uri.topic LIKE '%lidar%' OR\n",
    "#               uri.topic LIKE '%camera%'\n",
    "#             )\n",
    "#           GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "#         \"\"\").show()\n",
    "# df = spark.sql(\"\"\"\n",
    "#           SELECT \n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds,\n",
    "#               MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS lidar_time,\n",
    "#               FIRST(uri.extra.`nuscenes-sample-token`) AS sample_token\n",
    "#           FROM datums\n",
    "#           WHERE \n",
    "#             uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "#               uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#               uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#             ) AND (\n",
    "#               uri.topic LIKE '%cuboid%' OR\n",
    "#               uri.topic LIKE '%lidar%' OR\n",
    "#               uri.topic LIKE '%camera%'\n",
    "#             )\n",
    "#           GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "#           ORDER BY lidar_time\n",
    "#         \"\"\")\n",
    "# sample_tokens_ordered = [r.sample_token for r in df.select('sample_token').collect()]\n",
    "# task_to_stoken = [\n",
    "#     {'task_id': task_id, 'sample_token': sample_token}\n",
    "#     for task_id, sample_token in enumerate(sample_tokens_ordered)\n",
    "# ]\n",
    "# task_id_rdd = spark.sparkContext.parallelize(task_to_stoken)\n",
    "# task_id_df = spark.createDataFrame(task_id_rdd)\n",
    "# df.join(task_id_df, on=['sample_token'], how='inner').orderBy('task_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
