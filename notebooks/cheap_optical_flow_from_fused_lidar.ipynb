{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Optical Flow from Fused Lidar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/psegs')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from psegs.exp.fused_lidar import FusedLidarCloudTableBase\n",
    "\n",
    "import IPython.display\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "## General Notebook Utilities\n",
    "    \n",
    "def imshow(x):\n",
    "    IPython.display.display(PIL.Image.fromarray(x))\n",
    "\n",
    "def show_html(x):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemanticKITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.exp.semantic_kitti import SemanticKITTISDTable\n",
    "class SemanticKITTIFusedWorldCloudTable(FusedLidarCloudTableBase):\n",
    "    SRC_SD_TABLE = SemanticKITTISDTable\n",
    "\n",
    "    # SemanticKITTI has no cuboids, so we skip this step.\n",
    "    HAS_OBJ_CLOUDS = False\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "        seg_rdd = cls.SRC_SD_TABLE.get_segment_datum_rdd(spark, segment_uri)\n",
    "        \n",
    "        # SemanticKITTI has no cuboids, so the Fuser algo simply concats the cloud points\n",
    "        def iter_task_rows(iter_sds):\n",
    "            from pyspark import Row\n",
    "            from oarphpy.spark import RowAdapter\n",
    "            for sd in iter_sds:\n",
    "                if sd.point_cloud is not None:\n",
    "                    pc = sd.point_cloud\n",
    "                    task_id = \"%s.%s\" % (sd.uri.segment_id, pc.extra['semantic_kitti.scan_id'])\n",
    "                    yield Row(\n",
    "                        task_id=task_id,\n",
    "                        point_clouds=[pc],\n",
    "                        cuboids=[])\n",
    "        \n",
    "        task_rdd = seg_rdd.mapPartitions(iter_task_rows)\n",
    "        import pyspark\n",
    "        task_rdd = task_rdd.persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "        return task_rdd\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI-360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.datasets.kitti_360 import KITTI360SDTable\n",
    "class KITTI360OurFusedClouds(KITTI360SDTable):\n",
    "    INCLUDE_FISHEYES = False\n",
    "    INCLUDE_FUSED_CLOUDS = False  # Use our own fused clouds\n",
    "\n",
    "class KITTI360OurFusedWorldCloudTable(FusedLidarCloudTableBase):\n",
    "    SRC_SD_TABLE = KITTI360OurFusedClouds\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "        datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "        datum_df.registerTempTable('datums')\n",
    "        spark.catalog.dropTempView('culi_tasks_df')\n",
    "        print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        spark.sql(\"\"\"\n",
    "          CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "          SELECT \n",
    "              CONCAT(uri.segment_id, '.', uri.extra.`kitti-360.frame_id`) AS task_id,\n",
    "              FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "              COLLECT_LIST(point_cloud) AS point_clouds\n",
    "          FROM datums\n",
    "          WHERE \n",
    "              uri.topic LIKE '%cuboid%' OR uri.topic LIKE '%lidar%'\n",
    "          GROUP BY task_id\n",
    "          LIMIT 100\n",
    "        \"\"\")\n",
    "        tasks_df = spark.sql('SELECT * FROM culi_tasks_df')\n",
    "        print('... done.')\n",
    "        return tasks_df.rdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NuScenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip3 install nuscenes-devkit==1.1.2\n",
    "from psegs.datasets.nuscenes import NuscStampedDatumTableBase, NuscStampedDatumTableLabelsAllFrames\n",
    "class NuscFusedWorldCloudTable(FusedLidarCloudTableBase):\n",
    "    SRC_SD_TABLE = NuscStampedDatumTableBase\n",
    "    \n",
    "    SPLITS = ['train_detect', 'train_track']\n",
    "    \n",
    "    @classmethod\n",
    "    def _filter_ego_vehicle(cls, cloud_ego):\n",
    "        cloud_ego = cloud_ego[np.where(  ~(\n",
    "                        (cloud_ego[:, 0] <= 1.5) & (cloud_ego[:, 0] >= -1.5) &  # Nusc lidar +x is +right\n",
    "                        (cloud_ego[:, 1] <= 2.5) & (cloud_ego[:, 0] >= -2.5) &  # Nusc lidar +y is +forward\n",
    "                        (cloud_ego[:, 1] <= 1.5) & (cloud_ego[:, 0] >= -1.5)   # Nusc lidar +z is +up\n",
    "        ))]\n",
    "        return cloud_ego\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "        datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "        datum_df.registerTempTable('datums')\n",
    "        spark.catalog.dropTempView('culi_tasks_df')\n",
    "        print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        if cls.SRC_SD_TABLE.LABELS_KEYFRAMES_ONLY:\n",
    "            # For Nusc: group by nuscenes-sample-token WITH KEYFRAMES\n",
    "            spark.sql(\"\"\"\n",
    "              CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "              SELECT \n",
    "                  uri.extra.`nuscenes-sample-token` AS task_id,\n",
    "                  FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "                  COLLECT_LIST(point_cloud) AS point_clouds\n",
    "              FROM datums\n",
    "              WHERE \n",
    "                uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "                  uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "                  uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "                ) AND (\n",
    "                  uri.topic LIKE '%cuboid%' OR\n",
    "                  uri.topic LIKE '%lidar%'\n",
    "                )\n",
    "              GROUP BY task_id\n",
    "            \"\"\")\n",
    "        else:\n",
    "            # For Nusc: group by nuscenes-sample-token WITH ALL FRAMES\n",
    "            spark.sql(\"\"\"\n",
    "              CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "              SELECT \n",
    "                  CONCAT(uri.segment_id, '.', uri.timestamp) AS task_id,\n",
    "                  FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "                  COLLECT_LIST(point_cloud) AS point_clouds\n",
    "              FROM datums\n",
    "              WHERE \n",
    "                (\n",
    "                  uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "                  uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "                ) AND (\n",
    "                  uri.topic LIKE '%cuboid%' OR\n",
    "                  uri.topic LIKE '%lidar%'\n",
    "                )\n",
    "              GROUP BY task_id\n",
    "              HAVING SIZE(cuboids) > 0 AND SIZE(point_clouds) > 0\n",
    "            \"\"\")\n",
    "        \n",
    "        tasks_df = spark.sql('SELECT * FROM culi_tasks_df')\n",
    "        print('... done.')\n",
    "        return tasks_df.rdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-13 21:38:42,885\toarph 726314 : Using source root /opt/psegs/psegs \n",
      "2021-02-13 21:38:42,886\toarph 726314 : Using source root /opt/psegs \n",
      "2021-02-13 21:38:42,930\toarph 726314 : Generating egg to /tmp/tmpr_ovivrq_oarphpy_eggbuild ...\n",
      "2021-02-13 21:38:43,010\toarph 726314 : ... done.  Egg at /tmp/tmpr_ovivrq_oarphpy_eggbuild/psegs-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from psegs.spark import NBSpark\n",
    "spark = NBSpark.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fused Lidar Assets\n",
    "\n",
    "```\n",
    "docker --context default run -it --name=potree_viewer --rm --net=host -v `pwd`:/shared  jonazpiazu/potree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-13 21:38:46,162\tps   726314 : Filtering to only 1 segments\n",
      "2021-02-13 21:38:46,163\tps   726314 : NuscFusedWorldCloudTable building fused clouds ...\n",
      "2021-02-13 21:38:46,164\tps   726314 : ... have 1 segments to fuse ...\n",
      "2021-02-13 21:38:46,164\tps   726314 : ... working on scene-0464 ...\n",
      "2021-02-13 21:38:47,229\tps   726314 : Filtering to only 1 segments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tasks table for scene-0464 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-13 21:39:06,065\tps   726314 : Building world cloud to /opt/psegs/dataroot/fused_world_clouds/naive_cuboid_scrubber/nuscenes-Lkfo+lseg/train_detect/scene-0464/fused_world.ply ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-13 21:39:06,977\tps   726314 : ... have 41 fusion tasks ...\n",
      "2021-02-13 21:41:04,179\tps   726314 : ... computed world cloud for scene-0464 of shape (796491, 3) (0.02 GB) ...\n",
      "2021-02-13 21:41:04,180\tps   726314 : ... writing ply to /opt/psegs/dataroot/fused_world_clouds/naive_cuboid_scrubber/nuscenes-Lkfo+lseg/train_detect/scene-0464/fused_world.ply ...\n",
      "2021-02-13 21:41:05,068\tps   726314 : ... done writing ply.\n",
      "INFO - 2021-02-13 21:41:05,068 - fused_lidar - ... done writing ply.\n",
      "2021-02-13 21:41:05,075\toarph 726314 : \n",
      "ComputeWorldClouds [Pid:726314 Id:140426301035712]\n",
      "-----------------------  --------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   41 (of 41)\n",
      "N chunks                 42\n",
      "Total time               1 minute and 58.07 seconds\n",
      "Total thru               19.12 MB\n",
      "Rate                     161.9 KB / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         100.000000\n",
      "Est. Time To Completion  0 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      2 seconds, 811 milliseconds, 304 microseconds and 983.64 nanoseconds\n",
      "p50                      2 seconds, 217 milliseconds, 489 microseconds and 600.18 nanoseconds\n",
      "p95                      9 seconds, 134 milliseconds, 800 microseconds and 517.56 nanoseconds\n",
      "p99                      12 seconds, 875 milliseconds, 499 microseconds and 43.46 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------\n",
      "\n",
      "INFO - 2021-02-13 21:41:05,075 - thruput_observer - \n",
      "ComputeWorldClouds [Pid:726314 Id:140426301035712]\n",
      "-----------------------  --------------------------------------------------------------------\n",
      "Thruput\n",
      "N thru                   41 (of 41)\n",
      "N chunks                 42\n",
      "Total time               1 minute and 58.07 seconds\n",
      "Total thru               19.12 MB\n",
      "Rate                     161.9 KB / sec\n",
      "Hz                       0\n",
      "Progress\n",
      "Percent Complete         100.000000\n",
      "Est. Time To Completion  0 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      2 seconds, 811 milliseconds, 304 microseconds and 983.64 nanoseconds\n",
      "p50                      2 seconds, 217 milliseconds, 489 microseconds and 600.18 nanoseconds\n",
      "p95                      9 seconds, 134 milliseconds, 800 microseconds and 517.56 nanoseconds\n",
      "p99                      12 seconds, 875 milliseconds, 499 microseconds and 43.46 nanoseconds\n",
      "-----------------------  --------------------------------------------------------------------\n",
      "\n",
      "2021-02-13 21:41:05,076\tps   726314 : Pruning object clouds ...\n",
      "INFO - 2021-02-13 21:41:05,076 - fused_lidar - Pruning object clouds ...\n",
      "2021-02-13 21:42:09,871\tps   726314 : ... have 1255 clouds of 38 objects to fuse ...\n",
      "INFO - 2021-02-13 21:42:09,871 - fused_lidar - ... have 1255 clouds of 38 objects to fuse ...\n",
      "2021-02-13 21:42:09,872\tps   726314 : ... doing fusion, saving to /opt/psegs/dataroot/fused_obj_clouds/naive_cuboid_scrubber/nuscenes-Lkfo+lseg/train_detect/scene-0464 ...\n",
      "INFO - 2021-02-13 21:42:09,872 - fused_lidar - ... doing fusion, saving to /opt/psegs/dataroot/fused_obj_clouds/naive_cuboid_scrubber/nuscenes-Lkfo+lseg/train_detect/scene-0464 ...\n"
     ]
    }
   ],
   "source": [
    "T = NuscFusedWorldCloudTable\n",
    "seg_uris = T.get_all_segment_uris()\n",
    "samp = T.get_sample(seg_uris[10], spark=spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print([lc.sensor_name for lc in samp.lidar_clouds][:10])\n",
    "c = samp.lidar_clouds[0]#[lc for lc in samp.lidar_clouds if lc.sensor_name == '11002'][0]\n",
    "print(c.get_cloud().shape)\n",
    "imshow(c.get_bev_debug_image())\n",
    "imshow(c.get_front_rv_debug_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
