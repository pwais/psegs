{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Optical Flow from Fused Lidar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/psegs')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from psegs.exp.fused_lidar_flow import FusedLidarCloudTableBase\n",
    "from psegs.exp.fused_lidar_flow import TaskLidarCuboidCameraDFFactory\n",
    "from psegs.exp.fused_lidar_flow import OpticalFlowRenderBase\n",
    "\n",
    "import IPython.display\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "## General Notebook Utilities\n",
    "    \n",
    "def imshow(x):\n",
    "    IPython.display.display(PIL.Image.fromarray(x))\n",
    "\n",
    "def show_html(x):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemanticKITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.exp.semantic_kitti import SemanticKITTISDTable\n",
    "\n",
    "class SemanticKITTILCCDFFactory(TaskLidarCuboidCameraDFFactory):\n",
    "    \n",
    "    SRC_SD_TABLE = SemanticKITTISDTable\n",
    "    \n",
    "    @classmethod\n",
    "    def build_df_for_segment(cls, spark, segment_uri):\n",
    "        seg_rdd = cls.SRC_SD_TABLE.get_segment_datum_rdd(spark, segment_uri)\n",
    "        \n",
    "        def to_task_row(scan_id_iter_sds):\n",
    "            scan_id, iter_sds = scan_id_iter_sds\n",
    "            camera_images = []\n",
    "            point_clouds = []\n",
    "            for sd in iter_sds:\n",
    "                if sd.camera_image is not None:\n",
    "                    camera_images.append(sd)\n",
    "                elif sd.point_cloud is not None:\n",
    "                    point_clouds.append(sd)\n",
    "            \n",
    "            from pyspark import Row\n",
    "            r = Row(\n",
    "                    task_id=int(scan_id),\n",
    "                    pc_sds=point_clouds,\n",
    "                    cuboids_sds=[], # SemanticKITTI has no cuboids\n",
    "                    ci_sds=camera_images) \n",
    "            from oarphpy.spark import RowAdapter\n",
    "            return RowAdapter.to_row(r)\n",
    "            \n",
    "        grouped = seg_rdd.groupBy(lambda sd: sd.uri.extra['semantic_kitti.scan_id'])\n",
    "        row_rdd = grouped.map(to_task_row)\n",
    "\n",
    "        df = spark.createDataFrame(row_rdd, schema=cls.table_schema())\n",
    "        df = df.persist()\n",
    "        return df\n",
    "\n",
    "class SemanticKITTIFusedWorldCloudTable(FusedLidarCloudTableBase):\n",
    "    TASK_DF_FACTORY = SemanticKITTILCCDFFactory\n",
    "\n",
    "    # SemanticKITTI has no cuboids, so we skip this step.\n",
    "    HAS_OBJ_CLOUDS = False\n",
    "\n",
    "class SemanticKITTIOFlowRenderer(OpticalFlowRenderBase):\n",
    "    FUSED_LIDAR_SD_TABLE = SemanticKITTIFusedWorldCloudTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI-360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.datasets.kitti_360 import KITTI360SDTable\n",
    "class KITTI360OurFusedClouds(KITTI360SDTable):\n",
    "    INCLUDE_FISHEYES = False\n",
    "    INCLUDE_FUSED_CLOUDS = False  # Use our own fused clouds\n",
    "\n",
    "class KITTI360LCCDFFactory(TaskLidarCuboidCameraDFFactory):\n",
    "    \n",
    "    SRC_SD_TABLE = KITTI360OurFusedClouds\n",
    "    \n",
    "    @classmethod\n",
    "    def build_df_for_segment(cls, spark, segment_uri):\n",
    "        datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "        datum_df.registerTempTable('datums')\n",
    "        print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        \n",
    "        spark.catalog.dropTempView('kitti360_tasks_df')\n",
    "        task_data_df = spark.sql(\"\"\"\n",
    "            CACHE TABLE kitti360_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "            SELECT \n",
    "              INT(uri.extra.`kitti-360.frame_id`) AS task_id,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds\n",
    "            FROM datums\n",
    "            WHERE (\n",
    "              uri.topic LIKE '%cuboid%' OR\n",
    "              uri.topic LIKE '%lidar%' OR\n",
    "              uri.topic LIKE '%camera%'\n",
    "            ) AND (\n",
    "              camera_image is NULL OR (camera_image.extra.`kitti-360.has-valid-ego-pose` = 'True')\n",
    "            ) AND (\n",
    "              point_cloud is NULL OR (point_cloud.extra.`kitti-360.has-valid-ego-pose` = 'True')\n",
    "            )\n",
    "            GROUP BY task_id\n",
    "        \"\"\")\n",
    "        \n",
    "        tasks_df = spark.sql('SELECT * FROM kitti360_tasks_df')\n",
    "        print('... done.')\n",
    "        return tasks_df\n",
    "        \n",
    "    \n",
    "class KITTI360WorldCloudTableBase(FusedLidarCloudTableBase):\n",
    "    TASK_DF_FACTORY = KITTI360LCCDFFactory\n",
    "        \n",
    "class KITTI360OFlowRenderer(OpticalFlowRenderBase):\n",
    "    FUSED_LIDAR_SD_TABLE = KITTI360WorldCloudTableBase\n",
    "\n",
    "    \n",
    "    \n",
    "# class KITTI360OurFusedWorldCloudTable(FusedLidarCloudTableBase):\n",
    "#     SRC_SD_TABLE = KITTI360OurFusedClouds\n",
    "    \n",
    "#     @classmethod\n",
    "#     def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "#         datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "#         datum_df.registerTempTable('datums')\n",
    "#         spark.catalog.dropTempView('culi_tasks_df')\n",
    "#         print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "#         spark.sql(\"\"\"\n",
    "#           CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#           SELECT \n",
    "#               CONCAT(uri.segment_id, '.', uri.extra.`kitti-360.frame_id`) AS task_id,\n",
    "#               FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "#               COLLECT_LIST(point_cloud) AS point_clouds\n",
    "#           FROM datums\n",
    "#           WHERE \n",
    "#               uri.topic LIKE '%cuboid%' OR uri.topic LIKE '%lidar%'\n",
    "#           GROUP BY task_id\n",
    "#         \"\"\")\n",
    "        \n",
    "        \n",
    "#         # TODO! for lidar and camera image!\n",
    "#         #         both_have_ego_pose = (\n",
    "#         #             ci1.extra.get('kitti-360.has-valid-ego-pose') and\n",
    "#         #             ci2.extra.get('kitti-360.has-valid-ego-pose'))\n",
    "        \n",
    "#         tasks_df = spark.sql('SELECT * FROM culi_tasks_df')\n",
    "#         print('... done.')\n",
    "#         return tasks_df.rdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NuScenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip3 install nuscenes-devkit==1.1.2\n",
    "from psegs.datasets.nuscenes import NuscStampedDatumTableBase\n",
    "from psegs.datasets.nuscenes import NuscStampedDatumTableLabelsAllFrames\n",
    "\n",
    "\n",
    "class NuscKFOnlyLCCDFFactory(TaskLidarCuboidCameraDFFactory):\n",
    "    \n",
    "    SRC_SD_TABLE = NuscStampedDatumTableBase\n",
    "    \n",
    "    @classmethod\n",
    "    def build_df_for_segment(cls, spark, segment_uri):\n",
    "        datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "        datum_df.registerTempTable('datums')\n",
    "        print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        \n",
    "        # Nusc doesn't have numerical task_ids so we'll have to induce\n",
    "        # one via lidar timestamp.\n",
    "        # NB: for Nusc: can group by nuscenes-sample-token FOR KEYFRAMES-ONLY DATA\n",
    "        task_data_df = spark.sql(\"\"\"\n",
    "            SELECT \n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds,\n",
    "              MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS lidar_time,\n",
    "              FIRST(uri.extra.`nuscenes-sample-token`) AS sample_token\n",
    "            FROM datums\n",
    "            WHERE \n",
    "            uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "              uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "              uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "            ) AND (\n",
    "              uri.topic LIKE '%cuboid%' OR\n",
    "              uri.topic LIKE '%lidar%' OR\n",
    "              uri.topic LIKE '%camera%'\n",
    "            )\n",
    "            GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "            ORDER BY lidar_time\n",
    "        \"\"\")\n",
    "        sample_tokens_ordered = [r.sample_token for r in task_data_df.select('sample_token').collect()]\n",
    "        task_to_stoken = [\n",
    "            {'task_id': task_id, 'sample_token': sample_token}\n",
    "            for task_id, sample_token in enumerate(sample_tokens_ordered)\n",
    "        ]\n",
    "        task_id_rdd = spark.sparkContext.parallelize(task_to_stoken)\n",
    "        task_id_df = spark.createDataFrame(task_id_rdd)\n",
    "        tasks_df = task_data_df.join(task_id_df, on=['sample_token'], how='inner')\n",
    "        tasks_df = tasks_df.persist()\n",
    "        print('... done.')\n",
    "        return tasks_df\n",
    "\n",
    "\n",
    "class NuscAllFramesLCCDFFactory(TaskLidarCuboidCameraDFFactory):\n",
    "    \n",
    "    SRC_SD_TABLE = NuscStampedDatumTableLabelsAllFrames\n",
    "    \n",
    "    @classmethod\n",
    "    def build_df_for_segment(cls, spark, segment_uri):\n",
    "        datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "        datum_df.registerTempTable('datums')\n",
    "        print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        \n",
    "        task_data_df = spark.sql(\"\"\"\n",
    "            SELECT \n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds,\n",
    "              MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS lidar_time,\n",
    "              FIRST(uri.extra.`nuscenes-sample-token`) AS sample_token\n",
    "            FROM datums\n",
    "            WHERE \n",
    "            (\n",
    "              uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "              uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "            ) AND (\n",
    "              uri.topic LIKE '%cuboid%' OR\n",
    "              uri.topic LIKE '%lidar%' OR\n",
    "              uri.topic LIKE '%camera%'\n",
    "            )\n",
    "            GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "            ORDER BY lidar_time\n",
    "        \"\"\")\n",
    "        sample_tokens_ordered = [r.sample_token for r in task_data_df.select('sample_token').collect()]\n",
    "        task_to_stoken = [\n",
    "            {'task_id': task_id, 'sample_token': sample_token}\n",
    "            for task_id, sample_token in enumerate(sample_tokens_ordered)\n",
    "        ]\n",
    "        task_id_rdd = spark.sparkContext.parallelize(task_to_stoken)\n",
    "        task_id_df = spark.createDataFrame(task_id_rdd)\n",
    "        tasks_df = task_data_df.join(task_id_df, on=['sample_token'], how='inner')\n",
    "        tasks_df = tasks_df.persist()\n",
    "        print('... done.')\n",
    "        return tasks_df\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         # Nusc doesn't have numerical task_ids so we'll have to induce\n",
    "#         # one via lidar timestamp.\n",
    "#         task_data_df = spark.sql(\"\"\"\n",
    "#             SELECT \n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds,\n",
    "#               MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS lidar_time,\n",
    "#               FIRST(uri.extra.`nuscenes-sample-token`) AS sample_token\n",
    "#             FROM datums\n",
    "#             WHERE \n",
    "#              (\n",
    "#                uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#                uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#              ) AND (\n",
    "#                uri.topic LIKE '%cuboid%' OR\n",
    "#                uri.topic LIKE '%lidar%' OR\n",
    "#                uri.topic LIKE '%camera%'\n",
    "#              )\n",
    "            \n",
    "#            SELECT \n",
    "#                CONCAT(uri.segment_id, '.', uri.timestamp) AS task_id,\n",
    "#                FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "#                COLLECT_LIST(point_cloud) AS point_clouds\n",
    "#            FROM datums\n",
    "           \n",
    "#            WHERE \n",
    "#              (\n",
    "#                uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#                uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#              ) AND (\n",
    "#                uri.topic LIKE '%cuboid%' OR\n",
    "#                uri.topic LIKE '%lidar%'\n",
    "#              )\n",
    "#            GROUP BY task_id\n",
    "#            HAVING SIZE(cuboids) > 0 AND SIZE(point_clouds) > 0\n",
    "#          \"\"\")\n",
    "#         # #             spark.sql(\"\"\"\n",
    "#         # #               CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#         # #               SELECT \n",
    "#         # #                   CONCAT(uri.segment_id, '.', uri.timestamp) AS task_id,\n",
    "#         # #                   FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "#         # #                   COLLECT_LIST(point_cloud) AS point_clouds\n",
    "#         # #               FROM datums\n",
    "#         # #               WHERE \n",
    "#         # #                 (\n",
    "#         # #                   uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#         # #                   uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#         # #                 ) AND (\n",
    "#         # #                   uri.topic LIKE '%cuboid%' OR\n",
    "#         # #                   uri.topic LIKE '%lidar%'\n",
    "#         # #                 )\n",
    "#         # #               GROUP BY task_id\n",
    "#         # #               HAVING SIZE(cuboids) > 0 AND SIZE(point_clouds) > 0\n",
    "#         # #             \"\"\")\n",
    "        \n",
    "        \n",
    "        \n",
    "#         task_data_df = spark.sql(\"\"\"\n",
    "#             SELECT \n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds,\n",
    "#               MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS lidar_time,\n",
    "#               FIRST(uri.extra.`nuscenes-sample-token`) AS sample_token\n",
    "#             FROM datums\n",
    "#             WHERE \n",
    "#             uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "#               uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#               uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#             ) AND (\n",
    "#               uri.topic LIKE '%cuboid%' OR\n",
    "#               uri.topic LIKE '%lidar%' OR\n",
    "#               uri.topic LIKE '%camera%'\n",
    "#             )\n",
    "#             GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "#             ORDER BY lidar_time\n",
    "#         \"\"\")\n",
    "#         sample_tokens_ordered = [r.sample_token for r in task_data_df.select('sample_token').collect()]\n",
    "#         task_to_stoken = [\n",
    "#             {'task_id': task_id, 'sample_token': sample_token}\n",
    "#             for task_id, sample_token in enumerate(sample_tokens_ordered)\n",
    "#         ]\n",
    "#         task_id_rdd = spark.sparkContext.parallelize(task_to_stoken)\n",
    "#         task_id_df = spark.createDataFrame(task_id_rdd)\n",
    "#         tasks_df = task_data_df.join(task_id_df, on=['sample_token'], how='inner')\n",
    "#         tasks_df = tasks_df.persist()\n",
    "#         print('... done.')\n",
    "#         return tasks_df\n",
    "    \n",
    "class NuscWorldCloudTableBase(FusedLidarCloudTableBase):\n",
    "    SPLITS = ['train_detect', 'train_track']\n",
    "    \n",
    "    @classmethod\n",
    "    def _filter_ego_vehicle(cls, cloud_ego):\n",
    "        # Note: NuScenes authors have already corrected clouds for ego motion:\n",
    "        # https://github.com/nutonomy/nuscenes-devkit/issues/481#issuecomment-716250423\n",
    "        # But have not filtered out ego self-returns\n",
    "        cloud_ego = cloud_ego[np.where(  ~(\n",
    "                        (cloud_ego[:, 0] <= 1.5) & (cloud_ego[:, 0] >= -1.5) &  # Nusc lidar +x is +right\n",
    "                        (cloud_ego[:, 1] <= 2.5) & (cloud_ego[:, 0] >= -2.5) &  # Nusc lidar +y is +forward\n",
    "                        (cloud_ego[:, 1] <= 1.5) & (cloud_ego[:, 0] >= -1.5)    # Nusc lidar +z is +up\n",
    "        ))]\n",
    "        return cloud_ego\n",
    "    \n",
    "class NuscKFOnlyFusedWorldCloudTable(NuscWorldCloudTableBase):\n",
    "    TASK_DF_FACTORY = NuscKFOnlyLCCDFFactory\n",
    "\n",
    "class NuscAllFramesFusedWorldCloudTable(NuscWorldCloudTableBase):\n",
    "    TASK_DF_FACTORY = NuscAllFramesLCCDFFactory\n",
    "    \n",
    "    \n",
    "#     task_id=int(scan_id),\n",
    "#                     pc_sds=point_clouds,\n",
    "#                     cuboids_sds=[], # SemanticKITTI has no cuboids\n",
    "#                     ci_sds=camera_images\n",
    "#     @classmethod\n",
    "#     def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "#         datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "#         datum_df.registerTempTable('datums')\n",
    "#         spark.catalog.dropTempView('nusc_task_df')\n",
    "#         print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        \n",
    "#         # Nusc doesn't have numerical task_ids so we'll have to induce\n",
    "#         # one via lidar timestamp.\n",
    "#         if cls.SRC_SD_TABLE.LABELS_KEYFRAMES_ONLY:\n",
    "#             # For Nusc: group by nuscenes-sample-token WITH KEYFRAMES\n",
    "#             spark.sql(\"\"\"\n",
    "#               CACHE TABLE nusc_task_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#               SELECT \n",
    "#                   MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS task_id,\n",
    "#                   COLLECT_LIST(*) FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#                   COLLECT_LIST(*) FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#                   COLLECT_LIST(*) FILTER (WHERE uri.topic LIKE '%cam%') AS ci_sds\n",
    "#               FROM datums\n",
    "#               WHERE \n",
    "#                 uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "#                   uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#                   uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%' OR\n",
    "#                   uri.extra['nuscenes-label-channel'] LIKE '%CAM%'\n",
    "#                 ) AND (\n",
    "#                   uri.topic LIKE '%cuboid%' OR\n",
    "#                   uri.topic LIKE '%lidar%' OR\n",
    "#                   uri.topic LIKE '%cam%'\n",
    "#                 )\n",
    "#               GROUP BY task_id\n",
    "#             \"\"\")\n",
    "#         else:\n",
    "#             # For Nusc: group by nuscenes-sample-token WITH ALL FRAMES\n",
    "#             spark.sql(\"\"\"\n",
    "#               CACHE TABLE nusc_task_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#               SELECT \n",
    "#                   CONCAT(uri.segment_id, '.', uri.timestamp) AS task_id,\n",
    "#                   FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "#                   COLLECT_LIST(point_cloud) AS point_clouds\n",
    "#               FROM datums\n",
    "#               WHERE \n",
    "#                 (\n",
    "#                   uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#                   uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#                 ) AND (\n",
    "#                   uri.topic LIKE '%cuboid%' OR\n",
    "#                   uri.topic LIKE '%lidar%'\n",
    "#                 )\n",
    "#               GROUP BY task_id\n",
    "#               HAVING SIZE(cuboids) > 0 AND SIZE(point_clouds) > 0\n",
    "#             \"\"\")\n",
    "# #             spark.sql(\"\"\"\n",
    "# #               CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "# #               SELECT \n",
    "# #                   CONCAT(uri.segment_id, '.', uri.timestamp) AS task_id,\n",
    "# #                   FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "# #                   COLLECT_LIST(point_cloud) AS point_clouds\n",
    "# #               FROM datums\n",
    "# #               WHERE \n",
    "# #                 (\n",
    "# #                   uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "# #                   uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "# #                 ) AND (\n",
    "# #                   uri.topic LIKE '%cuboid%' OR\n",
    "# #                   uri.topic LIKE '%lidar%'\n",
    "# #                 )\n",
    "# #               GROUP BY task_id\n",
    "# #               HAVING SIZE(cuboids) > 0 AND SIZE(point_clouds) > 0\n",
    "# #             \"\"\")\n",
    "        \n",
    "#         tasks_df = spark.sql('SELECT * FROM nusc_task_df')\n",
    "#         print('... done.')\n",
    "#         return tasks_df.rdd\n",
    "\n",
    "# class NuscFusedWorldCloudKeyframesOnlyTable(NuscFusedWorldCloudTableBase):\n",
    "#     SRC_SD_TABLE = NuscStampedDatumTableBase\n",
    "\n",
    "# class NuscFusedWorldCloudAllFramesTable(NuscFusedWorldCloudTableBase):\n",
    "#     SRC_SD_TABLE = NuscStampedDatumTableLabelsAllFrames\n",
    "    \n",
    "class NuscKeyframesOFlowRenderer(OpticalFlowRenderBase):\n",
    "    FUSED_LIDAR_SD_TABLE = NuscKFOnlyFusedWorldCloudTable\n",
    "\n",
    "class NuscAllFramesOFlowRenderer(OpticalFlowRenderBase):\n",
    "    FUSED_LIDAR_SD_TABLE = NuscAllFramesFusedWorldCloudTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.spark import NBSpark\n",
    "spark = NBSpark.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fused Lidar Assets\n",
    "\n",
    "```\n",
    "docker --context default run -it --name=potree_viewer --rm --net=host -v `pwd`:/shared  jonazpiazu/potree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# T = KITTI360OurFusedWorldCloudTable\n",
    "# rdds = T._create_datum_rdds(spark)\n",
    "# print([r.count() for r in rdds])\n",
    "\n",
    "# seg_uris = T.get_all_segment_uris()\n",
    "# samp = T.get_sample(seg_uris[0], spark=spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print([lc.sensor_name for lc in samp.lidar_clouds][:10])\n",
    "# c = samp.lidar_clouds[0]#[lc for lc in samp.lidar_clouds if lc.sensor_name == '11002'][0]\n",
    "# print(c.get_cloud().shape)\n",
    "# imshow(c.get_bev_debug_image(x_bounds_meters=None, y_bounds_meters=None))\n",
    "# imshow(c.get_front_rv_debug_image(y_bounds_meters=None, z_bounds_meters=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Candidate Optical Flow Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KITTI360OFlowRenderer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1da7e50b2e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKITTI360OFlowRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mseg_uris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFUSED_LIDAR_SD_TABLE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_segment_uris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_segments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseg_uris\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KITTI360OFlowRenderer' is not defined"
     ]
    }
   ],
   "source": [
    "R = KITTI360OFlowRenderer\n",
    "seg_uris = R.FUSED_LIDAR_SD_TABLE.get_all_segment_uris()\n",
    "R.build(spark=spark, only_segments=[seg_uris[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# assert False, spark.sql(\"\"\"select uri.topic t from datums group by t\"\"\").show(truncate=False)\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "#           CACHE TABLE nusc_task_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#           SELECT \n",
    "#               MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS task_id,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) FILTER (WHERE uri.topic LIKE '%cam%') AS ci_sds\n",
    "#           FROM datums\n",
    "#           WHERE \n",
    "#             uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "#               uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#               uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#             ) AND (\n",
    "#               uri.topic LIKE '%cuboid%' OR\n",
    "#               uri.topic LIKE '%lidar%' OR\n",
    "#               uri.topic LIKE '%camera%'\n",
    "#             )\n",
    "#           GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "#         \"\"\").show()\n",
    "# df = spark.sql(\"\"\"\n",
    "#           SELECT \n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds,\n",
    "#               MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS lidar_time,\n",
    "#               FIRST(uri.extra.`nuscenes-sample-token`) AS sample_token\n",
    "#           FROM datums\n",
    "#           WHERE \n",
    "#             uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "#               uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#               uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#             ) AND (\n",
    "#               uri.topic LIKE '%cuboid%' OR\n",
    "#               uri.topic LIKE '%lidar%' OR\n",
    "#               uri.topic LIKE '%camera%'\n",
    "#             )\n",
    "#           GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "#           ORDER BY lidar_time\n",
    "#         \"\"\")\n",
    "# sample_tokens_ordered = [r.sample_token for r in df.select('sample_token').collect()]\n",
    "# task_to_stoken = [\n",
    "#     {'task_id': task_id, 'sample_token': sample_token}\n",
    "#     for task_id, sample_token in enumerate(sample_tokens_ordered)\n",
    "# ]\n",
    "# task_id_rdd = spark.sparkContext.parallelize(task_to_stoken)\n",
    "# task_id_df = spark.createDataFrame(task_id_rdd)\n",
    "# df.join(task_id_df, on=['sample_token'], how='inner').orderBy('task_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
