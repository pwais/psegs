{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Optical Flow from Fused Lidar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/psegs')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from psegs.exp.fused_lidar_flow import FusedLidarCloudTableBase\n",
    "\n",
    "import IPython.display\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "## General Notebook Utilities\n",
    "    \n",
    "def imshow(x):\n",
    "    IPython.display.display(PIL.Image.fromarray(x))\n",
    "\n",
    "def show_html(x):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemanticKITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.exp.semantic_kitti import SemanticKITTISDTable\n",
    "class SemanticKITTIFusedWorldCloudTable(FusedLidarCloudTableBase):\n",
    "    SRC_SD_TABLE = SemanticKITTISDTable\n",
    "\n",
    "    # SemanticKITTI has no cuboids, so we skip this step.\n",
    "    HAS_OBJ_CLOUDS = False\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "        seg_rdd = cls.SRC_SD_TABLE.get_segment_datum_rdd(spark, segment_uri)\n",
    "        \n",
    "        # SemanticKITTI has no cuboids, so the Fuser algo simply concats the cloud points\n",
    "        def iter_task_rows(iter_sds):\n",
    "            from pyspark import Row\n",
    "            from oarphpy.spark import RowAdapter\n",
    "            for sd in iter_sds:\n",
    "                if sd.point_cloud is not None:\n",
    "                    pc = sd.point_cloud\n",
    "                    task_id = \"%s.%s\" % (sd.uri.segment_id, pc.extra['semantic_kitti.scan_id'])\n",
    "                    yield Row(\n",
    "                        task_id=task_id,\n",
    "                        point_clouds=[pc],\n",
    "                        cuboids=[])\n",
    "        \n",
    "        task_rdd = seg_rdd.mapPartitions(iter_task_rows)\n",
    "        import pyspark\n",
    "        task_rdd = task_rdd.persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "        return task_rdd\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI-360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.datasets.kitti_360 import KITTI360SDTable\n",
    "class KITTI360OurFusedClouds(KITTI360SDTable):\n",
    "    INCLUDE_FISHEYES = False\n",
    "    INCLUDE_FUSED_CLOUDS = False  # Use our own fused clouds\n",
    "\n",
    "class KITTI360OurFusedWorldCloudTable(FusedLidarCloudTableBase):\n",
    "    SRC_SD_TABLE = KITTI360OurFusedClouds\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "        datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "        datum_df.registerTempTable('datums')\n",
    "        spark.catalog.dropTempView('culi_tasks_df')\n",
    "        print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        spark.sql(\"\"\"\n",
    "          CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "          SELECT \n",
    "              CONCAT(uri.segment_id, '.', uri.extra.`kitti-360.frame_id`) AS task_id,\n",
    "              FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "              COLLECT_LIST(point_cloud) AS point_clouds\n",
    "          FROM datums\n",
    "          WHERE \n",
    "              uri.topic LIKE '%cuboid%' OR uri.topic LIKE '%lidar%'\n",
    "          GROUP BY task_id\n",
    "        \"\"\")\n",
    "        tasks_df = spark.sql('SELECT * FROM culi_tasks_df')\n",
    "        print('... done.')\n",
    "        return tasks_df.rdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NuScenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip3 install nuscenes-devkit==1.1.2\n",
    "from psegs.datasets.nuscenes import NuscStampedDatumTableBase, NuscStampedDatumTableLabelsAllFrames\n",
    "class NuscFusedWorldCloudTableBase(FusedLidarCloudTableBase):\n",
    "    #SRC_SD_TABLE = NuscStampedDatumTableBase -- Subclass must pick a NuScenes configuration!\n",
    "    \n",
    "    SPLITS = ['train_detect', 'train_track']\n",
    "    \n",
    "    @classmethod\n",
    "    def _filter_ego_vehicle(cls, cloud_ego):\n",
    "        # Note: NuScenes authors have already corrected clouds for ego motion:\n",
    "        # https://github.com/nutonomy/nuscenes-devkit/issues/481#issuecomment-716250423\n",
    "        # But have not filtered out ego self-returns\n",
    "        cloud_ego = cloud_ego[np.where(  ~(\n",
    "                        (cloud_ego[:, 0] <= 1.5) & (cloud_ego[:, 0] >= -1.5) &  # Nusc lidar +x is +right\n",
    "                        (cloud_ego[:, 1] <= 2.5) & (cloud_ego[:, 0] >= -2.5) &  # Nusc lidar +y is +forward\n",
    "                        (cloud_ego[:, 1] <= 1.5) & (cloud_ego[:, 0] >= -1.5)    # Nusc lidar +z is +up\n",
    "        ))]\n",
    "        return cloud_ego\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "        datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "        datum_df.registerTempTable('datums')\n",
    "        spark.catalog.dropTempView('culi_tasks_df')\n",
    "        print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        if cls.SRC_SD_TABLE.LABELS_KEYFRAMES_ONLY:\n",
    "            # For Nusc: group by nuscenes-sample-token WITH KEYFRAMES\n",
    "            spark.sql(\"\"\"\n",
    "              CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "              SELECT \n",
    "                  uri.extra.`nuscenes-sample-token` AS task_id,\n",
    "                  FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "                  COLLECT_LIST(point_cloud) AS point_clouds\n",
    "              FROM datums\n",
    "              WHERE \n",
    "                uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "                  uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "                  uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "                ) AND (\n",
    "                  uri.topic LIKE '%cuboid%' OR\n",
    "                  uri.topic LIKE '%lidar%'\n",
    "                )\n",
    "              GROUP BY task_id\n",
    "            \"\"\")\n",
    "        else:\n",
    "            # For Nusc: group by nuscenes-sample-token WITH ALL FRAMES\n",
    "            spark.sql(\"\"\"\n",
    "              CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "              SELECT \n",
    "                  CONCAT(uri.segment_id, '.', uri.timestamp) AS task_id,\n",
    "                  FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "                  COLLECT_LIST(point_cloud) AS point_clouds\n",
    "              FROM datums\n",
    "              WHERE \n",
    "                (\n",
    "                  uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "                  uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "                ) AND (\n",
    "                  uri.topic LIKE '%cuboid%' OR\n",
    "                  uri.topic LIKE '%lidar%'\n",
    "                )\n",
    "              GROUP BY task_id\n",
    "              HAVING SIZE(cuboids) > 0 AND SIZE(point_clouds) > 0\n",
    "            \"\"\")\n",
    "        \n",
    "        tasks_df = spark.sql('SELECT * FROM culi_tasks_df')\n",
    "        print('... done.')\n",
    "        return tasks_df.rdd\n",
    "\n",
    "class NuscFusedWorldCloudKeyframesOnlyTable(NuscFusedWorldCloudTableBase):\n",
    "    SRC_SD_TABLE = NuscStampedDatumTableBase\n",
    "\n",
    "class NuscFusedWorldCloudAllFramesTable(NuscFusedWorldCloudTableBase):\n",
    "    SRC_SD_TABLE = NuscStampedDatumTableLabelsAllFrames\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.spark import NBSpark\n",
    "spark = NBSpark.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fused Lidar Assets\n",
    "\n",
    "```\n",
    "docker --context default run -it --name=potree_viewer --rm --net=host -v `pwd`:/shared  jonazpiazu/potree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T = KITTI360OurFusedWorldCloudTable\n",
    "rdds = T._create_datum_rdds(spark)\n",
    "print([r.count() for r in rdds])\n",
    "\n",
    "seg_uris = T.get_all_segment_uris()\n",
    "samp = T.get_sample(seg_uris[0], spark=spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print([lc.sensor_name for lc in samp.lidar_clouds][:10])\n",
    "c = samp.lidar_clouds[0]#[lc for lc in samp.lidar_clouds if lc.sensor_name == '11002'][0]\n",
    "print(c.get_cloud().shape)\n",
    "imshow(c.get_bev_debug_image(x_bounds_meters=None, y_bounds_meters=None))\n",
    "imshow(c.get_front_rv_debug_image(y_bounds_meters=None, z_bounds_meters=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Candidate Optical Flow Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
