{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Optical Flow from Fused Lidar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/psegs')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from psegs.exp.fused_lidar_flow import FusedLidarCloudTableBase\n",
    "from psegs.exp.fused_lidar_flow import TaskLidarCuboidCameraDFFactory\n",
    "from psegs.exp.fused_lidar_flow import OpticalFlowRenderBase\n",
    "\n",
    "import IPython.display\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "## General Notebook Utilities\n",
    "    \n",
    "def imshow(x):\n",
    "    IPython.display.display(PIL.Image.fromarray(x))\n",
    "\n",
    "def show_html(x):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemanticKITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.exp.semantic_kitti import SemanticKITTISDTable\n",
    "\n",
    "class SemanticKITTILCCDFFactory(TaskLidarCuboidCameraDFFactory):\n",
    "    \n",
    "    SRC_SD_TABLE = SemanticKITTISDTable\n",
    "    \n",
    "    @classmethod\n",
    "    def build_df_for_segment(cls, spark, segment_uri):\n",
    "        seg_rdd = cls.SRC_SD_TABLE.get_segment_datum_rdd(spark, segment_uri)\n",
    "        \n",
    "        def to_task_row(scan_id_iter_sds):\n",
    "            scan_id, iter_sds = scan_id_iter_sds\n",
    "            camera_images = []\n",
    "            point_clouds = []\n",
    "            for sd in iter_sds:\n",
    "                if sd.camera_image is not None:\n",
    "                    camera_images.append(sd)\n",
    "                elif sd.point_cloud is not None:\n",
    "                    point_clouds.append(sd)\n",
    "            \n",
    "            from pyspark import Row\n",
    "            r = Row(\n",
    "                    task_id=int(scan_id),\n",
    "                    pc_sds=point_clouds,\n",
    "                    cuboids_sds=[], # SemanticKITTI has no cuboids\n",
    "                    ci_sds=camera_images) \n",
    "            from oarphpy.spark import RowAdapter\n",
    "            return RowAdapter.to_row(r)\n",
    "            \n",
    "        grouped = seg_rdd.groupBy(lambda sd: sd.uri.extra['semantic_kitti.scan_id'])\n",
    "        row_rdd = grouped.map(to_task_row)\n",
    "\n",
    "        df = spark.createDataFrame(row_rdd, schema=cls.table_schema())\n",
    "        df = df.persist()\n",
    "        return df\n",
    "\n",
    "class SemanticKITTIFusedWorldCloudTable(FusedLidarCloudTableBase):\n",
    "    TASK_DF_FACTORY = SemanticKITTILCCDFFactory\n",
    "\n",
    "    # SemanticKITTI has no cuboids, so we skip this step.\n",
    "    HAS_OBJ_CLOUDS = False\n",
    "\n",
    "class SemanticKITTIOFlowRenderer(OpticalFlowRenderBase):\n",
    "    FUSED_LIDAR_SD_TABLE = SemanticKITTIFusedWorldCloudTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI-360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psegs.datasets.kitti_360 import KITTI360SDTable\n",
    "class KITTI360OurFusedClouds(KITTI360SDTable):\n",
    "    INCLUDE_FISHEYES = False\n",
    "    INCLUDE_FUSED_CLOUDS = False  # Use our own fused clouds\n",
    "\n",
    "class KITTI360LCCDFFactory(TaskLidarCuboidCameraDFFactory):\n",
    "    \n",
    "    SRC_SD_TABLE = KITTI360OurFusedClouds\n",
    "    \n",
    "    @classmethod\n",
    "    def build_df_for_segment(cls, spark, segment_uri):\n",
    "        datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "        datum_df.registerTempTable('datums')\n",
    "        print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        \n",
    "        spark.catalog.dropTempView('kitti360_tasks_df')\n",
    "        task_data_df = spark.sql(\"\"\"\n",
    "            CACHE TABLE kitti360_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "            SELECT \n",
    "              INT(uri.extra.`kitti-360.frame_id`) AS task_id,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds\n",
    "            FROM datums\n",
    "            WHERE (\n",
    "              uri.topic LIKE '%cuboid%' OR\n",
    "              uri.topic LIKE '%lidar%' OR\n",
    "              uri.topic LIKE '%camera%'\n",
    "            ) AND (\n",
    "              camera_image is NULL OR (camera_image.extra.`kitti-360.has-valid-ego-pose` = 'True')\n",
    "            ) AND (\n",
    "              point_cloud is NULL OR (point_cloud.extra.`kitti-360.has-valid-ego-pose` = 'True')\n",
    "            )\n",
    "            GROUP BY task_id\n",
    "        \"\"\")\n",
    "        \n",
    "        tasks_df = spark.sql('SELECT * FROM kitti360_tasks_df')\n",
    "        print('... done.')\n",
    "        return tasks_df\n",
    "        \n",
    "    \n",
    "class KITTI360WorldCloudTableBase(FusedLidarCloudTableBase):\n",
    "    TASK_DF_FACTORY = KITTI360LCCDFFactory\n",
    "        \n",
    "class KITTI360OFlowRenderer(OpticalFlowRenderBase):\n",
    "    FUSED_LIDAR_SD_TABLE = KITTI360WorldCloudTableBase\n",
    "\n",
    "    \n",
    "    \n",
    "# class KITTI360OurFusedWorldCloudTable(FusedLidarCloudTableBase):\n",
    "#     SRC_SD_TABLE = KITTI360OurFusedClouds\n",
    "    \n",
    "#     @classmethod\n",
    "#     def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "#         datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "#         datum_df.registerTempTable('datums')\n",
    "#         spark.catalog.dropTempView('culi_tasks_df')\n",
    "#         print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "#         spark.sql(\"\"\"\n",
    "#           CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#           SELECT \n",
    "#               CONCAT(uri.segment_id, '.', uri.extra.`kitti-360.frame_id`) AS task_id,\n",
    "#               FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "#               COLLECT_LIST(point_cloud) AS point_clouds\n",
    "#           FROM datums\n",
    "#           WHERE \n",
    "#               uri.topic LIKE '%cuboid%' OR uri.topic LIKE '%lidar%'\n",
    "#           GROUP BY task_id\n",
    "#         \"\"\")\n",
    "        \n",
    "        \n",
    "#         # TODO! for lidar and camera image!\n",
    "#         #         both_have_ego_pose = (\n",
    "#         #             ci1.extra.get('kitti-360.has-valid-ego-pose') and\n",
    "#         #             ci2.extra.get('kitti-360.has-valid-ego-pose'))\n",
    "        \n",
    "#         tasks_df = spark.sql('SELECT * FROM culi_tasks_df')\n",
    "#         print('... done.')\n",
    "#         return tasks_df.rdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NuScenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip3 install nuscenes-devkit==1.1.2\n",
    "from psegs.datasets.nuscenes import NuscStampedDatumTableBase\n",
    "from psegs.datasets.nuscenes import NuscStampedDatumTableLabelsAllFrames\n",
    "\n",
    "\n",
    "class NuscKFOnlyLCCDFFactory(TaskLidarCuboidCameraDFFactory):\n",
    "    \n",
    "    SRC_SD_TABLE = NuscStampedDatumTableBase\n",
    "    \n",
    "    @classmethod\n",
    "    def build_df_for_segment(cls, spark, segment_uri):\n",
    "        datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "        datum_df.registerTempTable('datums')\n",
    "        print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        \n",
    "        # Nusc doesn't have numerical task_ids so we'll have to induce\n",
    "        # one via lidar timestamp.\n",
    "        # NB: for Nusc: can group by nuscenes-sample-token FOR KEYFRAMES-ONLY DATA\n",
    "        task_data_df = spark.sql(\"\"\"\n",
    "            SELECT \n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds,\n",
    "              MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS lidar_time,\n",
    "              FIRST(uri.extra.`nuscenes-sample-token`) AS sample_token\n",
    "            FROM datums\n",
    "            WHERE \n",
    "            uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "              uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "              uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "            ) AND (\n",
    "              uri.topic LIKE '%cuboid%' OR\n",
    "              uri.topic LIKE '%lidar%' OR\n",
    "              uri.topic LIKE '%camera%'\n",
    "            )\n",
    "            GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "            ORDER BY lidar_time\n",
    "        \"\"\")\n",
    "        sample_tokens_ordered = [r.sample_token for r in task_data_df.select('sample_token').collect()]\n",
    "        task_to_stoken = [\n",
    "            {'task_id': task_id, 'sample_token': sample_token}\n",
    "            for task_id, sample_token in enumerate(sample_tokens_ordered)\n",
    "        ]\n",
    "        task_id_rdd = spark.sparkContext.parallelize(task_to_stoken)\n",
    "        task_id_df = spark.createDataFrame(task_id_rdd)\n",
    "        tasks_df = task_data_df.join(task_id_df, on=['sample_token'], how='inner')\n",
    "        tasks_df = tasks_df.persist()\n",
    "        print('... done.')\n",
    "        return tasks_df\n",
    "\n",
    "\n",
    "class NuscAllFramesLCCDFFactory(TaskLidarCuboidCameraDFFactory):\n",
    "    \n",
    "    SRC_SD_TABLE = NuscStampedDatumTableLabelsAllFrames\n",
    "    \n",
    "    @classmethod\n",
    "    def build_df_for_segment(cls, spark, segment_uri):\n",
    "        datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "        datum_df.registerTempTable('datums')\n",
    "        print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        \n",
    "        task_data_df = spark.sql(\"\"\"\n",
    "            SELECT \n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "              COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "                  FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds,\n",
    "              MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS lidar_time,\n",
    "              FIRST(uri.extra.`nuscenes-sample-token`) AS sample_token\n",
    "            FROM datums\n",
    "            WHERE \n",
    "            (\n",
    "              uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "              uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "            ) AND (\n",
    "              uri.topic LIKE '%cuboid%' OR\n",
    "              uri.topic LIKE '%lidar%' OR\n",
    "              uri.topic LIKE '%camera%'\n",
    "            )\n",
    "            GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "            ORDER BY lidar_time\n",
    "        \"\"\")\n",
    "        sample_tokens_ordered = [r.sample_token for r in task_data_df.select('sample_token').collect()]\n",
    "        task_to_stoken = [\n",
    "            {'task_id': task_id, 'sample_token': sample_token}\n",
    "            for task_id, sample_token in enumerate(sample_tokens_ordered)\n",
    "        ]\n",
    "        task_id_rdd = spark.sparkContext.parallelize(task_to_stoken)\n",
    "        task_id_df = spark.createDataFrame(task_id_rdd)\n",
    "        tasks_df = task_data_df.join(task_id_df, on=['sample_token'], how='inner')\n",
    "        tasks_df = tasks_df.persist()\n",
    "        print('... done.')\n",
    "        return tasks_df\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         # Nusc doesn't have numerical task_ids so we'll have to induce\n",
    "#         # one via lidar timestamp.\n",
    "#         task_data_df = spark.sql(\"\"\"\n",
    "#             SELECT \n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds,\n",
    "#               MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS lidar_time,\n",
    "#               FIRST(uri.extra.`nuscenes-sample-token`) AS sample_token\n",
    "#             FROM datums\n",
    "#             WHERE \n",
    "#              (\n",
    "#                uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#                uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#              ) AND (\n",
    "#                uri.topic LIKE '%cuboid%' OR\n",
    "#                uri.topic LIKE '%lidar%' OR\n",
    "#                uri.topic LIKE '%camera%'\n",
    "#              )\n",
    "            \n",
    "#            SELECT \n",
    "#                CONCAT(uri.segment_id, '.', uri.timestamp) AS task_id,\n",
    "#                FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "#                COLLECT_LIST(point_cloud) AS point_clouds\n",
    "#            FROM datums\n",
    "           \n",
    "#            WHERE \n",
    "#              (\n",
    "#                uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#                uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#              ) AND (\n",
    "#                uri.topic LIKE '%cuboid%' OR\n",
    "#                uri.topic LIKE '%lidar%'\n",
    "#              )\n",
    "#            GROUP BY task_id\n",
    "#            HAVING SIZE(cuboids) > 0 AND SIZE(point_clouds) > 0\n",
    "#          \"\"\")\n",
    "#         # #             spark.sql(\"\"\"\n",
    "#         # #               CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#         # #               SELECT \n",
    "#         # #                   CONCAT(uri.segment_id, '.', uri.timestamp) AS task_id,\n",
    "#         # #                   FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "#         # #                   COLLECT_LIST(point_cloud) AS point_clouds\n",
    "#         # #               FROM datums\n",
    "#         # #               WHERE \n",
    "#         # #                 (\n",
    "#         # #                   uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#         # #                   uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#         # #                 ) AND (\n",
    "#         # #                   uri.topic LIKE '%cuboid%' OR\n",
    "#         # #                   uri.topic LIKE '%lidar%'\n",
    "#         # #                 )\n",
    "#         # #               GROUP BY task_id\n",
    "#         # #               HAVING SIZE(cuboids) > 0 AND SIZE(point_clouds) > 0\n",
    "#         # #             \"\"\")\n",
    "        \n",
    "        \n",
    "        \n",
    "#         task_data_df = spark.sql(\"\"\"\n",
    "#             SELECT \n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds,\n",
    "#               MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS lidar_time,\n",
    "#               FIRST(uri.extra.`nuscenes-sample-token`) AS sample_token\n",
    "#             FROM datums\n",
    "#             WHERE \n",
    "#             uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "#               uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#               uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#             ) AND (\n",
    "#               uri.topic LIKE '%cuboid%' OR\n",
    "#               uri.topic LIKE '%lidar%' OR\n",
    "#               uri.topic LIKE '%camera%'\n",
    "#             )\n",
    "#             GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "#             ORDER BY lidar_time\n",
    "#         \"\"\")\n",
    "#         sample_tokens_ordered = [r.sample_token for r in task_data_df.select('sample_token').collect()]\n",
    "#         task_to_stoken = [\n",
    "#             {'task_id': task_id, 'sample_token': sample_token}\n",
    "#             for task_id, sample_token in enumerate(sample_tokens_ordered)\n",
    "#         ]\n",
    "#         task_id_rdd = spark.sparkContext.parallelize(task_to_stoken)\n",
    "#         task_id_df = spark.createDataFrame(task_id_rdd)\n",
    "#         tasks_df = task_data_df.join(task_id_df, on=['sample_token'], how='inner')\n",
    "#         tasks_df = tasks_df.persist()\n",
    "#         print('... done.')\n",
    "#         return tasks_df\n",
    "    \n",
    "class NuscWorldCloudTableBase(FusedLidarCloudTableBase):\n",
    "    SPLITS = ['train_detect', 'train_track']\n",
    "    \n",
    "    @classmethod\n",
    "    def _filter_ego_vehicle(cls, cloud_ego):\n",
    "        # Note: NuScenes authors have already corrected clouds for ego motion:\n",
    "        # https://github.com/nutonomy/nuscenes-devkit/issues/481#issuecomment-716250423\n",
    "        # But have not filtered out ego self-returns\n",
    "        cloud_ego = cloud_ego[np.where(  ~(\n",
    "                        (cloud_ego[:, 0] <= 1.5) & (cloud_ego[:, 0] >= -1.5) &  # Nusc lidar +x is +right\n",
    "                        (cloud_ego[:, 1] <= 2.5) & (cloud_ego[:, 0] >= -2.5) &  # Nusc lidar +y is +forward\n",
    "                        (cloud_ego[:, 1] <= 1.5) & (cloud_ego[:, 0] >= -1.5)    # Nusc lidar +z is +up\n",
    "        ))]\n",
    "        return cloud_ego\n",
    "    \n",
    "class NuscKFOnlyFusedWorldCloudTable(NuscWorldCloudTableBase):\n",
    "    TASK_DF_FACTORY = NuscKFOnlyLCCDFFactory\n",
    "\n",
    "class NuscAllFramesFusedWorldCloudTable(NuscWorldCloudTableBase):\n",
    "    TASK_DF_FACTORY = NuscAllFramesLCCDFFactory\n",
    "    \n",
    "    \n",
    "#     task_id=int(scan_id),\n",
    "#                     pc_sds=point_clouds,\n",
    "#                     cuboids_sds=[], # SemanticKITTI has no cuboids\n",
    "#                     ci_sds=camera_images\n",
    "#     @classmethod\n",
    "#     def _get_task_lidar_cuboid_rdd(cls, spark, segment_uri):\n",
    "#         datum_df = cls.SRC_SD_TABLE.get_segment_datum_df(spark, segment_uri)\n",
    "#         datum_df.registerTempTable('datums')\n",
    "#         spark.catalog.dropTempView('nusc_task_df')\n",
    "#         print('Building tasks table for %s ...' % segment_uri.segment_id)\n",
    "        \n",
    "#         # Nusc doesn't have numerical task_ids so we'll have to induce\n",
    "#         # one via lidar timestamp.\n",
    "#         if cls.SRC_SD_TABLE.LABELS_KEYFRAMES_ONLY:\n",
    "#             # For Nusc: group by nuscenes-sample-token WITH KEYFRAMES\n",
    "#             spark.sql(\"\"\"\n",
    "#               CACHE TABLE nusc_task_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#               SELECT \n",
    "#                   MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS task_id,\n",
    "#                   COLLECT_LIST(*) FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#                   COLLECT_LIST(*) FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#                   COLLECT_LIST(*) FILTER (WHERE uri.topic LIKE '%cam%') AS ci_sds\n",
    "#               FROM datums\n",
    "#               WHERE \n",
    "#                 uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "#                   uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#                   uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%' OR\n",
    "#                   uri.extra['nuscenes-label-channel'] LIKE '%CAM%'\n",
    "#                 ) AND (\n",
    "#                   uri.topic LIKE '%cuboid%' OR\n",
    "#                   uri.topic LIKE '%lidar%' OR\n",
    "#                   uri.topic LIKE '%cam%'\n",
    "#                 )\n",
    "#               GROUP BY task_id\n",
    "#             \"\"\")\n",
    "#         else:\n",
    "#             # For Nusc: group by nuscenes-sample-token WITH ALL FRAMES\n",
    "#             spark.sql(\"\"\"\n",
    "#               CACHE TABLE nusc_task_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#               SELECT \n",
    "#                   CONCAT(uri.segment_id, '.', uri.timestamp) AS task_id,\n",
    "#                   FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "#                   COLLECT_LIST(point_cloud) AS point_clouds\n",
    "#               FROM datums\n",
    "#               WHERE \n",
    "#                 (\n",
    "#                   uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#                   uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#                 ) AND (\n",
    "#                   uri.topic LIKE '%cuboid%' OR\n",
    "#                   uri.topic LIKE '%lidar%'\n",
    "#                 )\n",
    "#               GROUP BY task_id\n",
    "#               HAVING SIZE(cuboids) > 0 AND SIZE(point_clouds) > 0\n",
    "#             \"\"\")\n",
    "# #             spark.sql(\"\"\"\n",
    "# #               CACHE TABLE culi_tasks_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "# #               SELECT \n",
    "# #                   CONCAT(uri.segment_id, '.', uri.timestamp) AS task_id,\n",
    "# #                   FLATTEN(COLLECT_LIST(cuboids)) AS cuboids, \n",
    "# #                   COLLECT_LIST(point_cloud) AS point_clouds\n",
    "# #               FROM datums\n",
    "# #               WHERE \n",
    "# #                 (\n",
    "# #                   uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "# #                   uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "# #                 ) AND (\n",
    "# #                   uri.topic LIKE '%cuboid%' OR\n",
    "# #                   uri.topic LIKE '%lidar%'\n",
    "# #                 )\n",
    "# #               GROUP BY task_id\n",
    "# #               HAVING SIZE(cuboids) > 0 AND SIZE(point_clouds) > 0\n",
    "# #             \"\"\")\n",
    "        \n",
    "#         tasks_df = spark.sql('SELECT * FROM nusc_task_df')\n",
    "#         print('... done.')\n",
    "#         return tasks_df.rdd\n",
    "\n",
    "# class NuscFusedWorldCloudKeyframesOnlyTable(NuscFusedWorldCloudTableBase):\n",
    "#     SRC_SD_TABLE = NuscStampedDatumTableBase\n",
    "\n",
    "# class NuscFusedWorldCloudAllFramesTable(NuscFusedWorldCloudTableBase):\n",
    "#     SRC_SD_TABLE = NuscStampedDatumTableLabelsAllFrames\n",
    "    \n",
    "class NuscKeyframesOFlowRenderer(OpticalFlowRenderBase):\n",
    "    FUSED_LIDAR_SD_TABLE = NuscKFOnlyFusedWorldCloudTable\n",
    "\n",
    "class NuscAllFramesOFlowRenderer(OpticalFlowRenderBase):\n",
    "    FUSED_LIDAR_SD_TABLE = NuscAllFramesFusedWorldCloudTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 09:40:58,644\toarph 4020481 : Using source root /opt/psegs/psegs \n",
      "2021-02-22 09:40:58,644\toarph 4020481 : Using source root /opt/psegs \n",
      "2021-02-22 09:40:58,726\toarph 4020481 : Generating egg to /tmp/tmpt3_mak3v_oarphpy_eggbuild ...\n",
      "2021-02-22 09:40:58,820\toarph 4020481 : ... done.  Egg at /tmp/tmpt3_mak3v_oarphpy_eggbuild/psegs-0.0.0-py3.8.egg\n"
     ]
    }
   ],
   "source": [
    "from psegs.spark import NBSpark\n",
    "spark = NBSpark.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fused Lidar Assets\n",
    "\n",
    "```\n",
    "docker --context default run -it --name=potree_viewer --rm --net=host -v `pwd`:/shared  jonazpiazu/potree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# T = KITTI360OurFusedWorldCloudTable\n",
    "# rdds = T._create_datum_rdds(spark)\n",
    "# print([r.count() for r in rdds])\n",
    "\n",
    "# seg_uris = T.get_all_segment_uris()\n",
    "# samp = T.get_sample(seg_uris[0], spark=spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print([lc.sensor_name for lc in samp.lidar_clouds][:10])\n",
    "# c = samp.lidar_clouds[0]#[lc for lc in samp.lidar_clouds if lc.sensor_name == '11002'][0]\n",
    "# print(c.get_cloud().shape)\n",
    "# imshow(c.get_bev_debug_image(x_bounds_meters=None, y_bounds_meters=None))\n",
    "# imshow(c.get_front_rv_debug_image(y_bounds_meters=None, z_bounds_meters=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Candidate Optical Flow Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 09:41:02,288\tps   4020481 : Found Sequence 00 with 4541 scans\n",
      "2021-02-22 09:41:02,291\tps   4020481 : Found Sequence 01 with 1101 scans\n",
      "2021-02-22 09:41:02,303\tps   4020481 : Found Sequence 02 with 4661 scans\n",
      "2021-02-22 09:41:02,305\tps   4020481 : Found Sequence 03 with 801 scans\n",
      "2021-02-22 09:41:02,310\tps   4020481 : Found Sequence 05 with 2761 scans\n",
      "2021-02-22 09:41:02,313\tps   4020481 : Found Sequence 06 with 1101 scans\n",
      "2021-02-22 09:41:02,316\tps   4020481 : Found Sequence 07 with 1101 scans\n",
      "2021-02-22 09:41:02,320\tps   4020481 : Found Sequence 09 with 1591 scans\n",
      "2021-02-22 09:41:02,323\tps   4020481 : Found Sequence 10 with 1201 scans\n",
      "2021-02-22 09:41:02,323\tps   4020481 : Found 18859 total scans\n",
      "2021-02-22 09:41:02,325\tps   4020481 : Filtering to only 1 segments\n",
      "2021-02-22 09:41:02,325\tps   4020481 : Finding scans for sequence 00 with no moving points ...\n",
      "2021-02-22 09:41:08,703\tps   4020481 : ... sequence 00 has 3097 scans with no movers.\n",
      "2021-02-22 09:41:23,356\tps   4020481 : Filtering to only 1 segments\n",
      "2021-02-22 09:41:23,356\tps   4020481 : SemanticKITTIFusedWorldCloudTable building fused clouds ...\n",
      "2021-02-22 09:41:23,357\tps   4020481 : ... have 1 segments to fuse ...\n",
      "2021-02-22 09:41:23,357\tps   4020481 : ... working on 00 ...\n",
      "2021-02-22 09:41:23,359\tps   4020481 : ... skipping 00; world and obj clouds done\n",
      "2021-02-22 09:41:23,360\tps   4020481 : World Cloud: /opt/psegs/dataroot/fused_world_clouds/naive_cuboid_scrubber/semantikitti/train/00/fused_world.ply\n",
      "2021-02-22 09:41:23,361\tps   4020481 : Obj Clouds: /opt/psegs/dataroot/fused_obj_clouds/naive_cuboid_scrubber/semantikitti/train/00\n",
      "2021-02-22 09:41:23,368\toarph 4020481 : Progress for \n",
      "FuseEachSegment [Pid:4020481 Id:139658779630512]\n",
      "-----------------------  ---------------\n",
      "Thruput\n",
      "N thru                   1 (of 1)\n",
      "N chunks                 1\n",
      "Total time               0 seconds\n",
      "Total thru               0 bytes\n",
      "Rate                     0.0 bytes / sec\n",
      "Hz                       255\n",
      "Progress\n",
      "Percent Complete         100.000000\n",
      "Est. Time To Completion  0 seconds\n",
      "-----------------------  ---------------\n",
      "2021-02-22 09:41:23,368\tps   4020481 : ... SemanticKITTIFusedWorldCloudTable done fusing clouds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tasks 3097\n",
      "restrict to 5\n",
      "oflow_task_df 5\n",
      "coalesc to  1\n"
     ]
    }
   ],
   "source": [
    "# R = NuscKeyframesOFlowRenderer\n",
    "# R.MAX_TASKS_PER_SEGMENT = 5\n",
    "R = SemanticKITTIOFlowRenderer\n",
    "R.MAX_TASKS_PER_SEGMENT = 5\n",
    "seg_uris = R.FUSED_LIDAR_SD_TABLE.get_all_segment_uris()\n",
    "R.build(spark=spark, only_segments=[seg_uris[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# assert False, spark.sql(\"\"\"select uri.topic t from datums group by t\"\"\").show(truncate=False)\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "#           CACHE TABLE nusc_task_df OPTIONS ( 'storageLevel' 'DISK_ONLY' ) AS\n",
    "#           SELECT \n",
    "#               MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS task_id,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) FILTER (WHERE uri.topic LIKE '%cam%') AS ci_sds\n",
    "#           FROM datums\n",
    "#           WHERE \n",
    "#             uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "#               uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#               uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#             ) AND (\n",
    "#               uri.topic LIKE '%cuboid%' OR\n",
    "#               uri.topic LIKE '%lidar%' OR\n",
    "#               uri.topic LIKE '%camera%'\n",
    "#             )\n",
    "#           GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "#         \"\"\").show()\n",
    "# df = spark.sql(\"\"\"\n",
    "#           SELECT \n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, point_cloud)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%lidar%') AS pc_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, cuboids)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%cuboid%') AS cuboids_sds,\n",
    "#               COLLECT_LIST(STRUCT(__pyclass__, uri, camera_image)) \n",
    "#                   FILTER (WHERE uri.topic LIKE '%camera%') AS ci_sds,\n",
    "#               MIN(uri.timestamp) FILTER (WHERE uri.topic LIKE '%lidar%') AS lidar_time,\n",
    "#               FIRST(uri.extra.`nuscenes-sample-token`) AS sample_token\n",
    "#           FROM datums\n",
    "#           WHERE \n",
    "#             uri.extra.`nuscenes-is-keyframe` = 'True' AND (\n",
    "#               uri.extra['nuscenes-label-channel'] is NULL OR \n",
    "#               uri.extra['nuscenes-label-channel'] LIKE '%LIDAR%'\n",
    "#             ) AND (\n",
    "#               uri.topic LIKE '%cuboid%' OR\n",
    "#               uri.topic LIKE '%lidar%' OR\n",
    "#               uri.topic LIKE '%camera%'\n",
    "#             )\n",
    "#           GROUP BY uri.extra.`nuscenes-sample-token`\n",
    "#           ORDER BY lidar_time\n",
    "#         \"\"\")\n",
    "# sample_tokens_ordered = [r.sample_token for r in df.select('sample_token').collect()]\n",
    "# task_to_stoken = [\n",
    "#     {'task_id': task_id, 'sample_token': sample_token}\n",
    "#     for task_id, sample_token in enumerate(sample_tokens_ordered)\n",
    "# ]\n",
    "# task_id_rdd = spark.sparkContext.parallelize(task_to_stoken)\n",
    "# task_id_df = spark.createDataFrame(task_id_rdd)\n",
    "# df.join(task_id_df, on=['sample_token'], how='inner').orderBy('task_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
