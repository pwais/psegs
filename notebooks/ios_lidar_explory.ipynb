{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CameraImage(sensor_name='camera_front', image_jpeg=bytearray(b''), image_png=bytearray(b''), image_factory=CloudpickeledCallable(_func_pyclass=psegs.datasets.ios_lidar.<lambda>), width=1920, height=1440, timestamp=853720759521708, ego_pose=Transform(rotation=array([[-0.16402921,  0.36367464, -0.91697067],\n",
       "       [-0.04168281,  0.92617565,  0.37478161],\n",
       "       [ 0.98557436,  0.09969707, -0.13676088]]), translation=array([[-2.92258954],\n",
       "       [ 0.93101001],\n",
       "       [ 0.19710636]]), src_frame='ego', dest_frame='world'), ego_to_sensor=Transform(rotation=array([[ 1,  0,  0],\n",
       "       [ 0, -1,  0],\n",
       "       [ 0,  0, -1]]), translation=array([[0.],\n",
       "       [0.],\n",
       "       [0.]]), src_frame='camera_front', dest_frame='ego'), K=array([[1.44565613e+03, 0.00000000e+00, 9.69324890e+02],\n",
       "       [0.00000000e+00, 1.44565613e+03, 6.90660400e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]), extra={'threeDScannerApp.cameraGrain': '0', 'threeDScannerApp.time': '853720.7595217085', 'threeDScannerApp.frame_index': '0', 'threeDScannerApp.motionQuality': '1', 'threeDScannerApp.projectionMatrix': '[1.5058917999267578, 0, -0.010234236717224121, 0, 0, 2.0078556537628174, -0.040055036544799805, 0, 0, 0, -0.9999997615814209, -0.0009999998146668077, 0, 0, -1, 0]', 'threeDScannerApp.intrinsics': '[1445.6561279296875, 0, 969.3248901367188, 0, 1445.6561279296875, 690.660400390625, 0, 0, 1]', 'threeDScannerApp.averageVelocity': '0', 'threeDScannerApp.averageAngularVelocity': '0', 'threeDScannerApp.exposureDuration': '0.016393441706895828'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/opt/psegs')\n",
    "\n",
    "from psegs.datasets import ios_lidar\n",
    "\n",
    "base_dir = '/outer_root/home/au/lidarphone_scans/2021_06_27_12_37_38'\n",
    "# base_dir = '/outer_root/home/au/lidarphone_scans/landscape_home_button_right_07_09_49'\n",
    "\n",
    "from oarphpy import util as oputil\n",
    "json_paths = oputil.all_files_recursive(base_dir, pattern='frame*.json')\n",
    "json_paths = sorted(json_paths)\n",
    "cis = [ios_lidar.threeDScannerApp_create_camera_image(p) for p in json_paths]\n",
    "\n",
    "print(len(cis))\n",
    "cis[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from psegs.mesh2rgb import pytorch3d_camera_images_to_rgbd_debug\n",
    "\n",
    "# mesh_path = os.path.join(base_dir, 'export_refined.obj')\n",
    "# outpath = os.path.join(base_dir, 'pytorch3d_rgbd_debug.mp4')\n",
    "\n",
    "# pytorch3d_camera_images_to_rgbd_debug(cis, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "init_notebook_mode(connected=False)         # initiate notebook for offline plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "mesh = o3d.io.read_triangle_mesh(os.path.join(base_dir, 'export_refined.obj'))\n",
    "vertices = np.asarray(mesh.vertices)\n",
    "print(vertices.shape)\n",
    "\n",
    "import numpy as np\n",
    "v_sub = vertices[np.random.choice(vertices.shape[0], 40000, replace=False)]\n",
    "print(v_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "plots = [ci.to_plotly_world_frame_3d() for ci in cis[:1]]\n",
    "\n",
    "import pandas as pd\n",
    "cloud_df = pd.DataFrame(v_sub, columns=['x', 'y', 'z'])\n",
    "from psegs.util.plotting import rgb_for_distance\n",
    "cloud_df['color'] = [\n",
    "  rgb_for_distance(np.linalg.norm(pt), period_meters=1.)\n",
    "  for pt in cloud_df[['x', 'y', 'z']].values\n",
    "]\n",
    "scatter = go.Scatter3d(\n",
    "                x=cloud_df['x'], y=cloud_df['y'], z=cloud_df['z'],\n",
    "                mode='markers',\n",
    "                marker=dict(size=2, color=cloud_df['color'], opacity=0.5),)\n",
    "\n",
    "plots.append(scatter)\n",
    "\n",
    "\n",
    "fig = go.Figure(data=plots)\n",
    "\n",
    "fig.update_layout(\n",
    "  width=1000, height=700,\n",
    "  scene_aspectmode='data')\n",
    "  # scene_camera=dict(\n",
    "  #   up=dict(x=0, y=0, z=1),\n",
    "  #   eye=dict(x=0, y=0, z=0),\n",
    "  #   center=dict(x=1, y=0, z=0),\n",
    "  # ))\n",
    "    \n",
    "iplot(fig)\n",
    "    \n",
    "    \n",
    "# plot_str = plotly.offline.plot(fig, output_type='div')\n",
    "\n",
    "# html += '<br/><br/>' + plot_str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False\n",
    "# # https://chart-studio.plotly.com/~empet/15040/plotly-mesh3d-from-a-wavefront-obj-f/#/\n",
    "\n",
    "\n",
    "# def obj_data_to_mesh3d(odata):\n",
    "#     # odata is the string read from an obj file\n",
    "#     vertices = []\n",
    "#     faces = []\n",
    "#     lines = odata.splitlines()   \n",
    "   \n",
    "#     for line in lines:\n",
    "#         slist = line.split()\n",
    "#         if slist:\n",
    "#             if slist[0] == 'v':\n",
    "#                 vertex = np.array(slist[1:], dtype=float)\n",
    "#                 vertices.append(vertex)\n",
    "#             elif slist[0] == 'f':\n",
    "#                 face = []\n",
    "#                 for k in range(1, len(slist)):\n",
    "#                     face.append([int(s) for s in slist[k].replace('//','/').split('/')])\n",
    "#                 if len(face) > 3: # triangulate the n-polyonal face, n>3\n",
    "#                     faces.extend([[face[0][0]-1, face[k][0]-1, face[k+1][0]-1] for k in range(1, len(face)-1)])\n",
    "#                 else:    \n",
    "#                     faces.append([face[j][0]-1 for j in range(len(face))])\n",
    "#             else: pass\n",
    "    \n",
    "    \n",
    "#     return np.array(vertices), np.array(faces)\n",
    "\n",
    "# import os\n",
    "# with open(os.path.join(base_dir, 'export_refined.obj'), 'rb') as f:\n",
    "#     obj_data = f.read().decode('utf-8')\n",
    "# vertices, faces = obj_data_to_mesh3d(obj_data)\n",
    "# print('vertices.shape', vertices.shape)\n",
    "# print('faces.shape', faces.shape)\n",
    "\n",
    "\n",
    "# x, y, z = vertices[:,:3].T\n",
    "# I, J, K = faces.T\n",
    "\n",
    "# mesh = go.Mesh3d(\n",
    "#             x=-x,\n",
    "#             y=-y,\n",
    "#             z=z,\n",
    "# #             vertexcolor=vertices[:, 3:], #the color codes must be triplets of floats  in [0,1]!!                      \n",
    "#             i=I,\n",
    "#             j=J,\n",
    "#             k=K,\n",
    "#             name='',\n",
    "#             showscale=False)\n",
    "\n",
    "# layout = go.Layout(width=900,\n",
    "#                    height=800,\n",
    "#                    scene=dict(xaxis=dict(visible=False),\n",
    "#                               yaxis=dict(visible=False),  \n",
    "#                               zaxis=dict(visible=False), \n",
    "#                               aspectratio=dict(x=1.5,\n",
    "#                                                y=0.9,\n",
    "#                                                z=0.5\n",
    "#                                          ),\n",
    "#                               camera=dict(eye=dict(x=1., y=1., z=0.5)),\n",
    "#                         ),\n",
    "#                   ) \n",
    "\n",
    "# fig = go.Figure(data=[mesh], layout=layout)\n",
    "# iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open3d as o3d\n",
    "# mesh = o3d.io.read_triangle_mesh(os.path.join(base_dir, 'export_refined.obj'))\n",
    "# vertices = np.asarray(mesh.vertices)\n",
    "# print(vertices.shape)\n",
    "\n",
    "# import numpy as np\n",
    "# v_sub = vertices[np.random.choice(vertices.shape[0], 10000, replace=False)]\n",
    "# print(v_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cloud_raw = vertices\n",
    "\n",
    "import imageio\n",
    "writer = imageio.get_writer('/outer_root/home/au/lidarphone_scans/test_video.mp4', fps=5)\n",
    "\n",
    "for i in range(len(cis)):\n",
    "    cloud_ego = cis[i].ego_pose.get_inverse().apply(cloud_raw).T\n",
    "\n",
    "    from psegs import datum\n",
    "    pc = datum.PointCloud(cloud=cloud_ego)\n",
    "\n",
    "    debug = cis[i].get_debug_image(clouds=[pc], period_meters=0.1)\n",
    "    writer.append_data(debug)\n",
    "    print(i)\n",
    "writer.close()\n",
    "\n",
    "#     from io import BytesIO\n",
    "#     import IPython.display\n",
    "#     import numpy as np\n",
    "#     import PIL.Image\n",
    "#     def showarray(a, fmt='png'):\n",
    "#         a = np.uint8(a)\n",
    "#         f = BytesIO()\n",
    "#         PIL.Image.fromarray(a).save(f, fmt)\n",
    "#         IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "#     showarray(debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/psegs')\n",
    "\n",
    "import os\n",
    "\n",
    "ROOT = '/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/'\n",
    "\n",
    "for d in os.listdir(ROOT):\n",
    "    if '.DS_Store' in d:\n",
    "        continue\n",
    "    base_dir = os.path.join(ROOT, d)\n",
    "    print(base_dir)\n",
    "    \n",
    "\n",
    "    from psegs.datasets import ios_lidar\n",
    "\n",
    "\n",
    "    from oarphpy import util as oputil\n",
    "    json_paths = oputil.all_files_recursive(base_dir, pattern='frame*.json')\n",
    "    json_paths = sorted(json_paths)\n",
    "    \n",
    "    try:\n",
    "        cis = [ios_lidar.threeDScannerApp_create_camera_image(p) for p in json_paths]\n",
    "    except AssertionError as e:\n",
    "        continue\n",
    "\n",
    "    print(len(cis))\n",
    "    \n",
    "    import os\n",
    "    import numpy as np\n",
    "    import open3d as o3d\n",
    "    mesh = o3d.io.read_triangle_mesh(os.path.join(base_dir, 'export.obj'))\n",
    "    vertices = np.asarray(mesh.vertices)\n",
    "    print(vertices.shape)\n",
    "\n",
    "    cloud_raw = vertices\n",
    "\n",
    "    outpath = os.path.join(ROOT, d + '.mp4')\n",
    "    \n",
    "    import imageio\n",
    "    writer = imageio.get_writer(outpath, fps=5)\n",
    "\n",
    "    for i in range(len(cis)):\n",
    "        cloud_ego = cis[i].ego_pose.get_inverse().apply(cloud_raw).T\n",
    "        cloud_ego[:, 0] *= -1\n",
    "\n",
    "        from psegs import datum\n",
    "        pc = datum.PointCloud(cloud=cloud_ego)\n",
    "\n",
    "        debug = cis[i].get_debug_image(clouds=[pc], period_meters=0.1)\n",
    "        writer.append_data(debug)\n",
    "        print(i)\n",
    "    writer.close()\n",
    "    \n",
    "    print('done', outpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !apt-get install -y python3-pybind11\n",
    "# # !cd /opt && git clone https://github.com/NVIDIA/cub\n",
    "# # !CUB_HOME=/opt/cub pip3 install \"git+https://github.com/facebookresearch/pytorch3d.git@stable\"\n",
    "\n",
    "# # import os\n",
    "# # !curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz\n",
    "# # !tar xzf 1.10.0.tar.gz\n",
    "# # os.environ[\"CUB_HOME\"] = os.getcwd() + \"/cub-1.10.0\"\n",
    "# # !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'`\n",
    "\n",
    "\n",
    "# # !pip3 install torch torchvision torchaudio\n",
    "# # !pip3 install pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py38_cu102_pyt190/download.html\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# import torch\n",
    "\n",
    "# import pytorch3d\n",
    "\n",
    "# import os\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from pytorch3d.utils import ico_sphere\n",
    "# import numpy as np\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# # Util function for loading meshes\n",
    "# from pytorch3d.io import load_objs_as_meshes, save_obj\n",
    "\n",
    "# from pytorch3d.loss import (\n",
    "#     chamfer_distance, \n",
    "#     mesh_edge_loss, \n",
    "#     mesh_laplacian_smoothing, \n",
    "#     mesh_normal_consistency,\n",
    "# )\n",
    "\n",
    "# # Data structures and functions for rendering\n",
    "# from pytorch3d.structures import Meshes\n",
    "# from pytorch3d.renderer import (\n",
    "#     look_at_view_transform,\n",
    "#     OpenGLPerspectiveCameras, \n",
    "#     PointLights, \n",
    "#     DirectionalLights, \n",
    "#     Materials, \n",
    "#     RasterizationSettings, \n",
    "#     MeshRenderer, \n",
    "#     MeshRasterizer,  \n",
    "#     SoftPhongShader,\n",
    "#     SoftSilhouetteShader,\n",
    "#     SoftPhongShader,\n",
    "#     TexturesVertex\n",
    "# )\n",
    "\n",
    "# # add path for demo utils functions \n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath(''))\n",
    "\n",
    "# !wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/docs/tutorials/utils/plot_image_grid.py\n",
    "# from plot_image_grid import image_grid\n",
    "\n",
    "# !mkdir -p data/cow_mesh\n",
    "# !wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.obj\n",
    "# !wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.mtl\n",
    "# !wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow_texture.png\n",
    "    \n",
    "\n",
    "    \n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda:0\")\n",
    "#     torch.cuda.set_device(device)\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "\n",
    "# # Set paths\n",
    "# DATA_DIR = \"./data\"\n",
    "# obj_filename = os.path.join(DATA_DIR, \"cow_mesh/cow.obj\")\n",
    "\n",
    "# # Load obj file\n",
    "# mesh = load_objs_as_meshes([obj_filename], device=device)\n",
    "\n",
    "# # We scale normalize and center the target mesh to fit in a sphere of radius 1 \n",
    "# # centered at (0,0,0). (scale, center) will be used to bring the predicted mesh \n",
    "# # to its original center and scale.  Note that normalizing the target mesh, \n",
    "# # speeds up the optimization but is not necessary!\n",
    "# verts = mesh.verts_packed()\n",
    "# N = verts.shape[0]\n",
    "# center = verts.mean(0)\n",
    "# scale = max((verts - center).abs().max(0)[0])\n",
    "# mesh.offset_verts_(-center)\n",
    "# mesh.scale_verts_((1.0 / float(scale)))\n",
    "\n",
    "\n",
    "# # the number of different viewpoints from which we want to render the mesh.\n",
    "# num_views = 20\n",
    "\n",
    "# # Get a batch of viewing angles. \n",
    "# elev = torch.linspace(0, 360, num_views)\n",
    "# azim = torch.linspace(-180, 180, num_views)\n",
    "\n",
    "# # Place a point light in front of the object. As mentioned above, the front of \n",
    "# # the cow is facing the -z direction. \n",
    "# lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "# # Initialize an OpenGL perspective camera that represents a batch of different \n",
    "# # viewing angles. All the cameras helper methods support mixed type inputs and \n",
    "# # broadcasting. So we can view the camera from the a distance of dist=2.7, and \n",
    "# # then specify elevation and azimuth angles for each viewpoint as tensors. \n",
    "# R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
    "# cameras = OpenGLPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "# # We arbitrarily choose one particular view that will be used to visualize \n",
    "# # results\n",
    "# camera = OpenGLPerspectiveCameras(device=device, R=R[None, 1, ...], \n",
    "#                                   T=T[None, 1, ...]) \n",
    "\n",
    "# # Define the settings for rasterization and shading. Here we set the output \n",
    "# # image to be of size 128X128. As we are rendering images for visualization \n",
    "# # purposes only we will set faces_per_pixel=1 and blur_radius=0.0. Refer to \n",
    "# # rasterize_meshes.py for explanations of these parameters.  We also leave \n",
    "# # bin_size and max_faces_per_bin to their default values of None, which sets \n",
    "# # their values using heuristics and ensures that the faster coarse-to-fine \n",
    "# # rasterization method is used.  Refer to docs/notes/renderer.md for an \n",
    "# # explanation of the difference between naive and coarse-to-fine rasterization. \n",
    "# raster_settings = RasterizationSettings(\n",
    "#     image_size=128, \n",
    "#     blur_radius=0.0, \n",
    "#     faces_per_pixel=1, \n",
    "# )\n",
    "\n",
    "# # Create a Phong renderer by composing a rasterizer and a shader. The textured \n",
    "# # Phong shader will interpolate the texture uv coordinates for each vertex, \n",
    "# # sample from a texture image and apply the Phong lighting model\n",
    "# renderer = MeshRenderer(\n",
    "#     rasterizer=MeshRasterizer(\n",
    "#         cameras=camera, \n",
    "#         raster_settings=raster_settings\n",
    "#     ),\n",
    "#     shader=SoftPhongShader(\n",
    "#         device=device, \n",
    "#         cameras=camera,\n",
    "#         lights=lights\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Create a batch of meshes by repeating the cow mesh and associated textures. \n",
    "# # Meshes has a useful `extend` method which allows us do this very easily. \n",
    "# # This also extends the textures. \n",
    "# meshes = mesh.extend(num_views)\n",
    "\n",
    "# # Render the cow mesh from each viewing angle\n",
    "# target_images = renderer(meshes, cameras=cameras, lights=lights)\n",
    "\n",
    "# # Our multi-view cow dataset will be represented by these 2 lists of tensors,\n",
    "# # each of length num_views.\n",
    "# target_rgb = [target_images[i, ..., :3] for i in range(num_views)]\n",
    "# target_cameras = [OpenGLPerspectiveCameras(device=device, R=R[None, i, ...], \n",
    "#                                            T=T[None, i, ...]) for i in range(num_views)]\n",
    "\n",
    "# image_grid(target_images.cpu().numpy(), rows=4, cols=5, rgb=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch3d.renderer.mesh.rasterizer import MeshRasterizer\n",
    "\n",
    "# from torchvision.transforms import ToPILImage\n",
    "\n",
    "# to_img = ToPILImage()\n",
    "\n",
    "# rasterizer = MeshRasterizer(\n",
    "#     cameras=cameras, \n",
    "#     raster_settings=raster_settings\n",
    "# )\n",
    "\n",
    "# fragments = rasterizer(meshes)\n",
    "\n",
    "# to_img(fragments.zbuf.cpu().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# IOS_CAM_TO_PYTORCH = np.array([\n",
    "#     [ 0, -1,  0,  0],\n",
    "#     [ 0,  0,  1,  0],\n",
    "#     [-1,  0,  0,  0],\n",
    "#     [ 0,  0,  0,  1],\n",
    "#   ], dtype=np.float32)\n",
    "\n",
    "from pytorch3d.io import load_obj\n",
    "obj_path = os.path.join(base_dir, 'export_refined.obj')\n",
    "verts, faces_idx, _ = load_obj(obj_path)\n",
    "faces = faces_idx.verts_idx\n",
    "print('verts', verts.shape)\n",
    "print('faces', faces.shape)\n",
    "\n",
    "nverts = verts.numpy()\n",
    "nfaces = faces.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "CI = cis[130]\n",
    "\n",
    "import numpy as np\n",
    "# obj_path = os.path.join(base_dir, 'export_refined.obj')\n",
    "fov_x, fov_y = CI.get_fov()\n",
    "K = CI.K\n",
    "height, width = CI.height, CI.width\n",
    "pose = CI.ego_pose['ego', 'world'].get_inverse().get_transformation_matrix(homogeneous=True)\n",
    "pose = pose.astype(np.float32)\n",
    "K = K.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "world2pytorch = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, -1, 0, 0],\n",
    "    [0, 0, -1, 0],\n",
    "    [0, 0, 0, 1],\n",
    "], dtype=np.float32)\n",
    "\n",
    "pose = world2pytorch @ pose\n",
    "\n",
    "# pose[0, 0] *= -1\n",
    "# pose[1, 1] *= -1\n",
    "# pose[2, 2] *= -1\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import pytorch3d\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch3d.utils import ico_sphere\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, save_obj, load_obj\n",
    "\n",
    "from pytorch3d.loss import (\n",
    "    chamfer_distance, \n",
    "    mesh_edge_loss, \n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency,\n",
    ")\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    OpenGLPerspectiveCameras, \n",
    "    PointLights, \n",
    "    DirectionalLights, \n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    SoftPhongShader,\n",
    "    TexturesVertex,\n",
    "    AmbientLights\n",
    ")\n",
    "\n",
    "# add path for demo utils functions \n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(''))\n",
    "\n",
    "\n",
    "# io utils\n",
    "from pytorch3d.io import load_obj\n",
    "\n",
    "# datastructures\n",
    "from pytorch3d.structures import Meshes\n",
    "\n",
    "# 3D transformations functions\n",
    "from pytorch3d.transforms import Rotate, Translate\n",
    "\n",
    "# rendering components\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, look_at_view_transform, look_at_rotation, \n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, PointLights, TexturesVertex,\n",
    "    HardGouraudShader, SoftGouraudShader,HardFlatShader,PerspectiveCameras,FoVOrthographicCameras,\n",
    ")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# # Set paths\n",
    "# DATA_DIR = \"./data\"\n",
    "# obj_filename = os.path.join(DATA_DIR, \"cow_mesh/cow.obj\")\n",
    "\n",
    "# Load obj file\n",
    "# mesh = load_objs_as_meshes([obj_path], device=device)\n",
    "\n",
    "# Load the obj and ignore the textures and materials.\n",
    "# verts, faces_idx, _ = load_obj(obj_path)\n",
    "# faces = faces_idx.verts_idx\n",
    "# print('verts', verts.shape)\n",
    "# print('faces', faces.shape)\n",
    "import torch\n",
    "verts = torch.from_numpy(nverts)\n",
    "faces = torch.from_numpy(nfaces)\n",
    "\n",
    "# Initialize each vertex to be white in color.\n",
    "verts_rgb = .9 * torch.ones_like(verts)[None]  # (1, V, 3)\n",
    "textures = TexturesVertex(verts_features=verts_rgb.to(device))\n",
    "\n",
    "# Create a Meshes object for the teapot. Here we have only one mesh in the batch.\n",
    "teapot_mesh = Meshes(\n",
    "    verts=[verts.to(device)],   \n",
    "    faces=[faces.to(device)], \n",
    "    textures=textures,\n",
    ")\n",
    "print('teapot_mesh', teapot_mesh)\n",
    "# teapot_mesh = load_objs_as_meshes([obj_path], device=device)\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "R = torch.from_numpy(pose[:3, :3].reshape([1, 3, 3])).to(device)\n",
    "T = torch.from_numpy(pose[:3, 3].reshape([1, 3])).to(device)\n",
    "\n",
    "\n",
    "# Select the viewpoint using spherical angles  \n",
    "distance = 5   # distance from camera to the object\n",
    "elevation = 50.0   # angle of elevation in degrees\n",
    "azimuth = 0.0  # No rotation so the camera is positioned on the +Z axis. \n",
    "\n",
    "# Get the position of the camera based on the spherical angles\n",
    "# R, T = look_at_view_transform(distance, elevation, azimuth, device=device)\n",
    "print('R', R)\n",
    "print('T', T)\n",
    "\n",
    "tK = np.eye(4).astype(np.float32)\n",
    "tK[:3, :3] = K\n",
    "\n",
    "# # Great job pytorch3d!! \n",
    "# # https://github.com/facebookresearch/pytorch3d/blob/103da63393d6bbb697835ddbfc86b07572ea4d0c/tests/test_camera_conversions.py#L116\n",
    "# tK[0, 0] = 1.1 * K[0, 0]\n",
    "# tK[1, 1] = 1.1 * K[1, 1]\n",
    "# tK[2, 0] = 1.1 * K[2, 0]\n",
    "# tK[2, 1] = 1.1 * K[2, 1]\n",
    "\n",
    "\n",
    "tK = torch.from_numpy(tK.reshape([1, 4, 4])).to(device)\n",
    "print('K', tK)\n",
    "\n",
    "image_size = torch.from_numpy(np.array([height, width]).reshape([1, 2])).to(device)\n",
    "print('image_size', image_size)\n",
    "\n",
    "# https://github.com/facebookresearch/pytorch3d/issues/522\n",
    "from pytorch3d.utils.camera_conversions import cameras_from_opencv_projection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cameras = cameras_from_opencv_projection(R, T, tK, image_size, device=device).cuda()\n",
    "\n",
    "# assert False, (cameras.R, cameras.T, cameras.get_world_to_view_transform().device)\n",
    "# assert False, cameras.get_world_to_view_transform().device\n",
    "# print('get_world_to_view_transform', xform.device, cameras.R, cameras.T)\n",
    "\n",
    "# cameras = FoVPerspectiveCameras(device=device, fov=fov_x, degrees=False)#, K=K)\n",
    "\n",
    "# # hack up cameras_from_opencv_projection\n",
    "# camera_matrix = tK\n",
    "tvec = T\n",
    "# focal_length = torch.stack([camera_matrix[:, 0, 0], camera_matrix[:, 1, 1]], dim=-1)\n",
    "# principal_point = camera_matrix[:, :2, 2]\n",
    "\n",
    "# # Retype the image_size correctly and flip to width, height.\n",
    "# image_size_wh = image_size.to(R).flip(dims=(1,))\n",
    "\n",
    "# # Get the PyTorch3D focal length and principal point.\n",
    "# focal_pytorch3d = focal_length / (0.5 * image_size_wh)\n",
    "# p0_pytorch3d = -(principal_point / (0.5 * image_size_wh) - 1)\n",
    "\n",
    "# For R, T we flip x, y axes (opencv screen space has an opposite\n",
    "# orientation of screen axes).\n",
    "# We also transpose R (opencv multiplies points from the opposite=left side).\n",
    "R_pytorch3d = R.clone().permute(0, 2, 1)\n",
    "T_pytorch3d = tvec.clone()\n",
    "R_pytorch3d[:, :, :2] *= -1\n",
    "T_pytorch3d[:, :2] *= -1\n",
    "# cameras = PerspectiveCameras(\n",
    "#             device=device, R=R_pytorch3d,\n",
    "#             T=T_pytorch3d,\n",
    "#             focal_length=focal_pytorch3d,\n",
    "#             principal_point=p0_pytorch3d, image_size=image_size, in_ndc=True)\n",
    "\n",
    "fov_x, fov_y = CI.get_fov()\n",
    "cameras = FoVPerspectiveCameras(\n",
    "    device=device, fov=fov_y, degrees=False, R=R_pytorch3d, T=T_pytorch3d, aspect_ratio=1.0)\n",
    "\n",
    "\n",
    "# cameras = PerspectiveCameras(device=device, K=K, R=R, T=T, in_ndc=False, image_size=image_size)\n",
    "\n",
    "\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=(height, width), \n",
    "    faces_per_pixel=1, \n",
    ")\n",
    "lights = PointLights(\n",
    "    device=device, \n",
    "    location=cameras.get_world_to_view_transform().transform_points(torch.tensor([[0., 0., -1.]]).cuda()),\n",
    ")\n",
    "# lights = AmbientLights(device=device)\n",
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4, background_color=(0.1, 0.1, 0.1))\n",
    "rasterizer = MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    )\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=rasterizer,\n",
    "    shader=HardPhongShader(device=device, cameras=cameras, lights=lights, blend_params=blend_params)\n",
    ")\n",
    "\n",
    "\n",
    "image_ref = phong_renderer(meshes_world=teapot_mesh)\n",
    "\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "pil_img = F.to_pil_image((255.0*image_ref.cpu().numpy()[0]).astype(np.uint8))\n",
    "\n",
    "from IPython.display import display\n",
    "display(pil_img)\n",
    "\n",
    "\n",
    "# from pytorch3d.vis.plotly_vis import plot_batch_individually\n",
    "# fig = plot_batch_individually([teapot_mesh, cameras])\n",
    "\n",
    "# from plotly.graph_objs import *\n",
    "# fig.layout = Layout(showlegend=True)\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Place a point light in front of the object. As mentioned above, the front of \n",
    "# # the cow is facing the -z direction. \n",
    "# lights = DirectionalLights()\n",
    "\n",
    "# # Initialize an OpenGL perspective camera that represents a batch of different \n",
    "# # viewing angles. All the cameras helper methods support mixed type inputs and \n",
    "# # broadcasting. So we can view the camera from the a distance of dist=2.7, and \n",
    "# # then specify elevation and azimuth angles for each viewpoint as tensors. \n",
    "\n",
    "# # cameras = OpenGLPerspectiveCameras(device=device, R=pose[:3, :3], T=pose[:3, 3])\n",
    "\n",
    "# # We arbitrarily choose one particular view that will be used to visualize \n",
    "# # results\n",
    "# camera = OpenGLPerspectiveCameras(device=device, R=pose[:3, :3], T=pose[:3, 3])\n",
    "\n",
    "# # Define the settings for rasterization and shading. Here we set the output \n",
    "# # image to be of size 128X128. As we are rendering images for visualization \n",
    "# # purposes only we will set faces_per_pixel=1 and blur_radius=0.0. Refer to \n",
    "# # rasterize_meshes.py for explanations of these parameters.  We also leave \n",
    "# # bin_size and max_faces_per_bin to their default values of None, which sets \n",
    "# # their values using heuristics and ensures that the faster coarse-to-fine \n",
    "# # rasterization method is used.  Refer to docs/notes/renderer.md for an \n",
    "# # explanation of the difference between naive and coarse-to-fine rasterization. \n",
    "# raster_settings = RasterizationSettings(\n",
    "#     image_size=512, \n",
    "#     blur_radius=0.0, \n",
    "#     faces_per_pixel=1, \n",
    "# )\n",
    "\n",
    "# # Create a Phong renderer by composing a rasterizer and a shader. The textured \n",
    "# # Phong shader will interpolate the texture uv coordinates for each vertex, \n",
    "# # sample from a texture image and apply the Phong lighting model\n",
    "# renderer = MeshRenderer(\n",
    "#     rasterizer=MeshRasterizer(\n",
    "#         cameras=camera, \n",
    "#         raster_settings=raster_settings\n",
    "#     ),\n",
    "#     shader=SoftPhongShader(\n",
    "#         device=device, \n",
    "#         cameras=camera,\n",
    "#         lights=lights\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # # Create a batch of meshes by repeating the cow mesh and associated textures. \n",
    "# # # Meshes has a useful `extend` method which allows us do this very easily. \n",
    "# # # This also extends the textures. \n",
    "# # meshes = mesh.extend(num_views)\n",
    "\n",
    "# # Render the cow mesh from each viewing angle\n",
    "# target_images = renderer([mesh], cameras=[camera], lights=lights)\n",
    "\n",
    "\n",
    "# target_images\n",
    "# # # Our multi-view cow dataset will be represented by these 2 lists of tensors,\n",
    "# # # each of length num_views.\n",
    "# # target_rgb = [target_images[i, ..., :3] for i in range(num_views)]\n",
    "# # target_cameras = [OpenGLPerspectiveCameras(device=device, R=R[None, i, ...], \n",
    "# #                                            T=T[None, i, ...]) for i in range(num_views)]\n",
    "\n",
    "# # image_grid(target_images.cpu().numpy(), rows=4, cols=5, rgb=True)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fragments = rasterizer(meshes_world=teapot_mesh)\n",
    "\n",
    "zbuf = fragments.zbuf\n",
    "plt.imshow(zbuf[0, ..., 0].cpu().numpy())\n",
    "plt.show()\n",
    "print('zbuf', zbuf.min(), zbuf.max(), zbuf[zbuf > 0].min())\n",
    "# display(F.to_pil_image(image_ref.cpu().numpy()[0].astype(np.uint8)[:, :, -1]))\n",
    "# image_ref.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zbuf[0, ..., 0].cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = CI.image\n",
    "depth = zbuf[0, ..., 0].cpu().numpy()\n",
    "h, w = debug.shape[:2]\n",
    "px_y = np.tile(np.arange(h)[:, np.newaxis], [1, w])\n",
    "px_x = np.tile(np.arange(w)[np.newaxis, :], [h, 1])\n",
    "pyx = np.concatenate([px_y[:,:,np.newaxis], px_x[:, :, np.newaxis]], axis=-1)\n",
    "pyx = pyx.astype(np.float32)\n",
    "\n",
    "vud1 = np.dstack([pyx, depth]).reshape([-1, 3])\n",
    "\n",
    "vud1 = vud1[vud1[:, 2] > 0]\n",
    "uvd = vud1[:, (1, 0, 2)]\n",
    "\n",
    "\n",
    "from psegs.util.plotting import draw_xy_depth_in_image\n",
    "draw_xy_depth_in_image(debug, uvd, period_meters=0.1)\n",
    "\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "pil_img = F.to_pil_image(debug.astype(np.uint8))\n",
    "\n",
    "from IPython.display import display\n",
    "display(pil_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select the viewpoint using spherical angles  \n",
    "# distance = 1.5   # distance from camera to the object\n",
    "# elevation = 50.0   # angle of elevation in degrees\n",
    "# azimuth = 0.0  # No rotation so the camera is positioned on the +Z axis. \n",
    "\n",
    "# # Get the position of the camera based on the spherical angles\n",
    "# R, T = look_at_view_transform(distance, elevation, azimuth, device=device)\n",
    "# print('R', R)\n",
    "# print('T', T)\n",
    "\n",
    "# cameras = FoVPerspectiveCameras(device=device, K=K, znear=0.1, zfar=100)\n",
    "\n",
    "# raster_settings = RasterizationSettings(\n",
    "#     image_size=256, \n",
    "#     blur_radius=0.0, \n",
    "#     faces_per_pixel=1, \n",
    "# )\n",
    "# lights = PointLights(\n",
    "#     device=device, \n",
    "#     location=[[0.0, 5.0, -10.0]], \n",
    "#     diffuse_color=((0, 0, 0),),\n",
    "#     specular_color=((0, 0, 0),),\n",
    "# )\n",
    "# blend_params = BlendParams(sigma=1e-4, gamma=1e-4, background_color=(0.1, 0.1, 0.1))\n",
    "# phong_renderer = MeshRenderer(\n",
    "#     rasterizer=MeshRasterizer(\n",
    "#         cameras=cameras, \n",
    "#         raster_settings=raster_settings\n",
    "#     ),\n",
    "#     shader=HardPhongShader(device=device, cameras=cameras, lights=lights, blend_params=blend_params)\n",
    "# )\n",
    "\n",
    "\n",
    "# image_ref = phong_renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
    "# print('image_ref', image_ref)\n",
    "\n",
    "# import torchvision.transforms.functional as F\n",
    "# import numpy as np\n",
    "# pil_img = F.to_pil_image((255*image_ref.cpu().numpy()[0]).astype(np.uint8))\n",
    "\n",
    "# from IPython.display import display\n",
    "# display(pil_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch3d.vis.plotly_vis\n",
    "dir(pytorch3d.vis.plotly_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_31_01\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_33_35\n",
      "24\n",
      "verts torch.Size([39194, 3])\n",
      "faces torch.Size([74260, 3])\n",
      "mesh <pytorch3d.structures.meshes.Meshes object at 0x7f8b6dc6f250>\n",
      "start batch\n",
      "batch done 6.707640171051025\n",
      "yielding 10\n",
      "0\n",
      "yielding 10\n",
      "1\n",
      "yielding 10\n",
      "2\n",
      "yielding 10\n",
      "3\n",
      "yielding 10\n",
      "4\n",
      "yielding 10\n",
      "5\n",
      "yielding 10\n",
      "6\n",
      "yielding 10\n",
      "7\n",
      "yielding 10\n",
      "8\n",
      "yielding 10\n",
      "9\n",
      "start batch\n",
      "batch done 6.772588729858398\n",
      "yielding 10\n",
      "10\n",
      "yielding 10\n",
      "11\n",
      "yielding 10\n",
      "12\n",
      "yielding 10\n",
      "13\n",
      "yielding 10\n",
      "14\n",
      "yielding 10\n",
      "15\n",
      "yielding 10\n",
      "16\n",
      "yielding 10\n",
      "17\n",
      "yielding 10\n",
      "18\n",
      "yielding 10\n",
      "19\n",
      "start batch\n",
      "batch done 6.8718461990356445\n",
      "yielding 10\n",
      "20\n",
      "yielding 10\n",
      "21\n",
      "yielding 10\n",
      "22\n",
      "yielding 10\n",
      "23\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_33_35pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_33_35.mp4\n",
      "0\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_33_35.mp4pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_33_35pytorch_rgbd_debug.mp4\n",
      "0\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_33_35pytorch_rgbd_debug.mp4pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_33_55\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_36_35\n",
      "84\n",
      "verts torch.Size([395584, 3])\n",
      "faces torch.Size([766400, 3])\n",
      "mesh <pytorch3d.structures.meshes.Meshes object at 0x7f8acde1de50>\n",
      "start batch\n",
      "batch done 68.76018977165222\n",
      "yielding 10\n",
      "0\n",
      "yielding 10\n",
      "1\n",
      "yielding 10\n",
      "2\n",
      "yielding 10\n",
      "3\n",
      "yielding 10\n",
      "4\n",
      "yielding 10\n",
      "5\n",
      "yielding 10\n",
      "6\n",
      "yielding 10\n",
      "7\n",
      "yielding 10\n",
      "8\n",
      "yielding 10\n",
      "9\n",
      "start batch\n",
      "batch done 69.0231020450592\n",
      "yielding 10\n",
      "10\n",
      "yielding 10\n",
      "11\n",
      "yielding 10\n",
      "12\n",
      "yielding 10\n",
      "13\n",
      "yielding 10\n",
      "14\n",
      "yielding 10\n",
      "15\n",
      "yielding 10\n",
      "16\n",
      "yielding 10\n",
      "17\n",
      "yielding 10\n",
      "18\n",
      "yielding 10\n",
      "19\n",
      "start batch\n",
      "batch done 68.19667315483093\n",
      "yielding 10\n",
      "20\n",
      "yielding 10\n",
      "21\n",
      "yielding 10\n",
      "22\n",
      "yielding 10\n",
      "23\n",
      "yielding 10\n",
      "24\n",
      "yielding 10\n",
      "25\n",
      "yielding 10\n",
      "26\n",
      "yielding 10\n",
      "27\n",
      "yielding 10\n",
      "28\n",
      "yielding 10\n",
      "29\n",
      "start batch\n",
      "batch done 68.28464531898499\n",
      "yielding 10\n",
      "30\n",
      "yielding 10\n",
      "31\n",
      "yielding 10\n",
      "32\n",
      "yielding 10\n",
      "33\n",
      "yielding 10\n",
      "34\n",
      "yielding 10\n",
      "35\n",
      "yielding 10\n",
      "36\n",
      "yielding 10\n",
      "37\n",
      "yielding 10\n",
      "38\n",
      "yielding 10\n",
      "39\n",
      "start batch\n",
      "batch done 69.00654196739197\n",
      "yielding 10\n",
      "40\n",
      "yielding 10\n",
      "41\n",
      "yielding 10\n",
      "42\n",
      "yielding 10\n",
      "43\n",
      "yielding 10\n",
      "44\n",
      "yielding 10\n",
      "45\n",
      "yielding 10\n",
      "46\n",
      "yielding 10\n",
      "47\n",
      "yielding 10\n",
      "48\n",
      "yielding 10\n",
      "49\n",
      "start batch\n",
      "batch done 69.10628414154053\n",
      "yielding 10\n",
      "50\n",
      "yielding 10\n",
      "51\n",
      "yielding 10\n",
      "52\n",
      "yielding 10\n",
      "53\n",
      "yielding 10\n",
      "54\n",
      "yielding 10\n",
      "55\n",
      "yielding 10\n",
      "56\n",
      "yielding 10\n",
      "57\n",
      "yielding 10\n",
      "58\n",
      "yielding 10\n",
      "59\n",
      "start batch\n",
      "batch done 69.11561274528503\n",
      "yielding 10\n",
      "60\n",
      "yielding 10\n",
      "61\n",
      "yielding 10\n",
      "62\n",
      "yielding 10\n",
      "63\n",
      "yielding 10\n",
      "64\n",
      "yielding 10\n",
      "65\n",
      "yielding 10\n",
      "66\n",
      "yielding 10\n",
      "67\n",
      "yielding 10\n",
      "68\n",
      "yielding 10\n",
      "69\n",
      "start batch\n",
      "batch done 69.5562355518341\n",
      "yielding 10\n",
      "70\n",
      "yielding 10\n",
      "71\n",
      "yielding 10\n",
      "72\n",
      "yielding 10\n",
      "73\n",
      "yielding 10\n",
      "74\n",
      "yielding 10\n",
      "75\n",
      "yielding 10\n",
      "76\n",
      "yielding 10\n",
      "77\n",
      "yielding 10\n",
      "78\n",
      "yielding 10\n",
      "79\n",
      "start batch\n",
      "batch done 68.92882204055786\n",
      "yielding 10\n",
      "80\n",
      "yielding 10\n",
      "81\n",
      "yielding 10\n",
      "82\n",
      "yielding 10\n",
      "83\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_36_35pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_36_35.mp4\n",
      "0\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_36_35.mp4pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_39_50\n",
      "13\n",
      "verts torch.Size([17327, 3])\n",
      "faces torch.Size([34196, 3])\n",
      "mesh <pytorch3d.structures.meshes.Meshes object at 0x7f8b6dc08bb0>\n",
      "start batch\n",
      "batch done 4.625994920730591\n",
      "yielding 10\n",
      "0\n",
      "yielding 10\n",
      "1\n",
      "yielding 10\n",
      "2\n",
      "yielding 10\n",
      "3\n",
      "yielding 10\n",
      "4\n",
      "yielding 10\n",
      "5\n",
      "yielding 10\n",
      "6\n",
      "yielding 10\n",
      "7\n",
      "yielding 10\n",
      "8\n",
      "yielding 10\n",
      "9\n",
      "start batch\n",
      "batch done 4.538568735122681\n",
      "yielding 10\n",
      "10\n",
      "yielding 10\n",
      "11\n",
      "yielding 10\n",
      "12\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_39_50pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_39_50.mp4\n",
      "0\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_39_50.mp4pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_40_05\n",
      "53\n",
      "verts torch.Size([38339, 3])\n",
      "faces torch.Size([72632, 3])\n",
      "mesh <pytorch3d.structures.meshes.Meshes object at 0x7f8b6de57250>\n",
      "start batch\n",
      "batch done 6.489795923233032\n",
      "yielding 10\n",
      "0\n",
      "yielding 10\n",
      "1\n",
      "yielding 10\n",
      "2\n",
      "yielding 10\n",
      "3\n",
      "yielding 10\n",
      "4\n",
      "yielding 10\n",
      "5\n",
      "yielding 10\n",
      "6\n",
      "yielding 10\n",
      "7\n",
      "yielding 10\n",
      "8\n",
      "yielding 10\n",
      "9\n",
      "start batch\n",
      "batch done 6.385814666748047\n",
      "yielding 10\n",
      "10\n",
      "yielding 10\n",
      "11\n",
      "yielding 10\n",
      "12\n",
      "yielding 10\n",
      "13\n",
      "yielding 10\n",
      "14\n",
      "yielding 10\n",
      "15\n",
      "yielding 10\n",
      "16\n",
      "yielding 10\n",
      "17\n",
      "yielding 10\n",
      "18\n",
      "yielding 10\n",
      "19\n",
      "start batch\n",
      "batch done 6.485798597335815\n",
      "yielding 10\n",
      "20\n",
      "yielding 10\n",
      "21\n",
      "yielding 10\n",
      "22\n",
      "yielding 10\n",
      "23\n",
      "yielding 10\n",
      "24\n",
      "yielding 10\n",
      "25\n",
      "yielding 10\n",
      "26\n",
      "yielding 10\n",
      "27\n",
      "yielding 10\n",
      "28\n",
      "yielding 10\n",
      "29\n",
      "start batch\n",
      "batch done 6.404800891876221\n",
      "yielding 10\n",
      "30\n",
      "yielding 10\n",
      "31\n",
      "yielding 10\n",
      "32\n",
      "yielding 10\n",
      "33\n",
      "yielding 10\n",
      "34\n",
      "yielding 10\n",
      "35\n",
      "yielding 10\n",
      "36\n",
      "yielding 10\n",
      "37\n",
      "yielding 10\n",
      "38\n",
      "yielding 10\n",
      "39\n",
      "start batch\n",
      "batch done 6.3574512004852295\n",
      "yielding 10\n",
      "40\n",
      "yielding 10\n",
      "41\n",
      "yielding 10\n",
      "42\n",
      "yielding 10\n",
      "43\n",
      "yielding 10\n",
      "44\n",
      "yielding 10\n",
      "45\n",
      "yielding 10\n",
      "46\n",
      "yielding 10\n",
      "47\n",
      "yielding 10\n",
      "48\n",
      "yielding 10\n",
      "49\n",
      "start batch\n",
      "batch done 6.355913877487183\n",
      "yielding 10\n",
      "50\n",
      "yielding 10\n",
      "51\n",
      "yielding 10\n",
      "52\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_40_05pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_40_05.mp4\n",
      "0\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_09_16_40_05.mp4pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_13_51_37\n",
      "49\n",
      "verts torch.Size([217631, 3])\n",
      "faces torch.Size([416426, 3])\n",
      "mesh <pytorch3d.structures.meshes.Meshes object at 0x7f8ae11aefa0>\n",
      "start batch\n",
      "batch done 0.022963285446166992\n",
      "yielding 10\n",
      "0\n",
      "yielding 10\n",
      "1\n",
      "yielding 10\n",
      "2\n",
      "yielding 10\n",
      "3\n",
      "yielding 10\n",
      "4\n",
      "yielding 10\n",
      "5\n",
      "yielding 10\n",
      "6\n",
      "yielding 10\n",
      "7\n",
      "yielding 10\n",
      "8\n",
      "yielding 10\n",
      "9\n",
      "start batch\n",
      "batch done 37.83021569252014\n",
      "yielding 10\n",
      "10\n",
      "yielding 10\n",
      "11\n",
      "yielding 10\n",
      "12\n",
      "yielding 10\n",
      "13\n",
      "yielding 10\n",
      "14\n",
      "yielding 10\n",
      "15\n",
      "yielding 10\n",
      "16\n",
      "yielding 10\n",
      "17\n",
      "yielding 10\n",
      "18\n",
      "yielding 10\n",
      "19\n",
      "start batch\n",
      "batch done 38.43993330001831\n",
      "yielding 10\n",
      "20\n",
      "yielding 10\n",
      "21\n",
      "yielding 10\n",
      "22\n",
      "yielding 10\n",
      "23\n",
      "yielding 10\n",
      "24\n",
      "yielding 10\n",
      "25\n",
      "yielding 10\n",
      "26\n",
      "yielding 10\n",
      "27\n",
      "yielding 10\n",
      "28\n",
      "yielding 10\n",
      "29\n",
      "start batch\n",
      "batch done 37.996631383895874\n",
      "yielding 10\n",
      "30\n",
      "yielding 10\n",
      "31\n",
      "yielding 10\n",
      "32\n",
      "yielding 10\n",
      "33\n",
      "yielding 10\n",
      "34\n",
      "yielding 10\n",
      "35\n",
      "yielding 10\n",
      "36\n",
      "yielding 10\n",
      "37\n",
      "yielding 10\n",
      "38\n",
      "yielding 10\n",
      "39\n",
      "start batch\n",
      "batch done 37.649367570877075\n",
      "yielding 10\n",
      "40\n",
      "yielding 10\n",
      "41\n",
      "yielding 10\n",
      "42\n",
      "yielding 10\n",
      "43\n",
      "yielding 10\n",
      "44\n",
      "yielding 10\n",
      "45\n",
      "yielding 10\n",
      "46\n",
      "yielding 10\n",
      "47\n",
      "yielding 10\n",
      "48\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_13_51_37pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_13_51_37.mp4\n",
      "0\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_13_51_37.mp4pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_13_53_07\n",
      "6\n",
      "verts torch.Size([20381, 3])\n",
      "faces torch.Size([38929, 3])\n",
      "mesh <pytorch3d.structures.meshes.Meshes object at 0x7f8b4d7fdd00>\n",
      "start batch\n",
      "batch done 0.008436203002929688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch3d/transforms/transform3d.py:782: UserWarning: R is not a valid rotation matrix\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yielding 10\n",
      "0\n",
      "yielding 10\n",
      "1\n",
      "yielding 10\n",
      "2\n",
      "yielding 10\n",
      "3\n",
      "yielding 10\n",
      "4\n",
      "yielding 10\n",
      "5\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_13_53_07pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_13_53_07.mp4\n",
      "0\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_13_53_07.mp4pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_13_53_25\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_14_01_25\n",
      "95\n",
      "verts torch.Size([96826, 3])\n",
      "faces torch.Size([178585, 3])\n",
      "mesh <pytorch3d.structures.meshes.Meshes object at 0x7f8acdedb0a0>\n",
      "start batch\n",
      "batch done 16.183963537216187\n",
      "yielding 10\n",
      "0\n",
      "yielding 10\n",
      "1\n",
      "yielding 10\n",
      "2\n",
      "yielding 10\n",
      "3\n",
      "yielding 10\n",
      "4\n",
      "yielding 10\n",
      "5\n",
      "yielding 10\n",
      "6\n",
      "yielding 10\n",
      "7\n",
      "yielding 10\n",
      "8\n",
      "yielding 10\n",
      "9\n",
      "start batch\n",
      "batch done 16.106112003326416\n",
      "yielding 10\n",
      "10\n",
      "yielding 10\n",
      "11\n",
      "yielding 10\n",
      "12\n",
      "yielding 10\n",
      "13\n",
      "yielding 10\n",
      "14\n",
      "yielding 10\n",
      "15\n",
      "yielding 10\n",
      "16\n",
      "yielding 10\n",
      "17\n",
      "yielding 10\n",
      "18\n",
      "yielding 10\n",
      "19\n",
      "start batch\n",
      "batch done 15.937429428100586\n",
      "yielding 10\n",
      "20\n",
      "yielding 10\n",
      "21\n",
      "yielding 10\n",
      "22\n",
      "yielding 10\n",
      "23\n",
      "yielding 10\n",
      "24\n",
      "yielding 10\n",
      "25\n",
      "yielding 10\n",
      "26\n",
      "yielding 10\n",
      "27\n",
      "yielding 10\n",
      "28\n",
      "yielding 10\n",
      "29\n",
      "start batch\n",
      "batch done 16.119457244873047\n",
      "yielding 10\n",
      "30\n",
      "yielding 10\n",
      "31\n",
      "yielding 10\n",
      "32\n",
      "yielding 10\n",
      "33\n",
      "yielding 10\n",
      "34\n",
      "yielding 10\n",
      "35\n",
      "yielding 10\n",
      "36\n",
      "yielding 10\n",
      "37\n",
      "yielding 10\n",
      "38\n",
      "yielding 10\n",
      "39\n",
      "start batch\n",
      "batch done 16.603670120239258\n",
      "yielding 10\n",
      "40\n",
      "yielding 10\n",
      "41\n",
      "yielding 10\n",
      "42\n",
      "yielding 10\n",
      "43\n",
      "yielding 10\n",
      "44\n",
      "yielding 10\n",
      "45\n",
      "yielding 10\n",
      "46\n",
      "yielding 10\n",
      "47\n",
      "yielding 10\n",
      "48\n",
      "yielding 10\n",
      "49\n",
      "start batch\n",
      "batch done 16.326587200164795\n",
      "yielding 10\n",
      "50\n",
      "yielding 10\n",
      "51\n",
      "yielding 10\n",
      "52\n",
      "yielding 10\n",
      "53\n",
      "yielding 10\n",
      "54\n",
      "yielding 10\n",
      "55\n",
      "yielding 10\n",
      "56\n",
      "yielding 10\n",
      "57\n",
      "yielding 10\n",
      "58\n",
      "yielding 10\n",
      "59\n",
      "start batch\n",
      "batch done 16.388190746307373\n",
      "yielding 10\n",
      "60\n",
      "yielding 10\n",
      "61\n",
      "yielding 10\n",
      "62\n",
      "yielding 10\n",
      "63\n",
      "yielding 10\n",
      "64\n",
      "yielding 10\n",
      "65\n",
      "yielding 10\n",
      "66\n",
      "yielding 10\n",
      "67\n",
      "yielding 10\n",
      "68\n",
      "yielding 10\n",
      "69\n",
      "start batch\n",
      "batch done 16.29348063468933\n",
      "yielding 10\n",
      "70\n",
      "yielding 10\n",
      "71\n",
      "yielding 10\n",
      "72\n",
      "yielding 10\n",
      "73\n",
      "yielding 10\n",
      "74\n",
      "yielding 10\n",
      "75\n",
      "yielding 10\n",
      "76\n",
      "yielding 10\n",
      "77\n",
      "yielding 10\n",
      "78\n",
      "yielding 10\n",
      "79\n",
      "start batch\n",
      "batch done 16.00083827972412\n",
      "yielding 10\n",
      "80\n",
      "yielding 10\n",
      "81\n",
      "yielding 10\n",
      "82\n",
      "yielding 10\n",
      "83\n",
      "yielding 10\n",
      "84\n",
      "yielding 10\n",
      "85\n",
      "yielding 10\n",
      "86\n",
      "yielding 10\n",
      "87\n",
      "yielding 10\n",
      "88\n",
      "yielding 10\n",
      "89\n",
      "start batch\n",
      "batch done 16.072527170181274\n",
      "yielding 10\n",
      "90\n",
      "yielding 10\n",
      "91\n",
      "yielding 10\n",
      "92\n",
      "yielding 10\n",
      "93\n",
      "yielding 10\n",
      "94\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_14_01_25pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_14_01_25.mp4\n",
      "0\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_14_01_25.mp4pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_14_03_43\n",
      "255\n",
      "verts torch.Size([377602, 3])\n",
      "faces torch.Size([649977, 3])\n",
      "mesh <pytorch3d.structures.meshes.Meshes object at 0x7f8ae11af1c0>\n",
      "start batch\n",
      "batch done 0.030042648315429688\n",
      "yielding 10\n",
      "0\n",
      "yielding 10\n",
      "1\n",
      "yielding 10\n",
      "2\n",
      "yielding 10\n",
      "3\n",
      "yielding 10\n",
      "4\n",
      "yielding 10\n",
      "5\n",
      "yielding 10\n",
      "6\n",
      "yielding 10\n",
      "7\n",
      "yielding 10\n",
      "8\n",
      "yielding 10\n",
      "9\n",
      "start batch\n",
      "batch done 0.0420835018157959\n",
      "yielding 10\n",
      "10\n",
      "yielding 10\n",
      "11\n",
      "yielding 10\n",
      "12\n",
      "yielding 10\n",
      "13\n",
      "yielding 10\n",
      "14\n",
      "yielding 10\n",
      "15\n",
      "yielding 10\n",
      "16\n",
      "yielding 10\n",
      "17\n",
      "yielding 10\n",
      "18\n",
      "yielding 10\n",
      "19\n",
      "start batch\n",
      "batch done 59.171571493148804\n",
      "yielding 10\n",
      "20\n",
      "yielding 10\n",
      "21\n",
      "yielding 10\n",
      "22\n",
      "yielding 10\n",
      "23\n",
      "yielding 10\n",
      "24\n",
      "yielding 10\n",
      "25\n",
      "yielding 10\n",
      "26\n",
      "yielding 10\n",
      "27\n",
      "yielding 10\n",
      "28\n",
      "yielding 10\n",
      "29\n",
      "start batch\n",
      "batch done 59.11370921134949\n",
      "yielding 10\n",
      "30\n",
      "yielding 10\n",
      "31\n",
      "yielding 10\n",
      "32\n",
      "yielding 10\n",
      "33\n",
      "yielding 10\n",
      "34\n",
      "yielding 10\n",
      "35\n",
      "yielding 10\n",
      "36\n",
      "yielding 10\n",
      "37\n",
      "yielding 10\n",
      "38\n",
      "yielding 10\n",
      "39\n",
      "start batch\n",
      "batch done 59.04243779182434\n",
      "yielding 10\n",
      "40\n",
      "yielding 10\n",
      "41\n",
      "yielding 10\n",
      "42\n",
      "yielding 10\n",
      "43\n",
      "yielding 10\n",
      "44\n",
      "yielding 10\n",
      "45\n",
      "yielding 10\n",
      "46\n",
      "yielding 10\n",
      "47\n",
      "yielding 10\n",
      "48\n",
      "yielding 10\n",
      "49\n",
      "start batch\n",
      "batch done 59.59892296791077\n",
      "yielding 10\n",
      "50\n",
      "yielding 10\n",
      "51\n",
      "yielding 10\n",
      "52\n",
      "yielding 10\n",
      "53\n",
      "yielding 10\n",
      "54\n",
      "yielding 10\n",
      "55\n",
      "yielding 10\n",
      "56\n",
      "yielding 10\n",
      "57\n",
      "yielding 10\n",
      "58\n",
      "yielding 10\n",
      "59\n",
      "start batch\n",
      "batch done 59.76929426193237\n",
      "yielding 10\n",
      "60\n",
      "yielding 10\n",
      "61\n",
      "yielding 10\n",
      "62\n",
      "yielding 10\n",
      "63\n",
      "yielding 10\n",
      "64\n",
      "yielding 10\n",
      "65\n",
      "yielding 10\n",
      "66\n",
      "yielding 10\n",
      "67\n",
      "yielding 10\n",
      "68\n",
      "yielding 10\n",
      "69\n",
      "start batch\n",
      "batch done 59.11456346511841\n",
      "yielding 10\n",
      "70\n",
      "yielding 10\n",
      "71\n",
      "yielding 10\n",
      "72\n",
      "yielding 10\n",
      "73\n",
      "yielding 10\n",
      "74\n",
      "yielding 10\n",
      "75\n",
      "yielding 10\n",
      "76\n",
      "yielding 10\n",
      "77\n",
      "yielding 10\n",
      "78\n",
      "yielding 10\n",
      "79\n",
      "start batch\n",
      "batch done 58.423585414886475\n",
      "yielding 10\n",
      "80\n",
      "yielding 10\n",
      "81\n",
      "yielding 10\n",
      "82\n",
      "yielding 10\n",
      "83\n",
      "yielding 10\n",
      "84\n",
      "yielding 10\n",
      "85\n",
      "yielding 10\n",
      "86\n",
      "yielding 10\n",
      "87\n",
      "yielding 10\n",
      "88\n",
      "yielding 10\n",
      "89\n",
      "start batch\n",
      "batch done 58.53560948371887\n",
      "yielding 10\n",
      "90\n",
      "yielding 10\n",
      "91\n",
      "yielding 10\n",
      "92\n",
      "yielding 10\n",
      "93\n",
      "yielding 10\n",
      "94\n",
      "yielding 10\n",
      "95\n",
      "yielding 10\n",
      "96\n",
      "yielding 10\n",
      "97\n",
      "yielding 10\n",
      "98\n",
      "yielding 10\n",
      "99\n",
      "start batch\n",
      "batch done 58.85455942153931\n",
      "yielding 10\n",
      "100\n",
      "yielding 10\n",
      "101\n",
      "yielding 10\n",
      "102\n",
      "yielding 10\n",
      "103\n",
      "yielding 10\n",
      "104\n",
      "yielding 10\n",
      "105\n",
      "yielding 10\n",
      "106\n",
      "yielding 10\n",
      "107\n",
      "yielding 10\n",
      "108\n",
      "yielding 10\n",
      "109\n",
      "start batch\n",
      "batch done 58.46099400520325\n",
      "yielding 10\n",
      "110\n",
      "yielding 10\n",
      "111\n",
      "yielding 10\n",
      "112\n",
      "yielding 10\n",
      "113\n",
      "yielding 10\n",
      "114\n",
      "yielding 10\n",
      "115\n",
      "yielding 10\n",
      "116\n",
      "yielding 10\n",
      "117\n",
      "yielding 10\n",
      "118\n",
      "yielding 10\n",
      "119\n",
      "start batch\n",
      "batch done 59.131831407547\n",
      "yielding 10\n",
      "120\n",
      "yielding 10\n",
      "121\n",
      "yielding 10\n",
      "122\n",
      "yielding 10\n",
      "123\n",
      "yielding 10\n",
      "124\n",
      "yielding 10\n",
      "125\n",
      "yielding 10\n",
      "126\n",
      "yielding 10\n",
      "127\n",
      "yielding 10\n",
      "128\n",
      "yielding 10\n",
      "129\n",
      "start batch\n",
      "batch done 58.82161569595337\n",
      "yielding 10\n",
      "130\n",
      "yielding 10\n",
      "131\n",
      "yielding 10\n",
      "132\n",
      "yielding 10\n",
      "133\n",
      "yielding 10\n",
      "134\n",
      "yielding 10\n",
      "135\n",
      "yielding 10\n",
      "136\n",
      "yielding 10\n",
      "137\n",
      "yielding 10\n",
      "138\n",
      "yielding 10\n",
      "139\n",
      "start batch\n",
      "batch done 58.33298206329346\n",
      "yielding 10\n",
      "140\n",
      "yielding 10\n",
      "141\n",
      "yielding 10\n",
      "142\n",
      "yielding 10\n",
      "143\n",
      "yielding 10\n",
      "144\n",
      "yielding 10\n",
      "145\n",
      "yielding 10\n",
      "146\n",
      "yielding 10\n",
      "147\n",
      "yielding 10\n",
      "148\n",
      "yielding 10\n",
      "149\n",
      "start batch\n",
      "batch done 57.73146176338196\n",
      "yielding 10\n",
      "150\n",
      "yielding 10\n",
      "151\n",
      "yielding 10\n",
      "152\n",
      "yielding 10\n",
      "153\n",
      "yielding 10\n",
      "154\n",
      "yielding 10\n",
      "155\n",
      "yielding 10\n",
      "156\n",
      "yielding 10\n",
      "157\n",
      "yielding 10\n",
      "158\n",
      "yielding 10\n",
      "159\n",
      "start batch\n",
      "batch done 57.902730226516724\n",
      "yielding 10\n",
      "160\n",
      "yielding 10\n",
      "161\n",
      "yielding 10\n",
      "162\n",
      "yielding 10\n",
      "163\n",
      "yielding 10\n",
      "164\n",
      "yielding 10\n",
      "165\n",
      "yielding 10\n",
      "166\n",
      "yielding 10\n",
      "167\n",
      "yielding 10\n",
      "168\n",
      "yielding 10\n",
      "169\n",
      "start batch\n",
      "batch done 59.02810978889465\n",
      "yielding 10\n",
      "170\n",
      "yielding 10\n",
      "171\n",
      "yielding 10\n",
      "172\n",
      "yielding 10\n",
      "173\n",
      "yielding 10\n",
      "174\n",
      "yielding 10\n",
      "175\n",
      "yielding 10\n",
      "176\n",
      "yielding 10\n",
      "177\n",
      "yielding 10\n",
      "178\n",
      "yielding 10\n",
      "179\n",
      "start batch\n",
      "batch done 58.587236404418945\n",
      "yielding 10\n",
      "180\n",
      "yielding 10\n",
      "181\n",
      "yielding 10\n",
      "182\n",
      "yielding 10\n",
      "183\n",
      "yielding 10\n",
      "184\n",
      "yielding 10\n",
      "185\n",
      "yielding 10\n",
      "186\n",
      "yielding 10\n",
      "187\n",
      "yielding 10\n",
      "188\n",
      "yielding 10\n",
      "189\n",
      "start batch\n",
      "batch done 59.01538968086243\n",
      "yielding 10\n",
      "190\n",
      "yielding 10\n",
      "191\n",
      "yielding 10\n",
      "192\n",
      "yielding 10\n",
      "193\n",
      "yielding 10\n",
      "194\n",
      "yielding 10\n",
      "195\n",
      "yielding 10\n",
      "196\n",
      "yielding 10\n",
      "197\n",
      "yielding 10\n",
      "198\n",
      "yielding 10\n",
      "199\n",
      "start batch\n",
      "batch done 58.89523100852966\n",
      "yielding 10\n",
      "200\n",
      "yielding 10\n",
      "201\n",
      "yielding 10\n",
      "202\n",
      "yielding 10\n",
      "203\n",
      "yielding 10\n",
      "204\n",
      "yielding 10\n",
      "205\n",
      "yielding 10\n",
      "206\n",
      "yielding 10\n",
      "207\n",
      "yielding 10\n",
      "208\n",
      "yielding 10\n",
      "209\n",
      "start batch\n",
      "batch done 59.06337022781372\n",
      "yielding 10\n",
      "210\n",
      "yielding 10\n",
      "211\n",
      "yielding 10\n",
      "212\n",
      "yielding 10\n",
      "213\n",
      "yielding 10\n",
      "214\n",
      "yielding 10\n",
      "215\n",
      "yielding 10\n",
      "216\n",
      "yielding 10\n",
      "217\n",
      "yielding 10\n",
      "218\n",
      "yielding 10\n",
      "219\n",
      "start batch\n",
      "batch done 58.29233264923096\n",
      "yielding 10\n",
      "220\n",
      "yielding 10\n",
      "221\n",
      "yielding 10\n",
      "222\n",
      "yielding 10\n",
      "223\n",
      "yielding 10\n",
      "224\n",
      "yielding 10\n",
      "225\n",
      "yielding 10\n",
      "226\n",
      "yielding 10\n",
      "227\n",
      "yielding 10\n",
      "228\n",
      "yielding 10\n",
      "229\n",
      "start batch\n",
      "batch done 57.86449980735779\n",
      "yielding 10\n",
      "230\n",
      "yielding 10\n",
      "231\n",
      "yielding 10\n",
      "232\n",
      "yielding 10\n",
      "233\n",
      "yielding 10\n",
      "234\n",
      "yielding 10\n",
      "235\n",
      "yielding 10\n",
      "236\n",
      "yielding 10\n",
      "237\n",
      "yielding 10\n",
      "238\n",
      "yielding 10\n",
      "239\n",
      "start batch\n",
      "batch done 58.66905236244202\n",
      "yielding 10\n",
      "240\n",
      "yielding 10\n",
      "241\n",
      "yielding 10\n",
      "242\n",
      "yielding 10\n",
      "243\n",
      "yielding 10\n",
      "244\n",
      "yielding 10\n",
      "245\n",
      "yielding 10\n",
      "246\n",
      "yielding 10\n",
      "247\n",
      "yielding 10\n",
      "248\n",
      "yielding 10\n",
      "249\n",
      "start batch\n",
      "batch done 60.20902061462402\n",
      "yielding 10\n",
      "250\n",
      "yielding 10\n",
      "251\n",
      "yielding 10\n",
      "252\n",
      "yielding 10\n",
      "253\n",
      "yielding 10\n",
      "254\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_14_03_43pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_14_03_43.mp4\n",
      "0\n",
      "done /outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_14_03_43.mp4pytorch_rgbd_debug.mp4\n",
      "/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/2021_06_12_14_18_46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "verts torch.Size([494355, 3])\n",
      "faces torch.Size([849403, 3])\n",
      "mesh <pytorch3d.structures.meshes.Meshes object at 0x7f8ae11a16a0>\n",
      "start batch\n",
      "batch done 77.76547455787659\n",
      "yielding 10\n",
      "0\n",
      "yielding 10\n",
      "1\n",
      "yielding 10\n",
      "2\n",
      "yielding 10\n",
      "3\n",
      "yielding 10\n",
      "4\n",
      "yielding 10\n",
      "5\n",
      "yielding 10\n",
      "6\n",
      "yielding 10\n",
      "7\n",
      "yielding 10\n",
      "8\n",
      "yielding 10\n",
      "9\n",
      "start batch\n",
      "batch done 76.28297543525696\n",
      "yielding 10\n",
      "10\n",
      "yielding 10\n",
      "11\n",
      "yielding 10\n",
      "12\n",
      "yielding 10\n",
      "13\n",
      "yielding 10\n",
      "14\n",
      "yielding 10\n",
      "15\n",
      "yielding 10\n",
      "16\n",
      "yielding 10\n",
      "17\n",
      "yielding 10\n",
      "18\n",
      "yielding 10\n",
      "19\n",
      "start batch\n",
      "batch done 77.92781257629395\n",
      "yielding 10\n",
      "20\n",
      "yielding 10\n",
      "21\n",
      "yielding 10\n",
      "22\n",
      "yielding 10\n",
      "23\n",
      "yielding 10\n",
      "24\n",
      "yielding 10\n",
      "25\n",
      "yielding 10\n",
      "26\n",
      "yielding 10\n",
      "27\n",
      "yielding 10\n",
      "28\n",
      "yielding 10\n",
      "29\n",
      "start batch\n",
      "batch done 76.22453212738037\n",
      "yielding 10\n",
      "30\n",
      "yielding 10\n",
      "31\n",
      "yielding 10\n",
      "32\n",
      "yielding 10\n",
      "33\n",
      "yielding 10\n",
      "34\n",
      "yielding 10\n",
      "35\n",
      "yielding 10\n",
      "36\n",
      "yielding 10\n",
      "37\n",
      "yielding 10\n",
      "38\n",
      "yielding 10\n",
      "39\n",
      "start batch\n",
      "batch done 76.08658266067505\n",
      "yielding 10\n",
      "40\n",
      "yielding 10\n",
      "41\n",
      "yielding 10\n",
      "42\n",
      "yielding 10\n",
      "43\n",
      "yielding 10\n",
      "44\n",
      "yielding 10\n",
      "45\n",
      "yielding 10\n",
      "46\n",
      "yielding 10\n",
      "47\n",
      "yielding 10\n",
      "48\n",
      "yielding 10\n",
      "49\n",
      "start batch\n",
      "batch done 75.78864288330078\n",
      "yielding 10\n",
      "50\n",
      "yielding 10\n",
      "51\n",
      "yielding 10\n",
      "52\n",
      "yielding 10\n",
      "53\n",
      "yielding 10\n",
      "54\n",
      "yielding 10\n",
      "55\n",
      "yielding 10\n",
      "56\n",
      "yielding 10\n",
      "57\n",
      "yielding 10\n",
      "58\n",
      "yielding 10\n",
      "59\n",
      "start batch\n",
      "batch done 75.90212106704712\n",
      "yielding 10\n",
      "60\n",
      "yielding 10\n",
      "61\n",
      "yielding 10\n",
      "62\n",
      "yielding 10\n",
      "63\n",
      "yielding 10\n",
      "64\n",
      "yielding 10\n",
      "65\n",
      "yielding 10\n",
      "66\n",
      "yielding 10\n",
      "67\n",
      "yielding 10\n",
      "68\n",
      "yielding 10\n",
      "69\n",
      "start batch\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/psegs')\n",
    "\n",
    "import os\n",
    "\n",
    "ROOT = '/outer_root/media/red14000/Pictures_and_Docs/lidarphone_lidar_scans/'\n",
    "\n",
    "for d in sorted(os.listdir(ROOT)):\n",
    "    if '.DS_Store' in d:\n",
    "        continue\n",
    "    base_dir = os.path.join(ROOT, d)\n",
    "    print(base_dir)\n",
    "    \n",
    "\n",
    "    from psegs.datasets import ios_lidar\n",
    "\n",
    "\n",
    "    from oarphpy import util as oputil\n",
    "    json_paths = oputil.all_files_recursive(base_dir, pattern='frame*.json')\n",
    "    json_paths = sorted(json_paths)\n",
    "    \n",
    "    try:\n",
    "        cis = [ios_lidar.threeDScannerApp_create_camera_image(p) for p in json_paths]\n",
    "    except AssertionError as e:\n",
    "        continue\n",
    "\n",
    "    print(len(cis))\n",
    "    \n",
    "    mesh_path = os.path.join(base_dir, 'export_refined.obj')\n",
    "    if not os.path.exists(mesh_path):\n",
    "        mesh_path = os.path.join(base_dir, 'export.obj')\n",
    "    \n",
    "    outpath = os.path.join(ROOT, d + 'pytorch_rgbd_debug.mp4')\n",
    "    import imageio\n",
    "    writer = imageio.get_writer(outpath, fps=5)\n",
    "    \n",
    "    from psegs.render.mesh2rgbd import pytorch3d_iter_mesh2uvd_for_camera_images\n",
    "    \n",
    "    iter_uvds = pytorch3d_iter_mesh2uvd_for_camera_images(cis, mesh_path)\n",
    "    for i, (ci, uvd) in enumerate(zip(cis, iter_uvds)):\n",
    "        debug = ci.image\n",
    "        from psegs.util.plotting import draw_xy_depth_in_image\n",
    "        draw_xy_depth_in_image(debug, uvd, period_meters=0.1)\n",
    "        writer.append_data(debug)\n",
    "        print(i)\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    print('done', outpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
